{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/CANE/blob/master/code/CANE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm_6DHW8WU30"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDqukMh63rB7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import gc\n",
        "import math\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "from math import pow\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup for GPU utilization"
      ],
      "metadata": {
        "id": "4xFZ1pXoJetp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRBhpBhe2_ks"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Optional: Set GPU memory growth to avoid over-allocation\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU is ready for use!\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4m5FOfsdNrY"
      },
      "outputs": [],
      "source": [
        "print(\"Is TensorFlow using GPU?\", tf.test.is_built_with_cuda())  # Should return True\n",
        "print(\"GPU device:\", tf.test.gpu_device_name())  # Should return something like /device:GPU:0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVyefNnedNrZ"
      },
      "outputs": [],
      "source": [
        "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0e5GPf8RBx"
      },
      "source": [
        "# ***Global Variables & General Functionality***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iTSFdmS8THg"
      },
      "outputs": [],
      "source": [
        "MAX_LEN          = 300 # Default value for single execution\n",
        "MAX_LENS         = [] # List to hold values for multiple executions\n",
        "neg_table_size   = 1000000\n",
        "NEG_SAMPLE_POWER = 0.75\n",
        "batch_size       = 64\n",
        "num_epoch        = 50 # Default: 200\n",
        "embed_size       = 200\n",
        "lr               = 1e-3\n",
        "rho              = \"1.0,0.3,0.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y-INwI4KbIs"
      },
      "outputs": [],
      "source": [
        "dataset_name    = \"arxiv\"\n",
        "\n",
        "parent_path = f'Datasets/{dataset_name}/graph-v2'\n",
        "parent_path = '/content'\n",
        "\n",
        "data_text_file  = \"data-v3-500.txt\"\n",
        "data_text_files = [\"data-v3-500.txt\",\n",
        "                   \"data-v3-500C.txt\",\n",
        "                   \"YAKE10.txt\",\n",
        "                   \"YAKE5.txt\",\n",
        "                   \"RAKE10.txt\",\n",
        "                   \"RAKE5.txt\",\n",
        "                   \"RAKE10C.txt\",\n",
        "                   \"RAKE5C.txt\",\n",
        "                   \"TFIDF10.txt\",\n",
        "                   \"TFIDF5.txt\",\n",
        "                   \"PosR5.txt\",\n",
        "                   \"PosR10.txt\",\n",
        "                   \"TextR5.txt\",\n",
        "                   \"TextR10.txt\",\n",
        "                   \"TopicR5.txt\",\n",
        "                   \"TopicR10.txt\"]\n",
        "\n",
        "\n",
        "graph_file      = 'graph.txt'\n",
        "categories_file = 'group-v2.txt'\n",
        "\n",
        "\n",
        "log_file               = 'CANE_Execution_Logs.txt'\n",
        "link_pred_results_file = 'CANE_Link_Pred_Res.txt'\n",
        "node_clf_results_file  = 'CANE_Node_Clf_Res.txt'\n",
        "\n",
        "\n",
        "split_graph_file  = 'sgraph15.txt' # For single execution\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "\n",
        "test_graph_file   = 'tgraph85.txt' # For single execution\n",
        "test_graph_files  = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbyLcchZiSt0"
      },
      "outputs": [],
      "source": [
        "# Parameters for node classification\n",
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num   = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xEhbCPl7UQY"
      },
      "outputs": [],
      "source": [
        "# Find the maximum number of words from each data text file\n",
        "for txtf in data_text_files:\n",
        "  max_word_count = 0\n",
        "  min_word_count = float('inf')\n",
        "\n",
        "  with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "          word_count = len(re.findall(r\"\\b\\w+\\b\", line)) # Find only the words\n",
        "\n",
        "          if word_count > max_word_count:\n",
        "              max_word_count = word_count\n",
        "\n",
        "          if word_count < min_word_count:\n",
        "              min_word_count = word_count\n",
        "\n",
        "  MAX_LENS.append(max_word_count+1)\n",
        "  print(f'=== {txtf} ===')\n",
        "  print(\"Max word count:\", max_word_count)\n",
        "  print(f'Min word count: {min_word_count}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EQCjraydNrc"
      },
      "outputs": [],
      "source": [
        "# Find the average number of words from each data text file\n",
        "for txtf in data_text_files: # Options 1) ['data.txt'] 2) data_text_files:\n",
        "    total_word_count = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    with open(f'{parent_path}/{txtf}', 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            total_word_count += len(re.findall(r\"\\b\\w+\\b\", line))\n",
        "            total_lines += 1\n",
        "\n",
        "    mean_word_count = total_word_count / total_lines if total_lines > 0 else 0\n",
        "    MAX_LENS.append(int(math.ceil(mean_word_count)))\n",
        "    print(f'=== {txtf} ===')\n",
        "    print(f'Mean word count: {math.ceil(mean_word_count)}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zVeaFyDyEgP"
      },
      "outputs": [],
      "source": [
        "MAX_LENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al6hac8qDB03"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For single execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WZ7YnkTr6zl"
      },
      "outputs": [],
      "source": [
        "zero_list = np.array([0 for _ in range(embed_size)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs1QG7X_gbvY"
      },
      "outputs": [],
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "\n",
        "  with open(f'{file_path}', \"r\") as f:\n",
        "      for idx, line in enumerate(f):\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMT8a7k02JC9"
      },
      "source": [
        "Execute the following cell only for node classification tasks. In node classification tasks the number of nodes and edges remain invariably the same. Only the text files change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M8ZhhEergjM"
      },
      "outputs": [],
      "source": [
        "# Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "with open(f'{parent_path}/{graph_file}', 'r') as f:\n",
        "  edges = f.readlines()\n",
        "\n",
        "edge_list = []\n",
        "nodes = [] # \"nodes\" will contain all the unique nodes of the graph\n",
        "\n",
        "for edge in edges:\n",
        "  edge_list.append(list(edge.split()))\n",
        "\n",
        "for edge in edge_list:\n",
        "  for node in edge:\n",
        "    if node not in nodes:\n",
        "      nodes.append(nodes)\n",
        "    else:\n",
        "      continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e79uwM60xaNN"
      },
      "outputs": [],
      "source": [
        "len(nodes) # The number of nodes should be equal to the true number of nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZoTrK80xhm3"
      },
      "outputs": [],
      "source": [
        "len(edge_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwNH2N0iVJ1Y"
      },
      "source": [
        "# ***DATASET***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz7H_lORVMOm"
      },
      "outputs": [],
      "source": [
        "class dataSet:\n",
        "    def __init__(self, text_path, graph_path, text_filename=None):\n",
        "\n",
        "        self.text_filename = text_filename if text_filename else \"\"\n",
        "        text_file, graph_file = self.load(text_path, graph_path)\n",
        "        self.edges = self.load_edges(graph_file)\n",
        "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "        self.negative_table = InitNegTable(self.edges)\n",
        "\n",
        "    def load(self, text_path, graph_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        for a in range(0, len(text_file)):\n",
        "            text_file[a] = str(text_file[a])\n",
        "\n",
        "        return text_file, open(graph_path, 'rb').readlines()\n",
        "\n",
        "    def load_edges(self, graph_file):\n",
        "        return [list(map(int, edge.strip().decode().split())) for edge in graph_file]\n",
        "\n",
        "    def load_text(self, text_file):\n",
        "        \"\"\"\n",
        "        Adapting with adapt(text_data):\n",
        "\n",
        "        vectorize_layer.adapt(text_data) analyzes text_data, builds a vocabulary, and assigns a unique integer ID to each word based on its frequency (most frequent words get lower IDs).\n",
        "        Transforming with vectorize_layer(text_data):\n",
        "\n",
        "        This maps each word in text_data to its corresponding integer token ID, producing a 2D array where each row represents a sequence of token IDs for a given input line, padded or truncated to MAX_LEN.\n",
        "        \"\"\"\n",
        "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=None,  # Set a limit if needed\n",
        "            output_mode='int',\n",
        "            output_sequence_length=MAX_LEN\n",
        "        )\n",
        "\n",
        "        text_data = [line.strip() for line in text_file]\n",
        "\n",
        "        if \"data-v3-500\" in self.text_filename:\n",
        "          text_data_size = len(text_data)\n",
        "          batch_size = int(text_data_size / 10)\n",
        "          ranges = []\n",
        "          start = 0\n",
        "          while start < text_data_size:\n",
        "              end = min(start + batch_size, text_data_size)  # Ensure the last range includes all remaining abstracts\n",
        "              ranges.append([start, end])\n",
        "              start = end\n",
        "\n",
        "          ranges[-2][1] = ranges[-1][1]\n",
        "          del ranges[-1]\n",
        "\n",
        "          for range in ranges:\n",
        "            vectorize_layer.adapt(text_data[range[0]:range[1]])\n",
        "\n",
        "        else:\n",
        "          vectorize_layer.adapt(text_data)\n",
        "\n",
        "        return vectorize_layer(text_data).numpy(), len(vectorize_layer.get_vocabulary()), len(text_data)\n",
        "\n",
        "    def negative_sample(self, edges):\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)]\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = func()\n",
        "            while node1[i] == neg_node or node2[i] == neg_node:\n",
        "                neg_node = func()\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "\n",
        "        return sample_edges\n",
        "\n",
        "    def generate_batches(self, mode=None):\n",
        "        num_batch = len(self.edges) // batch_size\n",
        "        edges = self.edges\n",
        "\n",
        "        if mode == 'add':\n",
        "          num_batch += 1\n",
        "          edges.extend(edges[:(batch_size - len(self.edges) // batch_size)])\n",
        "\n",
        "        if mode != 'add':\n",
        "          random.shuffle(edges)\n",
        "\n",
        "        sample_edges = edges[:num_batch * batch_size]\n",
        "        sample_edges = self.negative_sample(sample_edges)\n",
        "\n",
        "        batches = []\n",
        "        for i in range(num_batch):\n",
        "            batches.append(sample_edges[i * batch_size:(i + 1) * batch_size])\n",
        "        #print sample_edges[0]\n",
        "        return batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AV9wwI_7NSC"
      },
      "source": [
        "# ***CANE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxoCqzz37O8y"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, vocab_size, num_nodes, rho):\n",
        "        rho = rho.split(\",\")\n",
        "        self.rho1 = float(rho[0])\n",
        "        self.rho2 = float(rho[1])\n",
        "        self.rho3 = float(rho[2])\n",
        "\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "          self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta')\n",
        "          self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb')\n",
        "          self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg')\n",
        "          self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1')\n",
        "          self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2')\n",
        "          self.Node_neg = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n3')\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            self.text_embed = tf.Variable(tf.random.truncated_normal([vocab_size, embed_size // 2], stddev=0.3))\n",
        "            self.node_embed = tf.clip_by_norm(tf.Variable(tf.random.truncated_normal([num_nodes, embed_size // 2], stddev=0.3)), clip_norm=1, axes=1)\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.embedding_lookup(self.node_embed, self.Node_a)\n",
        "            self.N_B = tf.nn.embedding_lookup(self.node_embed, self.Node_b)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_neg)\n",
        "\n",
        "        self.convA, self.convB, self.convNeg = self.conv()\n",
        "        self.loss = self.compute_loss()\n",
        "\n",
        "    def conv(self):\n",
        "        W2 = tf.Variable(tf.random.truncated_normal([2, embed_size // 2, 1, 100], stddev=0.3))\n",
        "        rand_matrix = tf.Variable(tf.random.truncated_normal([100, 100], stddev=0.3)) # rand_matrix is the attentive matrix A\n",
        "\n",
        "        # The convolution operation\n",
        "        convA = tf.nn.conv2d(self.T_A, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convB = tf.nn.conv2d(self.T_B, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convNEG = tf.nn.conv2d(self.T_NEG, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "        hA = tf.tanh(tf.squeeze(convA)) # P\n",
        "        hB = tf.tanh(tf.squeeze(convB)) # Q\n",
        "        hNEG = tf.tanh(tf.squeeze(convNEG))\n",
        "\n",
        "        tmphA = tf.reshape(hA, [batch_size * (MAX_LEN - 1), embed_size // 2]) # Reshaping P so that it can be multiplied with A\n",
        "        ha_mul_rand = tf.reshape(tf.matmul(tmphA, rand_matrix),\n",
        "                                 [batch_size, MAX_LEN - 1, embed_size // 2]) # Multiplying P^T with A and reshaping the result so that it can be multiplied with Q\n",
        "\n",
        "        r1 = tf.matmul(ha_mul_rand, hB, adjoint_b=True) # r1 = P^T * A * Q\n",
        "        r3 = tf.matmul(ha_mul_rand, hNEG, adjoint_b=True)\n",
        "        att1 = tf.expand_dims(tf.stack(r1), -1)\n",
        "        att3 = tf.expand_dims(tf.stack(r3), -1)\n",
        "\n",
        "        att1 = tf.tanh(att1) # tanh(P^T * A * Q). This is the F matrix\n",
        "        att3 = tf.tanh(att3)\n",
        "\n",
        "        pooled_A = tf.reduce_mean(att1, 2) # Row-mean-pooling F\n",
        "        pooled_B = tf.reduce_mean(att1, 1) # Column-mean-pooling F\n",
        "        pooled_NEG = tf.reduce_mean(att3, 1)\n",
        "\n",
        "        a_flat = tf.squeeze(pooled_A)\n",
        "        b_flat = tf.squeeze(pooled_B)\n",
        "        neg_flat = tf.squeeze(pooled_NEG)\n",
        "\n",
        "        w_A = tf.nn.softmax(a_flat) # Row-mean-pooling F + softmax\n",
        "        w_B = tf.nn.softmax(b_flat) # Column-mean-pooling F + softmax\n",
        "        w_NEG = tf.nn.softmax(neg_flat)\n",
        "\n",
        "        rep_A = tf.expand_dims(w_A, -1)\n",
        "        rep_B = tf.expand_dims(w_B, -1)\n",
        "        rep_NEG = tf.expand_dims(w_NEG, -1)\n",
        "\n",
        "        # Transpose P and Q so that they can be multiplied with a^p and a^q\n",
        "        hA = tf.transpose(hA, perm=[0, 2, 1])\n",
        "        hB = tf.transpose(hB, perm=[0, 2, 1])\n",
        "        hNEG = tf.transpose(hNEG, perm=[0, 2, 1])\n",
        "\n",
        "        rep1 = tf.matmul(hA, rep_A)\n",
        "        rep2 = tf.matmul(hB, rep_B)\n",
        "        rep3 = tf.matmul(hNEG, rep_NEG)\n",
        "\n",
        "        attA = tf.squeeze(rep1)\n",
        "        attB = tf.squeeze(rep2)\n",
        "        attNEG = tf.squeeze(rep3)\n",
        "\n",
        "        return attA, attB, attNEG\n",
        "\n",
        "    def compute_loss(self):\n",
        "        p1 = tf.reduce_sum(tf.multiply(self.convA, self.convB), 1)\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(p1) + 0.001)\n",
        "\n",
        "        p2 = tf.reduce_sum(tf.multiply(self.convA, self.convNeg), 1)\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(-p2) + 0.001)\n",
        "\n",
        "        p3 = tf.reduce_sum(tf.multiply(self.N_A, self.N_B), 1)\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(p3) + 0.001)\n",
        "\n",
        "        p4 = tf.reduce_sum(tf.multiply(self.N_A, self.N_NEG), 1)\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(-p4) + 0.001)\n",
        "\n",
        "        p5 = tf.reduce_sum(tf.multiply(self.convB, self.N_A), 1)\n",
        "        p5 = tf.math.log(tf.nn.sigmoid(p5) + 0.001)\n",
        "\n",
        "        p6 = tf.reduce_sum(tf.multiply(self.convNeg, self.N_A), 1)\n",
        "        p6 = tf.math.log(tf.nn.sigmoid(-p6) + 0.001)\n",
        "\n",
        "        p7 = tf.reduce_sum(tf.multiply(self.N_B, self.convA), 1)\n",
        "        p7 = tf.math.log(tf.nn.sigmoid(p7) + 0.001)\n",
        "\n",
        "        p8 = tf.reduce_sum(tf.multiply(self.N_B, self.convNeg), 1)\n",
        "        p8 = tf.math.log(tf.nn.sigmoid(-p8) + 0.001)\n",
        "\n",
        "        temp_loss = self.rho1 * (p1 + p2) + self.rho2 * (p3 + p4) + self.rho3 * (p5 + p6) + self.rho3 * (p7 + p8)\n",
        "        loss = -tf.reduce_sum(temp_loss)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vt_wvryq0oh"
      },
      "source": [
        "# ***NEGATIVE TABLE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdwgdktTq504"
      },
      "outputs": [],
      "source": [
        "def InitNegTable(edges):\n",
        "    a_list, b_list = zip(*edges)\n",
        "    a_list = list(a_list)\n",
        "    b_list = list(b_list)\n",
        "    node = a_list\n",
        "    node.extend(b_list)\n",
        "\n",
        "    node_degree = {}\n",
        "    for i in node:\n",
        "        if i in node_degree:\n",
        "            node_degree[i] += 1\n",
        "        else:\n",
        "            node_degree[i] = 1\n",
        "    sum_degree = 0\n",
        "    for i in node_degree.values():\n",
        "        sum_degree += pow(i, 0.75)\n",
        "\n",
        "    por = 0\n",
        "    cur_sum = 0\n",
        "    vid = -1\n",
        "    neg_table = []\n",
        "    degree_list = list(node_degree.values())\n",
        "    node_id = list(node_degree.keys())\n",
        "    for i in range(neg_table_size):\n",
        "        if ((i + 1) / float(neg_table_size)) > por:\n",
        "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
        "            por = cur_sum / sum_degree\n",
        "            vid += 1\n",
        "        neg_table.append(node_id[vid])\n",
        "\n",
        "    #print(len(neg_table))\n",
        "    return neg_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPF03ZFtp5cg"
      },
      "source": [
        "# ***CLASSIFY***\n",
        "Taken directly from the DeepEmLAN NE (Node Embedding) method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HNJ1pwZp8m0"
      },
      "outputs": [],
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        # X_train = [self.embeddings[x] for x in X]\n",
        "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
        "\n",
        "        # Access the values of X and Y based on the shuffled indices\n",
        "\n",
        "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "\n",
        "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLaqva64WoA0"
      },
      "source": [
        "# ***RUN(Single execution)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgbe18H4WsgX"
      },
      "outputs": [],
      "source": [
        "def prepareData(graph, ratio):\n",
        "  with open(f'{parent_path}/{graph}', 'rb') as f:\n",
        "    edges = [edge for edge in f]\n",
        "\n",
        "  selected = int(len(edges) * float(ratio))\n",
        "  selected = selected - selected % batch_size\n",
        "  selected = random.sample(edges, selected)\n",
        "  remain = [edge for edge in edges if edge not in selected]\n",
        "  ''' try:\n",
        "    temp_dir = Path('temp')\n",
        "\n",
        "    # Check if the directory exists, if so, delete it\n",
        "    if temp_dir.exists() and temp_dir.is_dir():\n",
        "        shutil.rmtree(temp_dir)\n",
        "        print(\"Existing directory deleted.\")\n",
        "\n",
        "    # Create the directory\n",
        "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Directory created successfully.\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\") '''\n",
        "\n",
        "  with open(f\"{parent_path}/sgraph{str(ratio).split('.')[1]}.txt\", 'wb') as f:\n",
        "    for edge in selected:\n",
        "      f.write(edge)\n",
        "\n",
        "  with open(f\"{parent_path}/tgraph{str(1.0 - ratio).split('.')[1]}.txt\", 'wb') as f:\n",
        "    for edge in remain:\n",
        "      f.write(edge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK_Z-E1HXRVm"
      },
      "outputs": [],
      "source": [
        "prepareData(graph_file, ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wFgChvWggiu"
      },
      "outputs": [],
      "source": [
        "data = dataSet(f'{parent_path}/{data_text_file}',\n",
        "               f'{parent_path}/{graph_file}')\n",
        "\n",
        "# Saving embeddings\n",
        "#embed_file = f\"{parent_path}/Results/CANE/embed_link_pred_{split_graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
        "embed_file = f\"{parent_path}/Results/CANE/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\" # For node classification the whole graph ('graph.txt') is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QsRmNqLXrGb"
      },
      "outputs": [],
      "source": [
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "        model = Model(data.num_vocab, data.num_nodes, rho)\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "        train_op = opt.minimize(model.loss)\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        # Training\n",
        "        start_time = datetime.now()\n",
        "        for epoch in range(num_epoch):\n",
        "            batches = data.generate_batches()\n",
        "            h1 = 0\n",
        "            num_batch = len(batches)\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3\n",
        "                }\n",
        "\n",
        "                # Run the graph\n",
        "                _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        print(f'Total time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "        with open(embed_file, 'wb') as f: # In a single execution setting only one \"embeddings.txt\" file is required\n",
        "            batches = data.generate_batches(mode='add')\n",
        "            num_batch = len(batches)\n",
        "            embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3\n",
        "                }\n",
        "\n",
        "                # Fetch embeddings\n",
        "                convA, convB, NA, NB = sess.run([model.convA, model.convB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "                for j in range(batch_size):\n",
        "                    em = list(convA[j]) + list(NA[j])\n",
        "                    #em = list(TA[j])\n",
        "                    embed[node1[j]].append(em)\n",
        "                    em = list(convB[j]) + list(NB[j])\n",
        "                    #em = list(TB[j])\n",
        "                    embed[node2[j]].append(em)\n",
        "\n",
        "\n",
        "            for i in range(data.num_nodes):\n",
        "                if embed[i]:\n",
        "                    tmp = np.sum(embed[i], axis=0) / len(embed[i]) # np.mean(embed[i], axis=0)\n",
        "                    f.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                else:\n",
        "                    #f.write('\\n'.encode()) # For link prediction\n",
        "                    f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP2Rr_5Lgzq5"
      },
      "source": [
        "## ***Link Prediction (Single and multiple executions)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZsQv5KyY83v"
      },
      "outputs": [],
      "source": [
        "# embed_files contains the node embeddings\n",
        "embed_files = [\n",
        "    f\"{parent_path}/Results/CANE/embed_link_pred_{graph}_{txtf}.txt\"\n",
        "    for txtf in [\"data-v3-500\",\n",
        "                 \"data-v3-500C\",\n",
        "                 \"YAKE10\",\n",
        "                 \"PosR10\",\n",
        "                 \"PosR5\",\n",
        "                 \"YAKE5\",\n",
        "                 \"RAKE10\",\n",
        "                 \"RAKE5\",\n",
        "                 \"RAKE10C\",\n",
        "                 \"RAKE5C\",\n",
        "                 \"TFIDF5\",\n",
        "                 \"TFIDF10\",\n",
        "                 \"TextR10\",\n",
        "                 \"TextR5\",\n",
        "                 \"TopicR10\",\n",
        "                 \"TopicR5\"]\n",
        "    for graph in split_graph_files\n",
        "]\n",
        "\n",
        "# Initialize a log file to store the AUC results\n",
        "with open(f'{parent_path}/Results/CANE/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode() != '\\n': #j.decode().strip():\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split()))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
        "\n",
        "      nodes = list(set([i for j in edges for i in j]))\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for i, j in edges:\n",
        "          if i in node2vec.keys() and j in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == j or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/CANE/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inFtZZQg20T"
      },
      "source": [
        "## ***Node Classification (Single and multiple executions)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8nCKWP3g6Lh"
      },
      "outputs": [],
      "source": [
        "# embed_files contains the node embedding\n",
        "''' embed_files = [\n",
        "    f\"{parent_path}/Results/CANE/embed_node_clf_graph_{txtf}.txt\"\n",
        "    for txtf in [\"data-v3-500\",\n",
        "                 \"data-v3-500C\",\n",
        "                 \"YAKE10\",\n",
        "                 \"PosR10\",\n",
        "                 \"PosR5\",\n",
        "                 \"YAKE5\",\n",
        "                 \"RAKE10\",\n",
        "                 \"RAKE5\",\n",
        "                 \"RAKE10C\",\n",
        "                 \"RAKE5C\",\n",
        "                 \"TFIDF5\",\n",
        "                 \"TFIDF10\",\n",
        "                 \"TextR10\",\n",
        "                 \"TextR5\",\n",
        "                 \"TopicR10\",\n",
        "                 \"TopicR5\"]] '''\n",
        "\n",
        "embed_files = ['embed_graph_YAKE.txt']\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "\n",
        "for ef in embed_files:\n",
        "  X = []\n",
        "  Y = []\n",
        "  new_vector = get_vectors_from_file(ef)\n",
        "\n",
        "  for jk in range(0, len(nodes)):\n",
        "    if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "      tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "      # Y.append([(int)(i) for i in tags])\n",
        "      lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "      if len(lli) != 0:\n",
        "        if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "          X.append(jk)\n",
        "          Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "  # This part of the code uses only the X and Y lists created above\n",
        "  mi = {}\n",
        "  ma = {}\n",
        "  li1 = []\n",
        "  li2 = []\n",
        "  with open(f'{parent_path}/Results/CANE/{node_clf_results_file}', 'a') as f:\n",
        "    f.write(f\"{ef.split('/')[-1]} \\n\")\n",
        "    print(ef.split('/')[-1])\n",
        "    for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "      for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "        clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                        clf=LogisticRegression())\n",
        "\n",
        "        result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "        # Results\n",
        "        li1.append(result['micro'])\n",
        "        li2.append(result['macro'])\n",
        "\n",
        "\n",
        "      mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "      ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "      print(mi)\n",
        "      print(ma)\n",
        "      print()\n",
        "\n",
        "      f.writelines(str(str(mi)+str(ma)))\n",
        "      f.write('\\n')\n",
        "\n",
        "      # Reinitialize the dictionaries and lists\n",
        "      mi = {}\n",
        "      ma = {}\n",
        "      li1 = []\n",
        "      li2 = []\n",
        "\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggfZeV0G2dgF"
      },
      "source": [
        "# ***RUN(Multiple executions)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYbahwK42hhF"
      },
      "outputs": [],
      "source": [
        "for gf in split_graph_files: # for gf in split_graph_files: -> For link prediction. For node classification just use -> for gf in ['graph.txt']:\n",
        "    for t, txtf in enumerate(data_text_files):\n",
        "\n",
        "        MAX_LEN = MAX_LENS[t]\n",
        "        print(f'The maximum length is: {MAX_LEN}')\n",
        "\n",
        "        data = dataSet(f'{parent_path}/{txtf}', f'{parent_path}/{gf}', text_filename=txtf)\n",
        "\n",
        "        # Logging the execution details\n",
        "        with open(f'{parent_path}/Results/CANE/{log_file}', 'a') as f:\n",
        "            f.write(f'Processing graph: {gf}, text: {txtf}\\n')\n",
        "\n",
        "        print(f'Processing graph: {gf}, text: {txtf}')\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            sess = tf.compat.v1.Session()\n",
        "            with sess.as_default():\n",
        "                model = Model(data.num_vocab, data.num_nodes, rho)\n",
        "                opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "                train_op = opt.minimize(model.loss)\n",
        "                sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "                # Training\n",
        "                start_time = datetime.now()\n",
        "                for epoch in range(num_epoch):\n",
        "                    batches = data.generate_batches()\n",
        "                    num_batch = len(batches)\n",
        "\n",
        "                    for i in range(num_batch):\n",
        "                        batch = batches[i]\n",
        "                        node1, node2, node3 = zip(*batch)\n",
        "                        node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                        text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                        feed_dict = {\n",
        "                            model.Text_a: text1,\n",
        "                            model.Text_b: text2,\n",
        "                            model.Text_neg: text3,\n",
        "                            model.Node_a: node1,\n",
        "                            model.Node_b: node2,\n",
        "                            model.Node_neg: node3\n",
        "                        }\n",
        "\n",
        "                        # Run the graph\n",
        "                        _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "\n",
        "                end_time = datetime.now()\n",
        "                with open(f'{parent_path}/Results/CANE/{log_file}', 'a') as f:\n",
        "                    f.write(f'Time: {((end_time - start_time).total_seconds()) / 60.0} min\\n')\n",
        "\n",
        "                print(f'Total Time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "                # Save embeddings with a unique name\n",
        "                embed_file = f\"{parent_path}/Results/CANE/embed_link_pred_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "                #embed_file = f\"{parent_path}/Results/CANE/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "\n",
        "                with open(embed_file, 'wb') as f:\n",
        "                    batches = data.generate_batches(mode='add')\n",
        "                    num_batch = len(batches)\n",
        "                    embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "                    for i in range(num_batch):\n",
        "                        batch = batches[i]\n",
        "                        node1, node2, node3 = zip(*batch)\n",
        "                        node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                        text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                        feed_dict = {\n",
        "                            model.Text_a: text1,\n",
        "                            model.Text_b: text2,\n",
        "                            model.Text_neg: text3,\n",
        "                            model.Node_a: node1,\n",
        "                            model.Node_b: node2,\n",
        "                            model.Node_neg: node3\n",
        "                        }\n",
        "\n",
        "                        # Fetch embeddings\n",
        "                        convA, convB, NA, NB = sess.run(\n",
        "                            [model.convA, model.convB, model.N_A, model.N_B],\n",
        "                            feed_dict=feed_dict\n",
        "                        )\n",
        "\n",
        "                        for j in range(batch_size):\n",
        "                            em = list(convA[j]) + list(NA[j])\n",
        "                            #em = list(TA[j])\n",
        "                            embed[node1[j]].append(em)\n",
        "                            em = list(convB[j]) + list(NB[j])\n",
        "                            #em = list(TB[j])\n",
        "                            embed[node2[j]].append(em)\n",
        "\n",
        "                    for i in range(data.num_nodes):\n",
        "                      if embed[i]:\n",
        "                          tmp = np.sum(embed[i], axis=0) / len(embed[i]) # np.mean(embed[i], axis=0)\n",
        "                          f.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                      else:\n",
        "                          f.write('\\n'.encode()) # For link prediction\n",
        "                          #f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
        "\n",
        "                # Log completion\n",
        "                with open(f'{parent_path}/Results/CANE/{log_file}', 'a') as f:\n",
        "                    f.write(f'Embeddings saved to: {embed_file}\\n')\n",
        "\n",
        "        gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Context_Aware_Node_Embeddings",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}