{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/CANE/blob/master/code/CANE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Libraries & Tools***"
      ],
      "metadata": {
        "id": "Lm_6DHW8WU30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wDqukMh63rB7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "from math import pow\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Cs_tnKOwef_Q",
        "outputId": "0075d865-488f-4839-e469-3784f69df6d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Global Variables & General Functionality***"
      ],
      "metadata": {
        "id": "Rt0e5GPf8RBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 300 # Default value for single execution\n",
        "MAX_LENS = [] # List to hold the values for multiple execution\n",
        "neg_table_size = 1000000\n",
        "NEG_SAMPLE_POWER = 0.75\n",
        "batch_size = 64\n",
        "num_epoch = 50 # Default: 200\n",
        "embed_size = 200\n",
        "lr = 1e-3"
      ],
      "metadata": {
        "id": "0iTSFdmS8THg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"cora\"\n",
        "data_text_file = \"YAKE.txt\"\n",
        "data_text_files = [\"YAKE10.txt\", \"PositionRank.txt\", \"PositionRank10.txt\"]\n",
        "graph_file = 'graph.txt'\n",
        "parent_path = f'/content/Datasets/{dataset_name}'\n",
        "log_file = 'CANE_Execution_Logs.txt'\n",
        "link_pred_results_file = 'CANE_Link_Pred_Res.txt'\n",
        "node_clf_results_file = 'CANE_Node_Clf_Res.txt'\n",
        "categories_file = 'group-v3.txt'\n",
        "\n",
        "\n",
        "split_graph_file = 'sgraph15.txt'\n",
        "split_graph_files = ['sgraph15.txt', 'sgraph45.txt', 'sgraph75.txt']\n",
        "test_graph_file = 'tgraph85.txt'\n",
        "test_graph_files = ['tgraph85.txt', 'tgraph55.txt', 'tgraph25.txt']\n",
        "\n",
        "# Model parameters\n",
        "ratio = 0.15\n",
        "rho = \"1.0,0.3,0.3\""
      ],
      "metadata": {
        "id": "4Y-INwI4KbIs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_ratio = [0.15, 0.45, 0.75]\n",
        "clf_num = 5\n",
        "train_classifier = True"
      ],
      "metadata": {
        "id": "vbyLcchZiSt0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tf in data_text_files:\n",
        "  max_word_count = 0\n",
        "  min_word_count = float('inf')\n",
        "\n",
        "  with open(f'{parent_path}/{tf}', 'r') as file:\n",
        "      for line in file:\n",
        "          word_count = len(line.split())\n",
        "\n",
        "          if word_count > max_word_count:\n",
        "              max_word_count = word_count\n",
        "\n",
        "          if word_count < min_word_count:\n",
        "              min_word_count = word_count\n",
        "\n",
        "  MAX_LENS.append(max_word_count+1)\n",
        "  print(f'=== {tf} ===')\n",
        "  print(\"Max word count:\", max_word_count)\n",
        "  print(\"Min word count:\", min_word_count)\n",
        "  print()\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "0xEhbCPl7UQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENS"
      ],
      "metadata": {
        "id": "2zVeaFyDyEgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = MAX_LENS[-1] # For single execution"
      ],
      "metadata": {
        "id": "al6hac8qDB03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile('/content/PartialData.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(parent_path)\n",
        "\n",
        "print(\"Extraction complete!\")"
      ],
      "metadata": {
        "id": "sdXCdfP_-hHD",
        "outputId": "d6cf2899-955b-4b14-a486-b1a4069168ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_list = []\n",
        "for i in range(0, embed_size):\n",
        "    zero_list.append(0)\n",
        "zero_list = np.array(zero_list)"
      ],
      "metadata": {
        "id": "-WZ7YnkTr6zl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors_from_file(file_path):\n",
        "  vectors = {}\n",
        "\n",
        "  with open(f'{file_path}', \"r\") as f:\n",
        "      for idx, line in enumerate(f):\n",
        "          vector = list(map(float, line.strip().split()))  # Convert to list of floats\n",
        "          vectors[idx] = vector  # Assign embedding to node idx\n",
        "\n",
        "  return vectors"
      ],
      "metadata": {
        "id": "Gs1QG7X_gbvY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the python code below only for node classification tasks"
      ],
      "metadata": {
        "id": "XMT8a7k02JC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the edge list. Store the unique nodes in the list \"nodes\"\n",
        "with open(f'{parent_path}/{graph_file}', 'r') as f:\n",
        "  eedges = f.readlines()\n",
        "\n",
        "edge_list = []\n",
        "nodes = [] # \"nodes\" will contain all the unique nodes of the graph\n",
        "for ee in eedges:\n",
        "  edge_list.append(list(ee.split()))\n",
        "for ll in edge_list:\n",
        "  for ed in ll:\n",
        "    if ed not in nodes:\n",
        "      nodes.append(ed)\n",
        "    else:\n",
        "      continue"
      ],
      "metadata": {
        "id": "1M8ZhhEergjM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "id": "e79uwM60xaNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(edge_list)"
      ],
      "metadata": {
        "id": "4ZoTrK80xhm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***DataSet***"
      ],
      "metadata": {
        "id": "UwNH2N0iVJ1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataSet:\n",
        "    def __init__(self, text_path, graph_path):\n",
        "\n",
        "        text_file, graph_file = self.load(text_path, graph_path)\n",
        "        self.edges = self.load_edges(graph_file)\n",
        "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "        self.negative_table = InitNegTable(self.edges)\n",
        "\n",
        "    def load(self, text_path, graph_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        for a in range(0, len(text_file)):\n",
        "            text_file[a] = str(text_file[a])\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "\n",
        "        return text_file, graph_file\n",
        "\n",
        "    def load_edges(self, graph_file):\n",
        "        edges = []\n",
        "        for i in graph_file:\n",
        "            edges.append(list(map(int, i.strip().decode().split('\\t'))))\n",
        "\n",
        "        #print(\"Total load %d edges.\" % len(edges))\n",
        "\n",
        "        return edges\n",
        "\n",
        "    def load_text(self, text_file):\n",
        "        \"\"\"\n",
        "        Adapting with adapt(text_data):\n",
        "\n",
        "        vectorize_layer.adapt(text_data) analyzes text_data, builds a vocabulary, and assigns a unique integer ID to each word based on its frequency (most frequent words get lower IDs).\n",
        "        Transforming with vectorize_layer(text_data):\n",
        "\n",
        "        This maps each word in text_data to its corresponding integer token ID, producing a 2D array where each row represents a sequence of token IDs for a given input line, padded or truncated to max_len.\n",
        "        \"\"\"\n",
        "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=None,  # Set a limit if needed\n",
        "            output_mode='int',\n",
        "            output_sequence_length=MAX_LEN\n",
        "        )\n",
        "\n",
        "        text_data = [line.strip() for line in text_file]\n",
        "\n",
        "        vectorize_layer.adapt(text_data)\n",
        "\n",
        "        text = vectorize_layer(text_data).numpy()\n",
        "\n",
        "        num_vocab = len(vectorize_layer.get_vocabulary())\n",
        "        #print(f'Vocabulary: {num_vocab}')\n",
        "        num_nodes = len(text)\n",
        "\n",
        "        return text, num_vocab, num_nodes\n",
        "\n",
        "    def negative_sample(self, edges):\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)]\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = func()\n",
        "            while node1[i] == neg_node or node2[i] == neg_node:\n",
        "                neg_node = func()\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "\n",
        "        return sample_edges\n",
        "\n",
        "    def generate_batches(self, mode=None):\n",
        "\n",
        "        num_batch = len(self.edges) // batch_size\n",
        "        edges = self.edges\n",
        "\n",
        "        if mode == 'add':\n",
        "          num_batch += 1\n",
        "          edges.extend(edges[:(batch_size - len(self.edges) // batch_size)])\n",
        "\n",
        "        if mode != 'add':\n",
        "          random.shuffle(edges)\n",
        "        sample_edges = edges[:num_batch * batch_size]\n",
        "        sample_edges = self.negative_sample(sample_edges)\n",
        "\n",
        "        batches = []\n",
        "        for i in range(num_batch):\n",
        "            batches.append(sample_edges[i * batch_size:(i + 1) * batch_size])\n",
        "        # print sample_edges[0]\n",
        "        return batches\n"
      ],
      "metadata": {
        "id": "oz7H_lORVMOm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***CANE***"
      ],
      "metadata": {
        "id": "-AV9wwI_7NSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, vocab_size, num_nodes, rho):\n",
        "        rho = rho.split(\",\")\n",
        "        self.rho1 = float(rho[0])\n",
        "        self.rho2 = float(rho[1])\n",
        "        self.rho3 = float(rho[2])\n",
        "\n",
        "        # '''hyperparameter'''\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "          self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta')\n",
        "          self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb')\n",
        "          self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg')\n",
        "          self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1')\n",
        "          self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2')\n",
        "          self.Node_neg = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n3')\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            self.text_embed = tf.Variable(tf.random.truncated_normal([vocab_size, embed_size // 2], stddev=0.3))\n",
        "            self.node_embed = tf.clip_by_norm(tf.Variable(tf.random.truncated_normal([num_nodes, embed_size // 2], stddev=0.3)), clip_norm=1, axes=1)\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.embedding_lookup(self.node_embed, self.Node_a)\n",
        "            self.N_B = tf.nn.embedding_lookup(self.node_embed, self.Node_b)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_neg)\n",
        "\n",
        "        self.convA, self.convB, self.convNeg = self.conv()\n",
        "        self.loss = self.compute_loss()\n",
        "\n",
        "    def conv(self):\n",
        "        W2 = tf.Variable(tf.random.truncated_normal([2, embed_size // 2, 1, 100], stddev=0.3))\n",
        "        rand_matrix = tf.Variable(tf.random.truncated_normal([100, 100], stddev=0.3))\n",
        "\n",
        "        convA = tf.nn.conv2d(self.T_A, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convB = tf.nn.conv2d(self.T_B, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convNEG = tf.nn.conv2d(self.T_NEG, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "        hA = tf.tanh(tf.squeeze(convA))\n",
        "        hB = tf.tanh(tf.squeeze(convB))\n",
        "        hNEG = tf.tanh(tf.squeeze(convNEG))\n",
        "\n",
        "        tmphA = tf.reshape(hA, [batch_size * (MAX_LEN - 1), embed_size // 2])\n",
        "        ha_mul_rand = tf.reshape(tf.matmul(tmphA, rand_matrix),\n",
        "                                 [batch_size, MAX_LEN - 1, embed_size // 2])\n",
        "        r1 = tf.matmul(ha_mul_rand, hB, adjoint_b=True)\n",
        "        r3 = tf.matmul(ha_mul_rand, hNEG, adjoint_b=True)\n",
        "        att1 = tf.expand_dims(tf.stack(r1), -1)\n",
        "        att3 = tf.expand_dims(tf.stack(r3), -1)\n",
        "\n",
        "        att1 = tf.tanh(att1)\n",
        "        att3 = tf.tanh(att3)\n",
        "\n",
        "        pooled_A = tf.reduce_mean(att1, 2)\n",
        "        pooled_B = tf.reduce_mean(att1, 1)\n",
        "        pooled_NEG = tf.reduce_mean(att3, 1)\n",
        "\n",
        "        a_flat = tf.squeeze(pooled_A)\n",
        "        b_flat = tf.squeeze(pooled_B)\n",
        "        neg_flat = tf.squeeze(pooled_NEG)\n",
        "\n",
        "        w_A = tf.nn.softmax(a_flat)\n",
        "        w_B = tf.nn.softmax(b_flat)\n",
        "        w_NEG = tf.nn.softmax(neg_flat)\n",
        "\n",
        "        rep_A = tf.expand_dims(w_A, -1)\n",
        "        rep_B = tf.expand_dims(w_B, -1)\n",
        "        rep_NEG = tf.expand_dims(w_NEG, -1)\n",
        "\n",
        "        hA = tf.transpose(hA, perm=[0, 2, 1])\n",
        "        hB = tf.transpose(hB, perm=[0, 2, 1])\n",
        "        hNEG = tf.transpose(hNEG, perm=[0, 2, 1])\n",
        "\n",
        "        rep1 = tf.matmul(hA, rep_A)\n",
        "        rep2 = tf.matmul(hB, rep_B)\n",
        "        rep3 = tf.matmul(hNEG, rep_NEG)\n",
        "\n",
        "        attA = tf.squeeze(rep1)\n",
        "        attB = tf.squeeze(rep2)\n",
        "        attNEG = tf.squeeze(rep3)\n",
        "\n",
        "        return attA, attB, attNEG\n",
        "\n",
        "    def compute_loss(self):\n",
        "        p1 = tf.reduce_sum(tf.multiply(self.convA, self.convB), 1)\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(p1) + 0.001)\n",
        "\n",
        "        p2 = tf.reduce_sum(tf.multiply(self.convA, self.convNeg), 1)\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(-p2) + 0.001)\n",
        "\n",
        "        p3 = tf.reduce_sum(tf.multiply(self.N_A, self.N_B), 1)\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(p3) + 0.001)\n",
        "\n",
        "        p4 = tf.reduce_sum(tf.multiply(self.N_A, self.N_NEG), 1)\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(-p4) + 0.001)\n",
        "\n",
        "        p5 = tf.reduce_sum(tf.multiply(self.convB, self.N_A), 1)\n",
        "        p5 = tf.math.log(tf.nn.sigmoid(p5) + 0.001)\n",
        "\n",
        "        p6 = tf.reduce_sum(tf.multiply(self.convNeg, self.N_A), 1)\n",
        "        p6 = tf.math.log(tf.nn.sigmoid(-p6) + 0.001)\n",
        "\n",
        "        p7 = tf.reduce_sum(tf.multiply(self.N_B, self.convA), 1)\n",
        "        p7 = tf.math.log(tf.nn.sigmoid(p7) + 0.001)\n",
        "\n",
        "        p8 = tf.reduce_sum(tf.multiply(self.N_B, self.convNeg), 1)\n",
        "        p8 = tf.math.log(tf.nn.sigmoid(-p8) + 0.001)\n",
        "\n",
        "        rho1 = self.rho1\n",
        "        rho2 = self.rho2\n",
        "        rho3 = self.rho3\n",
        "        temp_loss = rho1 * (p1 + p2) + rho2 * (p3 + p4) + rho3 * (p5 + p6) + rho3 * (p7 + p8)\n",
        "        loss = -tf.reduce_sum(temp_loss)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "nxoCqzz37O8y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Negative Sample***"
      ],
      "metadata": {
        "id": "2Vt_wvryq0oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def InitNegTable(edges):\n",
        "    a_list, b_list = zip(*edges)\n",
        "    a_list = list(a_list)\n",
        "    b_list = list(b_list)\n",
        "    node = a_list\n",
        "    node.extend(b_list)\n",
        "\n",
        "    node_degree = {}\n",
        "    for i in node:\n",
        "        if i in node_degree:\n",
        "            node_degree[i] += 1\n",
        "        else:\n",
        "            node_degree[i] = 1\n",
        "    sum_degree = 0\n",
        "    for i in node_degree.values():\n",
        "        sum_degree += pow(i, 0.75)\n",
        "\n",
        "    por = 0\n",
        "    cur_sum = 0\n",
        "    vid = -1\n",
        "    neg_table = []\n",
        "    degree_list = list(node_degree.values())\n",
        "    node_id = list(node_degree.keys())\n",
        "    for i in range(neg_table_size):\n",
        "        if ((i + 1) / float(neg_table_size)) > por:\n",
        "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
        "            por = cur_sum / sum_degree\n",
        "            vid += 1\n",
        "        neg_table.append(node_id[vid])\n",
        "\n",
        "    #print(len(neg_table))\n",
        "    return neg_table\n"
      ],
      "metadata": {
        "id": "UdwgdktTq504"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Classify***"
      ],
      "metadata": {
        "id": "oPF03ZFtp5cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            probs_[:] = 0\n",
        "            probs_[labels] = 1\n",
        "            all_labels.append(probs_)\n",
        "        return np.asarray(all_labels)\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, vectors, clf):\n",
        "        self.embeddings = vectors\n",
        "        self.clf = TopKRanker(clf)\n",
        "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
        "\n",
        "    def train(self, X, Y, Y_all):\n",
        "        self.binarizer.fit(Y_all)\n",
        "        # X_train = [self.embeddings[x] for x in X]\n",
        "        X_train = [self.embeddings[int(x)] for x in X] # For each node in X, take its embedding\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        self.clf.fit(X_train, Y)\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        top_k_list = [len(l) for l in Y] # For each label in Y, take its size (multi-label)\n",
        "        Y_ = self.predict(X, top_k_list)\n",
        "        Y = self.binarizer.transform(Y)\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        results = {}\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(Y, Y_, average=average)\n",
        "        return results\n",
        "\n",
        "    def predict(self, X, top_k_list):\n",
        "        X_ = np.asarray([self.embeddings[int(x)] for x in X])\n",
        "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
        "        return Y\n",
        "\n",
        "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
        "        state = np.random.get_state()\n",
        "        training_size = int(train_precent * len(X)) # Set the ratio based on the size of X\n",
        "        np.random.seed(seed)\n",
        "        shuffle_indices = np.random.permutation(np.arange(len(X))) # Shuffle the indices of X (X contains all nodes)\n",
        "\n",
        "        # Access the values of X and Y based on the shuffled indices\n",
        "\n",
        "        # X_train and Y_train will have \"training_size\" number of values of X and Y\n",
        "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
        "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
        "\n",
        "        # X_test and Y_test will have \"len(X) - training_size\" number of values of X and Y\n",
        "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
        "\n",
        "        self.train(X_train, Y_train, Y) # Y has the labels of all nodes\n",
        "        np.random.set_state(state)\n",
        "        return self.evaluate(X_test, Y_test)\n",
        "\n",
        "\n",
        "\n",
        "def load_embeddings(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    node_num, size = [int(x) for x in fin.readline().strip().split()]\n",
        "    vectors = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        vec = l.strip().split(' ')\n",
        "        assert len(vec) == size + 1\n",
        "        vectors[vec[0]] = [float(x) for x in vec[1:]]\n",
        "    fin.close()\n",
        "    assert len(vectors) == node_num\n",
        "    return vectors\n",
        "\n",
        "def read_node_label(filename):\n",
        "    fin = open(filename, 'r')\n",
        "    X = []\n",
        "    Y = []\n",
        "    XY_dic = {}\n",
        "    X_Y_dic = {}\n",
        "    while 1:\n",
        "        l = fin.readline()\n",
        "        if l == '':\n",
        "            break\n",
        "        # vec = l.strip().split('\\t')\n",
        "        vec = l.strip().split(' ')\n",
        "        X.append(vec[0])\n",
        "        Y.append(vec[1:])\n",
        "        X_Y_dic[str(vec[0])] = str(vec[1:][0])\n",
        "        XY_dic.setdefault(str(vec[1:][0]), []).append(str(vec[0]))\n",
        "    fin.close()\n",
        "    return X, Y, XY_dic, X_Y_dic"
      ],
      "metadata": {
        "id": "8HNJ1pwZp8m0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Run (Single execution)***"
      ],
      "metadata": {
        "id": "rLaqva64WoA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(graph, ratio):\n",
        "  with open(f'{parent_path}/{graph}', 'rb') as f:\n",
        "    edges = [i for i in f]\n",
        "\n",
        "  selected = int(len(edges) * float(ratio))\n",
        "  selected = selected - selected % batch_size\n",
        "  selected = random.sample(edges, selected)\n",
        "  remain = [i for i in edges if i not in selected]\n",
        "  try:\n",
        "    temp_dir = Path('temp')\n",
        "\n",
        "    # Check if the directory exists, if so, delete it\n",
        "    if temp_dir.exists() and temp_dir.is_dir():\n",
        "        shutil.rmtree(temp_dir)\n",
        "        print(\"Existing directory deleted.\")\n",
        "\n",
        "    # Create the directory\n",
        "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Directory created successfully.\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  with open('temp/graph.txt', 'wb') as f:\n",
        "    for i in selected:\n",
        "      f.write(i)\n",
        "\n",
        "  with open('temp/test_graph.txt', 'wb') as f:\n",
        "    for i in remain:\n",
        "      f.write(i)"
      ],
      "metadata": {
        "id": "dgbe18H4WsgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepareData(graph_file, ratio)"
      ],
      "metadata": {
        "id": "yK_Z-E1HXRVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "#graph_path = os.path.join('/content/temp/graph.txt') # Use this if you executed the prepareData() function\n",
        "\n",
        "data = dataSet(f'{parent_path}/{data_text_file}',\n",
        "               f'{parent_path}/{graph_file}')\n",
        "\n",
        "# Saving embeddings\n",
        "#embed_file = f\"{parent_path}/Results/CANE/embed_link_pred_{split_graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\"\n",
        "embed_file = f\"{parent_path}/Results/CANE/embed_node_clf_{graph_file.split('.')[0]}_{data_text_file.split('.')[0]}.txt\" # For node classification the whole graph ('graph.txt') is used"
      ],
      "metadata": {
        "id": "-wFgChvWggiu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "        model = Model(data.num_vocab, data.num_nodes, rho)\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "        train_op = opt.minimize(model.loss)\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        #total_time = 0\n",
        "\n",
        "        # Training\n",
        "        print('Start training.......')\n",
        "        start_time = datetime.now()\n",
        "        for epoch in tqdm(range(num_epoch)):\n",
        "            #start_time = datetime.now()\n",
        "            loss_epoch = 0\n",
        "            batches = data.generate_batches()\n",
        "            h1 = 0\n",
        "            num_batch = len(batches)\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3\n",
        "                }\n",
        "\n",
        "                # run the graph\n",
        "                _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "                loss_epoch += loss_batch\n",
        "\n",
        "            #end_time = datetime.now()\n",
        "            #total_time += (end_time - start_time).total_seconds()\n",
        "            #print('epoch: ', epoch + 1, ' loss: ', loss_epoch)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        print(f'Total time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "        with open(embed_file, 'wb') as f: # In a single execution setting only one \"embeddings.txt\" file is required\n",
        "            batches = data.generate_batches(mode='add')\n",
        "            num_batch = len(batches)\n",
        "            embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3\n",
        "                }\n",
        "\n",
        "                # Fetch embeddings\n",
        "                convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "                for j in range(batch_size):\n",
        "                    em = list(convA[j]) + list(TA[j])\n",
        "                    #em = list(TA[j])\n",
        "                    embed[node1[j]].append(em)\n",
        "                    em = list(convB[j]) + list(TB[j])\n",
        "                    #em = list(TB[j])\n",
        "                    embed[node2[j]].append(em)\n",
        "\n",
        "\n",
        "            for i in range(data.num_nodes):\n",
        "                if embed[i]:\n",
        "                    tmp = np.sum(embed[i], axis=0) / len(embed[i]) # np.mean(embed[i], axis=0)\n",
        "                    f.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                else:\n",
        "                    #f.write('\\n'.encode())\n",
        "                    f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "1QsRmNqLXrGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Link Prediction***"
      ],
      "metadata": {
        "id": "cP2Rr_5Lgzq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node2vec = {}\n",
        "with open(embed_file, 'rb') as f:\n",
        "  for i, j in enumerate(f):\n",
        "    if j.decode() != '\\n':\n",
        "      node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "\n",
        "\n",
        "with open(os.path.join(f'{parent_path}/{test_graph_file}'), 'rb') as f:\n",
        "  edges = [list(map(int, i.strip().decode().split('\\t'))) for i in f]\n",
        "\n",
        "\n",
        "nodes = list(set([i for j in edges for i in j]))\n",
        "a = 0\n",
        "b = 0\n",
        "for i, j in edges:\n",
        "  if i in node2vec.keys() and j in node2vec.keys():\n",
        "    dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "    random_node = random.sample(nodes, 1)[0]\n",
        "    while random_node == j or random_node not in node2vec.keys():\n",
        "        random_node = random.sample(nodes, 1)[0]\n",
        "    dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "    if dot1 > dot2:\n",
        "        a += 1\n",
        "    elif dot1 == dot2:\n",
        "        a += 0.5\n",
        "    b += 1\n",
        "\n",
        "print(\"Auc value:\", float(a) / b)"
      ],
      "metadata": {
        "id": "NZsQv5KyY83v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7eabb81-f0d8-4fd1-88ed-20795d227256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auc value: 0.9501915708812261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Node Classification***"
      ],
      "metadata": {
        "id": "-inFtZZQg20T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "  print('Train classifier start!')\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "  new_vector = get_vectors_from_file(embed_file)\n",
        "\n",
        "  for jk in range(0, clf_test_len):\n",
        "    if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "      tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "      # Y.append([(int)(i) for i in tags])\n",
        "      lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "      if len(lli) != 0:\n",
        "        if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "          X.append(jk)\n",
        "          Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "  # This part of the code uses only the X and Y lists created above\n",
        "  mi = {}\n",
        "  ma = {}\n",
        "  li1 = []\n",
        "  li2 = []\n",
        "  with open(f'{parent_path}/Results/CANE/{node_clf_results_file}', 'a') as f:\n",
        "    f.write(f\"{embed_file.split('/')[-1]} \\n\")\n",
        "    print(embed_file.split('/')[-1])\n",
        "    for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "      for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "        clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                        clf=LogisticRegression())\n",
        "\n",
        "        result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "        # Results\n",
        "        li1.append(result['micro'])\n",
        "        li2.append(result['macro'])\n",
        "\n",
        "\n",
        "      mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "      ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "      print(mi)\n",
        "      print(ma)\n",
        "      print()\n",
        "\n",
        "      f.writelines(str(str(mi)+str(ma)))\n",
        "      f.write('\\n')\n",
        "\n",
        "      # Reinitialize the dictionaries and lists\n",
        "      mi = {}\n",
        "      ma = {}\n",
        "      li1 = []\n",
        "      li2 = []\n",
        "\n"
      ],
      "metadata": {
        "id": "O8nCKWP3g6Lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80193f90-7a54-4e22-ccd6-064080467956"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train classifier start!\n",
            "embed_node_clf_graph_YAKE.txt\n",
            "{'0.15-micro': 0.7486308871851041}\n",
            "{'0.15-macro': 0.7434241299869686}\n",
            "\n",
            "{'0.45-micro': 0.7698815566835872}\n",
            "{'0.45-macro': 0.7699308483872019}\n",
            "\n",
            "{'0.75-micro': 0.7839851024208566}\n",
            "{'0.75-macro': 0.7879445450861173}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Run (Multiple executions)***"
      ],
      "metadata": {
        "id": "ggfZeV0G2dgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for gf in ['graph.txt']: #for gf in split_graph_files: # For link prediction. For node classification just use: for gf in ['graph.txt']:\n",
        "    for t, txtf in enumerate(data_text_files):\n",
        "\n",
        "        MAX_LEN = MAX_LENS[t]\n",
        "        print(f'The maximum length is: {MAX_LEN}')\n",
        "\n",
        "        data = dataSet(f'{parent_path}/{txtf}', f'{parent_path}/{gf}')\n",
        "\n",
        "        # Logging the execution details\n",
        "        with open(f'{parent_path}/Results/CANE/{log_file}', 'a') as f:\n",
        "            f.write(f'Processing graph: {gf}, text: {txtf}\\n')\n",
        "\n",
        "        print(f'Processing graph: {gf}, text: {txtf}')\n",
        "\n",
        "        with tf.Graph().as_default():\n",
        "            sess = tf.compat.v1.Session()\n",
        "            with sess.as_default():\n",
        "                model = Model(data.num_vocab, data.num_nodes, rho)\n",
        "                opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
        "                train_op = opt.minimize(model.loss)\n",
        "                sess.run(tf.compat.v1.global_variables_initializer())\n",
        "                #total_time = 0\n",
        "\n",
        "                # Training\n",
        "                start_time = datetime.now()\n",
        "                for epoch in tqdm(range(num_epoch)):\n",
        "                    #start_time = datetime.now()\n",
        "                    loss_epoch = 0\n",
        "                    batches = data.generate_batches()\n",
        "                    num_batch = len(batches)\n",
        "\n",
        "                    for i in range(num_batch):\n",
        "                        batch = batches[i]\n",
        "                        node1, node2, node3 = zip(*batch)\n",
        "                        node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                        text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                        feed_dict = {\n",
        "                            model.Text_a: text1,\n",
        "                            model.Text_b: text2,\n",
        "                            model.Text_neg: text3,\n",
        "                            model.Node_a: node1,\n",
        "                            model.Node_b: node2,\n",
        "                            model.Node_neg: node3\n",
        "                        }\n",
        "\n",
        "                        # Run the graph\n",
        "                        _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "                        loss_epoch += loss_batch\n",
        "\n",
        "                    #end_time = datetime.now()\n",
        "                    #epoch_time = (end_time - start_time).total_seconds()\n",
        "                    #total_time += epoch_time\n",
        "\n",
        "                end_time = datetime.now()\n",
        "                with open(f'{parent_path}/Results/CANE/{log_file}', 'a') as f:\n",
        "                    f.write(f'Time: {((end_time - start_time).total_seconds()) / 60.0}\\n')\n",
        "\n",
        "                print(f'Total Time: {((end_time - start_time).total_seconds()) / 60.0} min')\n",
        "\n",
        "                # Save embeddings with a unique name\n",
        "                #embed_file = f\"{parent_path}/Results/CANE/embed_link_pred_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "                embed_file = f\"{parent_path}/Results/CANE/embed_node_clf_{gf.split('.')[0]}_{txtf.split('.')[0]}.txt\"\n",
        "\n",
        "                with open(embed_file, 'wb') as f:\n",
        "                    batches = data.generate_batches(mode='add')\n",
        "                    num_batch = len(batches)\n",
        "                    embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "                    for i in range(num_batch):\n",
        "                        batch = batches[i]\n",
        "                        node1, node2, node3 = zip(*batch)\n",
        "                        node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                        text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                        feed_dict = {\n",
        "                            model.Text_a: text1,\n",
        "                            model.Text_b: text2,\n",
        "                            model.Text_neg: text3,\n",
        "                            model.Node_a: node1,\n",
        "                            model.Node_b: node2,\n",
        "                            model.Node_neg: node3\n",
        "                        }\n",
        "\n",
        "                        # Fetch embeddings\n",
        "                        convA, convB, TA, TB = sess.run(\n",
        "                            [model.convA, model.convB, model.N_A, model.N_B],\n",
        "                            feed_dict=feed_dict\n",
        "                        )\n",
        "\n",
        "                        for j in range(batch_size):\n",
        "                            em = list(TA[j])\n",
        "                            embed[node1[j]].append(em)\n",
        "                            em = list(TB[j])\n",
        "                            embed[node2[j]].append(em)\n",
        "\n",
        "                    for i in range(data.num_nodes):\n",
        "                        if embed[i]:\n",
        "                            tmp = np.sum(embed[i], axis=0) / len(embed[i]) # np.mean(embed[i], axis=0)\n",
        "                            f.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                        else:\n",
        "                            f.write('\\n'.encode()) # For link prediction\n",
        "                            #f.write((' '.join(map(str, zero_list)) + '\\n').encode()) # For node classification\n",
        "\n",
        "                # Log completion\n",
        "                with open(f'{parent_path}/Results/CANE/{log_file}', 'a') as f:\n",
        "                    f.write(f'Embeddings saved to: {embed_file}\\n')\n",
        "\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "UYbahwK42hhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Node Classification***"
      ],
      "metadata": {
        "id": "5-AtYc-XqbNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_files = [f'{parent_path}/Results/CANE/embed_node_clf_graph_YAKE10.txt',\n",
        "               f'{parent_path}/Results/CANE/embed_node_clf_graph_PositionRank.txt',\n",
        "               f'{parent_path}/Results/CANE/embed_node_clf_graph_PositionRank10.txt']\n",
        "\n",
        "with open(f'{parent_path}/{categories_file}', 'r') as f:\n",
        "  tags = f.readlines() # \"tags\" will be a 2D list. Each sublist will have the form: nodeID     label\n",
        "\n",
        "if train_classifier:\n",
        "\n",
        "  clf_test_len = len(nodes) # The number of nodes will be the same in each run since we're using the whole graph and thus, all of its nodes\n",
        "  print('Train classifier start!')\n",
        "\n",
        "  for ef in embed_files:\n",
        "    X = []\n",
        "    Y = []\n",
        "    new_vector = get_vectors_from_file(ef)\n",
        "\n",
        "    for jk in range(0, clf_test_len):\n",
        "      if str(jk) in nodes: # If the index \"jk\" is a node\n",
        "        tag_list = tags[jk].strip().split() # For node \"jk\", take this info: jk     label\n",
        "        # Y.append([(int)(i) for i in tags])\n",
        "        lli = [str(i) for i in tag_list] # For node \"jk\", lli will contain all of its labels\n",
        "        if len(lli) != 0:\n",
        "          if np.array(new_vector[jk]).any() != np.array(zero_list).any(): # If there is no zero value in the embedding of \"jk\"\n",
        "            X.append(jk)\n",
        "            Y.append(lli[1:][0]) # Take the first label (if there are multiple) of node \"jk\"\n",
        "\n",
        "    # This part of the code uses only the X and Y lists created above\n",
        "    mi = {}\n",
        "    ma = {}\n",
        "    li1 = []\n",
        "    li2 = []\n",
        "    with open(f'{parent_path}/Results/CANE/{node_clf_results_file}', 'a') as f:\n",
        "      f.write(f\"{ef.split('/')[-1]} \\n\")\n",
        "      print(ef.split('/')[-1])\n",
        "      for i in range(0, len(clf_ratio)): # Experiment with each ratio\n",
        "        for j in range(0, clf_num): # clf_num = 5\n",
        "\n",
        "          clf = Classifier(vectors=new_vector, # All node embeddings\n",
        "                          clf=LogisticRegression())\n",
        "\n",
        "          result = clf.split_train_evaluate(X, Y, clf_ratio[i])\n",
        "\n",
        "          # Results\n",
        "          li1.append(result['micro'])\n",
        "          li2.append(result['macro'])\n",
        "\n",
        "\n",
        "        mi[str(str(clf_ratio[i]) + '-micro')] = sum(li1) / clf_num\n",
        "        ma[str(str(clf_ratio[i]) + '-macro')] = sum(li2) / clf_num\n",
        "\n",
        "        print(mi)\n",
        "        print(ma)\n",
        "        print()\n",
        "\n",
        "        f.writelines(str(str(mi)+str(ma)))\n",
        "        f.write('\\n')\n",
        "\n",
        "        # Reinitialize the dictionaries and lists\n",
        "        mi = {}\n",
        "        ma = {}\n",
        "        li1 = []\n",
        "        li2 = []\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "58U9nvtqqfbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Link Prediction***"
      ],
      "metadata": {
        "id": "xJaVLkwkqU9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_files = [[f'{parent_path}/Results/CANE/embed_link_pred_sgraph15_RAKE5.txt']]\n",
        "\n",
        "# Initialize a log file to store the AUC results\n",
        "with open(f'{parent_path}/Results/CANE/{link_pred_results_file}', \"a\") as f:\n",
        "    f.write(\"Embed File\\tAUC Value\\n\")\n",
        "\n",
        "for tgfi, tgf in enumerate(test_graph_files):\n",
        "  for ef in embed_files[tgfi]:\n",
        "      node2vec = {}\n",
        "\n",
        "      # Load the embeddings from the current embed file\n",
        "      with open(ef, 'rb') as f:\n",
        "          for i, j in enumerate(f):\n",
        "              if j.decode().strip():\n",
        "                  node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "\n",
        "      # Load the edges from the test graph file\n",
        "      with open(f'{parent_path}/{tgf}', 'rb') as f:\n",
        "          edges = [list(map(int, i.strip().decode().split())) for i in f]\n",
        "\n",
        "      nodes = list(set([i for j in edges for i in j]))\n",
        "\n",
        "      # Calculate AUC\n",
        "      a = 0\n",
        "      b = 0\n",
        "      for i, j in edges:\n",
        "          if i in node2vec.keys() and j in node2vec.keys():\n",
        "              dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "              random_node = random.sample(nodes, 1)[0]\n",
        "              while random_node == j or random_node not in node2vec.keys():\n",
        "                  random_node = random.sample(nodes, 1)[0]\n",
        "              dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "              if dot1 > dot2:\n",
        "                  a += 1\n",
        "              elif dot1 == dot2:\n",
        "                  a += 0.5\n",
        "              b += 1\n",
        "\n",
        "      auc_value = float(a) / b if b > 0 else 0\n",
        "      print(f\"AUC value for {ef.split('/')[-1]}: {auc_value}\")\n",
        "\n",
        "      # Log the result\n",
        "      with open(f'{parent_path}/Results/CANE/{link_pred_results_file}', \"a\") as f:\n",
        "          f.write(f\"{ef}\\t{tgf}\\t{auc_value}\\n\")\n",
        "\n",
        "      gc.collect()"
      ],
      "metadata": {
        "id": "4FNJ9KkL3xf0",
        "outputId": "27b00a2c-d793-4658-c444-0fb2a4cc6ad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC value for /content/datasets/arxiv/embed_sgraph15.txt_RAKE5.txt.txt: 0.9268285931769428\n",
            "AUC value for /content/datasets/arxiv/embed_sgraph15.txt_data.txt.txt: 0.929254030862916\n"
          ]
        }
      ]
    }
  ]
}