{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeM2000/CANE/blob/master/code/Automatic_Keyword_Extraction_for_Citation_Graphs_with_KeyBERT_and_KeyLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Aco3hDVCVYC"
      },
      "source": [
        "# ***Libraries & Tools***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "hT56J_etCxYV",
        "outputId": "ac5b9962-2e7a-46d0-8fca-f8d572c35d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.9.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (3.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (4.12.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.5.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.8.30)\n",
            "Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: keybert\n",
            "Successfully installed keybert-0.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "GfVXEyuiEOx_",
        "outputId": "9a169ff2-3135-4bca-875f-db324fd2d94f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1Ig5Z8AdCVYE",
        "outputId": "53ca6928-f775-4acd-b74e-d63684638eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "import openai\n",
        "\n",
        "from keybert import KeyBERT\n",
        "from keybert import KeyLLM\n",
        "from keybert.llm import OpenAI\n",
        "from keybert.llm import TextGeneration\n",
        "from keybert.llm import LiteLLM\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "import bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgPBxfLACVYF"
      },
      "outputs": [],
      "source": [
        "nltk.data.path.append(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk40T4VOCVYF"
      },
      "source": [
        "# ***Abstracts Retrieval***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3DEF6dl-CVYF",
        "outputId": "b0bb5bda-465d-4457-d961-f6fc18725ff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of extracted abstracts: 2277\n"
          ]
        }
      ],
      "source": [
        "def extract_abstracts(file_path):\n",
        "    # Read the contents of the file\n",
        "    with open(file_path, 'r') as file:\n",
        "        abstracts = file.readlines()\n",
        "\n",
        "   # Remove any leading or trailing whitespace characters from each line\n",
        "    abstracts = [abstract.strip() for abstract in abstracts if abstract.strip()]\n",
        "\n",
        "    # Track the number of abstracts\n",
        "    num_abstracts = len(abstracts)\n",
        "\n",
        "    return abstracts, num_abstracts\n",
        "\n",
        "# Example usage\n",
        "file_path = 'cora/data.txt'\n",
        "abstracts, num_abstracts = extract_abstracts(file_path)\n",
        "\n",
        "# Display the number of extracted abstracts\n",
        "print(f'Number of extracted abstracts: {num_abstracts}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jl6JmWU6CVYG",
        "outputId": "9c702742-02b6-4ae7-c8dc-8e868329f815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Several computer algorithms discovering patterns groups protein sequences use based fitting parameters statistical model group related sequences These include hidden Markov model HMM algorithms multiple sequence alignment MEME Gibbs sampler algorithms discovering motifs These algorithms sometimes prone producing models incorrect two patterns combined The statistical model produced situation convex combination weighted average two different models This paper presents solution problem convex combinations form heuristic based using extremely low variance Dirichlet mixture priors part statistical model This heuristic call megaprior heuristic increases strength ie decreases variance prior proportion size sequence dataset This causes column final model strongly resemble mean single component prior regardless size dataset We describe cause convex combination problem analyze mathematically motivate describe implementation megaprior heuristic show effectively eliminate problem convex combinations protein sequence pattern discovery',\n",
              " 'This paper describes preliminary work aims apply learning strategies medical followup study An investigation application three machine learning algorithmsR FOIL InductH identify risk factors govern colposuspension cure rate made The goal study induce generalised description explanation classification attribute colposuspension cure rate completely cured improved unchanged worse examples questionnaires We looked set rules described risk factors result differences cure rate The results encouraging indicate machine learning play useful role large scale medical problem solving',\n",
              " 'In cellular telephone systems important problem dynamically allocate communication resource channels maximize service stochastic caller environment This problem naturally formulated dynamic programming problem use reinforcement learning RL method find dynamic channel allocation policies better previous heuristic solutions The policies obtained perform well broad variety call traffic patterns We present results large cellular system In cellular communication systems important problem allocate communication resource bandwidth maximize service provided set mobile callers whose demand service changes stochastically A given geographical area divided mutually disjoint cells cell serves calls within boundaries see Figure The total system bandwidth divided channels channel centered around frequency Each channel used simultaneously different cells provided cells sufficiently separated spatially interference The minimum separation distance simultaneous reuse channel called channel reuse constraint When call requests service given cell either free channel one violate channel reuse constraint may assigned call else call blocked system happen free channel found Also mobile caller crosses one cell another call handed cell entry new free channel provided call new cell If channel available call must droppeddisconnected system One objective channel allocation policy allocate available channels calls number blocked calls minimized An additional objective minimize number calls dropped handed busy cell These two objectives must weighted appropriately reflect relative importance since dropping existing calls generally undesirable blocking new calls approximately states',\n",
              " 'In paper bring techniques operations research bear problem choosing optimal actions partially observable stochastic domains We begin introducing theory Markov decision processes mdps partially observable mdps pomdps We outline novel algorithm solving pomdps line show cases finitememory controller extracted solution pomdp We conclude discussion approach relates previous work complexity finding exact solutions pomdps possibilities finding approximate solutions',\n",
              " 'Graphical models enhance representational power probability models qualitative characterization properties This also leads greater efficiency terms computational algorithms empower representations The increasing complexity models however quickly renders exact probabilistic calculations infeasible We propose principled framework approximating graphical models based variational methods We develop variational techniques perspective unifies expands applicability graphical models These methods allow recursive computation upper lower bounds quantities interest Such bounds yield considerably information mere approximations provide inherent error metric tailoring approximations individually cases considered These desirable properties concomitant variational methods unlikely arise result deterministic stochastic approximations',\n",
              " 'Realtime Decision algorithms class incremental resourcebounded Horvitz anytime Dean algorithms evaluating influence diagrams We present test domain realtime decision algorithms results experiments several Realtime Decision Algorithms domain The results demonstrate high performance two algorithms decisionevaluation variant Incremental Probabilisitic Inference DAmbrosio variant algorithm suggested Goldszmidt Goldszmidt PKreduced We discuss implications experimental results explore broader applicability algorithms',\n",
              " 'Speedup learning seeks improve computational efficiency problem solving experience In paper develop formal framework learning efficient problem solving random problems solutions We apply framework two different representations learned knowledge namely control rules macrooperators prove theorems identify sufficient conditions learning representation Our proofs constructive accompanied learning algorithms Our framework captures empirical explanationbased speedup learning unified fashion We illustrate framework implementations two domains symbolic integration Eight Puzzle This work integrates many strands experimental theoretical work machine learning including empirical learning control rules macrooperator learning',\n",
              " 'In previous paper SM showed finite automata could used define objective functions assessing quality alignment two sequences In paper show results using cost functions We also show extend Hischbergs linear space algorithm Hir setting thus generalizing result Myers Miller MMb',\n",
              " 'We present offline variant mistakebound model learning Just like well studied online model learner offline model learn unknown concept sequence elements instance space makes guess test trials In models aim learner make mistakes possible The difference models online model set possible elements known offline model sequence elements ie identity elements well order presented known learner advance We give combinatorial characterization number mistakes offline model We apply characterization solve several natural questions arise new model First compare mistake bounds offline learner learner learning concept classes online scenario We show number mistakes online learning log n factor offline learning n length sequence In addition show offline algorithm make constant number mistakes sequence online algorithm also make constant number mistakes The second issue address effect ordering elements number mistakes offline learner It turns sequences offline learner guarantee one mistake yet permutation sequence forces err many elements We prove however gap offline mistake bounds permutations sequence nmany elements larger multiplicative factor log n present examples obtain gap',\n",
              " 'Wahba Wang Gu Klein Klein introduced Smoothing Spline ANalysis VAriance SS ANOVA method data exponential families Based RKPACK fits SS ANOVA models Gaussian data introduce GRKPACK collection Fortran subroutines binary binomial Poisson Gamma data We also show calculate Bayesian confidence intervals SS ANOVA estimates',\n",
              " 'This paper presents evolutionary approach incremental approach find learning rules several supervised learning tasks In evolutionary approach potential solutions represented variable length mathematical LISP S expressions Thus similar Genetic Programming GP employs fixed set nonproblem specific functions solve variety problems The model tested three Monks parity problems The results indicate usefulness encoding schema discovering learning rules simple supervised learning problems However hard learning problems require special attention terms need larger size codings potential solutions ability generalisation testing set In order find better solutions issues hill climbing strategy incremental coding potential solutions used discovering learning rules problems It found strategy larger solutions easily coded less computational effort Although better performance achieved training hard learning problems ability generalisation testing cases observed poor',\n",
              " 'The key quantity needed Bayesian hypothesis testing model selection marginal likelihood model also known integrated likelihood marginal probability data In paper describe way use posterior simulation output estimate marginal likelihoods We describe basic LaplaceMetropolis estimator models without random effects For models random effects compound LaplaceMetropolis estimator introduced This estimator applied data World Fertility Survey shown give accurate results Batching simulation output used assess uncertainty involved using compound LaplaceMetropolis estimator The method allows us test effects independent variables random effects model also test presence random effects',\n",
              " 'The overfit problem empirical learning utility problem explanationbased learning describe similar phenomenon degradation performance due increase amount learned knowledge Plotting performance learned knowledge course learning performance response reveals common trend several learning methods Modeling trend allows control system constrain amount learned knowledge achieve peak performance avoid general utility problem Experiments evaluate particular empirical model trend analysis learners derive several formal models If evidence suggests general utility problem modeled using mechanisms different learning paradigms model serves unify paradigms one framework capable comparing selecting different learning methods based predicted achievable performance',\n",
              " 'Hidden Markov Models HMMs applied problems statistical modeling database searching multiple sequence alignment protein families protein domains These methods demonstrated globin family protein kinase catalytic domain EFhand calcium binding motif In case parameters HMM estimated training set unaligned sequences After HMM built used obtain multiple alignment training sequences It also used search SWISSPROT database sequences members given protein family contain given domain The HMM produces multiple alignments good quality agree closely alignments produced programs incorporate threedimensional structural information When employed discrimination tests examining closely sequences database fit globin kinase EFhand HMMs HMM able distinguish members families nonmembers high degree accuracy Both HMM PROFILESEARCH technique used search relationships protein sequence multiply aligned sequences perform better tests PROSITE dictionary sites patterns proteins The HMM appears slight advantage',\n",
              " 'This paper explores effect initial weight selection feedforward networks learning simple functions backpropagation technique We first demonstrate use Monte Carlo techniques magnitude initial condition vector weight space significant parameter convergence time variability In order understand result additional deterministic experiments performed The results experiments demonstrate extreme sensitivity back propagation initial weight configuration',\n",
              " 'Some forms memory rely temporarily system brain structures located medial temporal lobe includes hippocampus The recall recent events one task relies crucially proper functioning system As event becomes less recent medial temporal lobe becomes less critical recall event recollection appears rely upon neocortex It proposed process called consolidation responsible transfer memory medial temporal lobe neocortex We examine network model proposed P Alvarez L Squire designed incorporate known features consolidation propose several possible experiments intended help evaluate performance model realistic conditions Finally implement extended version model accommodate varying assumptions number areas connections within brain memory capacity examine performance model Alvarez Squires original task',\n",
              " 'The map eye brain vertebrates topographic ie neighbouring points eye map neighbouring points brain In addition two eyes innervate target structure two sets fibres segregate form ocular dominance stripes Experimental evidence frog goldfish suggests two phenomena may subserved mechanisms We present computational model addresses formation topography ocular dominance The model based form competitive learning subtractive enforcement weight normalization rule Inputs model distributed patterns activity presented simultaneously eyes An important aspect model ocular dominance segregation occur two eyes positively correlated whereas previous models tended assume zero negative correlations eyes This allows investigation dependence pattern stripes degree correlation eyes find increasing correlation leads narrower stripes Experiments suggested test prediction',\n",
              " 'We examine methods estimate average variance test error rates set classifiers We begin process drawing classifier random example Given validation data average test error rate estimated validating single classifier Given test example inputs variance computed exactly Next consider process drawing classifier random using examples Once expected test error rate validated validating single classifier However variance must estimated validating classifers yields loose uncertain bounds',\n",
              " 'We consider formal models learning noisy data Specifically focus learning probability approximately correct model defined Valiant Two widely studied models noise setting classification noise malicious errors However realistic model combining two types noise formalized We define learning environment based natural combination two noise models We first show hypothesis testing possible model We next describe simple technique learning model describe powerful technique based statistical query learning We show noise tolerance improved technique roughly optimal respect desired learning accuracy provides smooth tradeoff tolerable amounts two types noise Finally show statistical query simulation yields learning algorithms combinations noise models thus demonstrating statistical query specification truly An important goal research machine learning determine tasks automated determine information computation requirements One way answer questions development investigation formal models machine learning capture task learning plausible assumptions In work consider formal model learning examples called probably approximately correct PAC learning defined Valiant Val In setting learner attempts approximate unknown target concept simply viewing positive negative examples concept An adversary chooses specified function class hidden f gvalued target function defined specified domain examples chooses probability distribution domain The goal learner output polynomial time high probability hypothesis close target function respect distribution examples The learner gains information target function distribution interacting example oracle At request learner oracle draws example randomly according hidden distribution labels according hidden target function returns labelled example learner A class functions F said PAC learnable captures generic fault tolerance learning algorithm',\n",
              " 'We present decision tree based approach function approximation reinforcement learning We compare approach table lookup neural network function approximator three problems well known mountain car pole balance problems well simulated automobile race car We find decision tree provide better learning performance neural network function approximation solve large problems infeasible using table lookup',\n",
              " 'An approach develop new game playing strategies based artificial evolution neural networks presented Evolution directed discover strategies Othello randommoving opponent later fffi search program The networks discovered first standard positional strategy subsequently mobility strategy advanced strategy rarely seen outside tournaments The latter discovery demonstrates evolutionary neural networks develop novel solutions turning initial disadvantage advantage changed environment',\n",
              " 'We derive general bounds complexity learning Statistical Query model PAC model classification noise We considering problem boosting accuracy weak learning algorithms fall within Statistical Query model This new model introduced Kearns provide general framework efficient PAC learning presence classification noise We first show general scheme boosting accuracy weak SQ learning algorithms proving weak SQ learning equivalent strong SQ learning The boosting efficient used show main result first general upper bounds complexity strong SQ learning Specifically derive simultaneous upper bounds respect number queries Olog VapnikChervonenkis dimension query space Olog inverse minimum tolerance O log In addition show general upper bounds nearly optimal describing class learning problems simultaneously lower bound number queries log We apply boosting results SQ model learning PAC model classification noise Since nearly PAC learning algorithms cast SQ model apply boosting techniques convert PAC algorithms highly efficient SQ algorithms By simulating efficient SQ algorithms PAC model classification noise show nearly PAC algorithms converted highly efficient PAC algorithms tolerate classification noise We give upper bound sample complexity noisetolerant PAC algorithms nearly optimal respect noise rate We also give upper bounds space complexity hypothesis size show two measures fact independent noise rate We note running times noisetolerant PAC algorithms efficient This sequence simulations also demonstrates possible boost accuracy nearly PAC algorithms even presence noise This provides partial answer open problem Schapire first theoretical evidence empirical result Drucker Schapire Simard',\n",
              " 'The tremendous current effort propose neurally inspired methods computation forces closer scrutiny real world application potential models This paper categorizes applications classes particularly discusses features applications make efficiently amenable neural network methods Computational machines deterministic mappings inputs outputs many computational mechanisms proposed problem solutions Neural network features include parallel execution adaptive learning generalization fault tolerance Often much effort given model applications already implemented much efficient way alternate technology Neural networks potentially powerful devices many classes applications However proposed class applications neural networks efficient large commonly occurring nature Comparison supervised unsupervised generalizing systems also included',\n",
              " 'Subjectivism become dominant philosophical foundation Bayesian inference Yet practice Bayesian analyses performed socalled noninformative priors priors constructed formal rule We review plethora techniques constructing priors discuss practical philosophical issues arise used We give special emphasis Jeffreyss rules discuss evolution point view interpretation priors away unique representation ignorance toward notion chosen convention We conclude problems raised research priors chosen formal rules serious may dismissed lightly sample sizes small relative number parameters estimated dangerous put faith default solution asymptotics take Jeffreyss rules variants remain reasonable choices We also provide annotated bibliography fl Robert E Kass Professor Larry Wasserman Associate Professor Department Statistics Carnegie Mellon University Pittsburgh Pennsylvania The work authors supported NSF grant DMS NIH grant RCA The authors thank Nick Polson helping annotations Jim Berger Teddy Seidenfeld Arnold Zellner useful comments discussion',\n",
              " 'Recurrent neural networks become popular models system identification time series prediction NARX Nonlinear AutoRegressive models eXogenous inputs neural network models popular subclass recurrent networks used many applications Though embedded memory found recurrent network models particularly prominent NARX models We show using intelligent memory order selection pruning good initial heuristics significantly improves generalization predictive performance nonlinear systems problems diverse grammatical inference time series prediction',\n",
              " 'This paper presents incremental concept learning approach identiflcation concepts high overall accuracy The main idea address concept overlap central problem learning multiple descriptions Many traditional inductive algorithms disjunctive version space family considered face problem The approach focuses combinations confldent possibly overlapping concepts original stochastic complexity formula The focusing ecient organized simulated annealingbased beam search The experiments show approach especially suitable developing incremental learning algorithms following advantages flrst generates highly accurate concepts second overcomes certain degree sensitivity order examples third handles noisy examples',\n",
              " 'Casebased reasoning CBR great deal offer supporting creative design particularly processes rely heavily previous design experience framing problem evaluating design alternatives However existing CBR systems living potential They tend adapt reuse old solutions routine ways producing robust uninspired results Little research effort directed towards kinds situation assessment evaluation assimilation processes facilitate exploration ideas elaboration redefinition problems crucial creative design Also typically rigid control structures facilitate kinds strategic control opportunism inherent creative reasoning In paper describe types behavior would like casebased design systems support based study designers working mechanical engineering problem We show standard CBR framework extended describe architecture developing experiment ideas',\n",
              " 'In paper present framework building probabilistic automata parameterized contextdependent probabilities Gibbs distributions used model state transitions output generation parameter estimation carried using EM algorithm Mstep uses generalized iterative scaling procedure We discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology',\n",
              " 'One characteristics design designers rely extensively past experience order create new designs Because memorybased techniques artificial intelligence help store organise retrieve reuse experiential knowledge held memory good candidates aiding designers Another characteristic design phenomenon exploration early stages design configuration A designer begins illstructured partially defined problem specification process exploration gradually refines modifies hisher understanding problem improves In paper describe demex interactive computeraided design system employs memorybased techniques help users explore design problems pose system order acquire better understanding requirements problems demex applied domain structural design buildings',\n",
              " 'Uppropagation algorithm inverting learning neural network generative models Sensory input processed inverting model generates patterns hidden variables using topdown connections The inversion process iterative utilizing negative feedback loop depends error signal propagated bottomup connections The error signal also used learn generative model examples The algorithm benchmarked principal component analysis In doctrine unconscious inference Helmholtz argued perceptions formed interaction bottomup sensory data topdown expectations According one interpretation doctrine perception procedure sequential hypothesis testing We propose new algorithm called uppropagation realizes interpretation layered neural networks It uses topdown connections generate hypotheses bottomup connections revise It important understand difference uppropagation ancestor backpropagation algorithm Backpropagation learning algorithm recognition models As shown Figure bottomup connections recognize patterns topdown connections propagate error signal used learn recognition model In contrast uppropagation algorithm inverting learning generative models shown Figure b Topdown connections generate patterns set hidden variables Sensory input processed inverting generative model recovering hidden variables could generated sensory data This operation called either pattern recognition pattern analysis depending meaning hidden variables Inversion generative model done iteratively negative feedback loop driven error signal bottomup connections The error signal also used learning connections experiments images handwritten digits',\n",
              " 'This paper demonstrates exploitation certain vision processing techniques index case base surfaces The surfaces result reinforcement learning represent optimum choice actions achieve goal anywhere state space This paper shows strong features occur interaction system environment detected early learning process Such features allow system identify identical similar task solved previously retrieve relevant surface This results orders magnitude increase learning rate',\n",
              " 'Combining different machine learning algorithms system produce benefits beyond either method could achieve alone This paper demonstrates genetic algorithms used conjunction lazy learning solve examples difficult class delayed reinforcement learning problems better either method alone This class class differential games includes numerous important control problems arise robotics planning game playing areas solutions differential games suggest solution strategies general class planning control problems We conducted series experiments applying three learning approacheslazy Qlearning knearest neighbor kNN genetic algorithmto particular differential game called pursuit game Our experiments demonstrate kNN great difficulty solving problem lazy version Qlearning performed moderately well genetic algorithm performed even better These results motivated next step experiments hypothesized kNN difficulty good examplesa common source difficulty lazy learning Therefore used genetic algorithm bootstrapping method kNN create system provide examples Our experiments demonstrate resulting joint system learned solve pursuit games high degree accuracyoutperforming either method aloneand relatively small memory requirements',\n",
              " 'We describe hierarchical generative model viewed nonlinear generalization factor analysis implemented neural network The model uses bottomup topdown lateral connections perform Bayesian perceptual inference correctly Once perceptual inference performed connection strengths updated using simple learning rule requires locally available information We demon strate network learns extract sparse distributed hierarchical representations',\n",
              " 'In applications neuroevolution individual population represents complete neural network Recent work SANE system however demonstrated evolving individual neurons often produces efficient genetic search This paper demonstrates SANE solve easy tasks quickly often stalls larger problems A hierarchical approach neuroevolution presented overcomes SANEs difficulties integrating neuronlevel exploratory search networklevel exploitive search In robot arm manipulation task hierarchical approach outperforms neuronbased search networkbased search',\n",
              " 'Reinforcement learning addresses problem learning select actions order maximize ones performance unknown environments To scale reinforcement learning complex realworld tasks typically studied AI one must ultimately able discover structure world order abstract away myriad details operate tractable problem spaces This paper presents SKILLS algorithm SKILLS discovers skills partially defined action policies arise context multiple related tasks Skills collapse whole action sequences single operators They learned minimizing compactness action policies using description length argument representation Empirical results simple grid navigation tasks illustrate successful discovery structure reinforcement learning',\n",
              " 'We consider generalization mistakebound model learning f gvalued functions learner must satisfy general constraint number M incorrect predictions number M incorrect predictions We describe generalpurpose optimal algorithm formulation problem We describe several applications general results involving situations learner wishes satisfy linear inequalities M M',\n",
              " 'A critical issue users Markov Chain Monte Carlo MCMC methods applications determine safe stop sampling use samples estimate characteristics distribution interest Research methods computing theoretical convergence bounds holds promise future currently yielded relatively little practical use applied work Consequently MCMC users address convergence problem applying diagnostic tools output produced running samplers After giving brief overview area provide expository review thirteen convergence diagnostics describing theoretical basis practical implementation We compare performance two simple models conclude methods fail detect sorts convergence failure designed identify We thus recommend combination strategies aimed evaluating accelerating MCMC sampler convergence including applying diagnostic procedures small number parallel chains monitoring autocorrelations crosscorrelations modifying parameterizations sampling algorithms appropriately We emphasize however possible say certainty finite sample MCMC algorithm representative underlying stationary distribution Mary Kathryn Cowles Assistant Professor Biostatistics Harvard School Public Health Boston MA Bradley P Carlin Associate Professor Division Biostatistics School Public Health University Minnesota Minneapolis MN Much work done first author graduate student Divison Biostatistics University Minnesota Assistant Professor Biostatistics Section Department Preventive Societal Medicine University Nebraska Medical Center Omaha NE The work authors supported part National Institute Allergy Infectious Diseases FIRST Award RAI The authors thank developers diagnostics studied sharing insights experiences software Drs Thomas Louis Luke Tierney helpful discussions suggestions greatly improved manuscript',\n",
              " 'Although detection invariant structure given set input patterns vital many recognition tasks connectionist learning rules tend focus directions high variance principal components The prediction paradigm often used reconcile dichotomy suggest direct approach invariant learning based antiHebbian learning rule An unsupervised twolayer network implementing method competitive setting learns extract coherent depth information randomdot stereograms',\n",
              " 'Instancebased learning methods explicitly remember data receive They usually training phase prediction time perform computation Then take query search database similar datapoints build online local model local average local regression predict output value In paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large We present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning Earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure Our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously This permits us query database exibility conventional linear search greatly reduced computational cost',\n",
              " 'Discrete Bayesian models used model uncertainty mobilerobot navigation question actions chosen remains largely unexplored This paper presents optimal solution problem formulated partially observable Markov decision process Since solving optimal control policy intractable general goes explore variety heuristic control strategies The control strategies compared experimentally simulation runs robot',\n",
              " 'This NRL NCARAI technical note AIC describes work Salzbergs NGE I recently implemented algorithm run case studies The purpose note publicize implementation note curious result using This implementation NGE available WWW address',\n",
              " 'Technical Report No Department Statistics University Toronto Abstract I present new Markov chain sampling method appropriate distributions isolated modes Like recentlydeveloped method simulated tempering tempered transition method uses series distributions interpolate distribution interest distribution sampling easier The new method advantage require approximate values normalizing constants distributions needed simulated tempering tedious estimate Simulated tempering performs random walk along series distributions used In contrast tempered transitions new method move systematically desired distribution easilysampled distribution back desired distribution This systematic movement avoids inefficiency random walk advantage unfortunately cancelled increase number interpolating distributions required Because sampling efficiency tempered transition method simple problems similar simulated tempering On complex distributions however simulated tempering tempered transitions may perform differently Which better depends ways interpolating distributions deceptive',\n",
              " 'We describe ongoing project develop adaptive training system ATS dynamically models students learning processes provide specialized tutoring adapted students knowledge state learning style The student modeling component ATS MLModeler uses machine learning ML techniques emulate students novicetoexpert transition MLModeler infers learning methods student used reach current knowledge state comparing students solution trace expert solution generating plausible hypotheses misconceptions errors student made A casebased approach used generate hypotheses incorrectly applying analogy overgeneralization overspecialization The student expert models use networkbased representation includes abstract concepts relationships well strategies problem solving Fuzzy methods used represent uncertainty student model This paper describes design ATS MLModeler gives detailed example system would model tutor student typical session The domain use example highschool level chemistry',\n",
              " 'Metacognition addresses issues knowledge cognition regulating cognition We argue regulation process improved growing experience Therefore mental models needed facilitate reuse previous regulation processes We satisfy requirement describing casebased approach Introspection Planning utilises previous experience obtained reasoning metalevel object level The introspection plans used approach support various metacognitive tasks identified generation selfquestions As example introspection planning metacognitive behaviour system IULIAN described',\n",
              " 'Graphical Markov models use graphs either undirected directed mixed represent possible dependences among statistical variables Applications undirected graphs UDGs include models spatial dependence image analysis acyclic directed graphs ADGs especially convenient statistical analysis arise fields genetics psychometrics models expert systems Bayesian belief networks Lauritzen Wermuth Frydenberg LWF introduced Markov property chain graphs mixed graphs used represent simultaneously causal associative dependencies include UDGs ADGs special cases In paper alternative Markov property AMP chain graphs introduced ways direct extension ADG Markov property LWF property chain graph',\n",
              " 'The fault hierarchy representation widely used expert systems diagnosis complex mechanical devices This paper describes theory revision algorithm revises fault hierarchies This task presents several challenges typical training instances missing feature values pattern missing features significant rather merely effect noise quality candidate theory depends correctness diagnoses returns set tests uses reach diagnoses This paper describes addresses challenges reports experiments use improve performance two fielded diagnostic systems fl This extended version paper appeared Proceedings Fifth International Workshop Principles Diagnosis Dx New York October We gratefully acknowledge receiving helpful comments CheoungNam Lee Glenn Meredith Chandra Mouleeswaran z Current address Robotics Laboratory Computer Science Department Stanford University Stanford CA email langleyflamingostanfordedu phone fax',\n",
              " 'Machine learning game strategies often depended competitive methods continually develop new strategies capable defeating previous ones We use inclusive definition game consider framework within competitive algorithm makes repeated use strategy learning component learn strategies defeat given set opponents We describe game learning terms sets H X first second player strategies connect model familiar models concept learning We show importance ideas teaching set specification number k new context The performance several competitive algorithms investigated using worstcase randomized strategy learning algorithms Our central result Theorem competitive algorithm solves games total number strategies polynomial lgjHj lgjX j k Its use demonstrated including application concept learning new kind counterexample oracle We conclude complexity analysis game learning list number new questions arising work',\n",
              " 'Most work attempts give bounds generalization error hypothesis generated learning algorithm based methods theory uniform convergence These bounds apriori bounds hold distribution examples calculated data observed In paper propose different approach bounding generalization error data observed A selfbounding learning algorithm algorithm addition hypothesis outputs outputs reliable upper bound generalization error hypothesis We first explore idea statistical query learning framework Kearns After give explicit self bounding algorithm learning algorithms based local search',\n",
              " 'In paper propose new framework studying Markov decision processes MDPs based ideas statistical mechanics The goal learning MDPs find policy yields maximum expected return time In choosing policies agents must therefore weigh prospects shortterm versus longterm gains We study simple MDP agent must constantly decide exploratory jumps local reward mining state space The number policies choose grows exponentially size state space N We view expected returns defining energy landscape policy space Methods statistical mechanics used analyze landscape thermodynamic limit N We calculate overall distribution expected returns well distribution returns policies fixed Hamming distance optimal one We briefly discuss problem learning optimal policies empirical estimates expected return As first step relate findings entropy limit hightemperature learning Numerical simulations support theoretical results',\n",
              " 'This paper shows neural networks use continuous activation functions VC dimension least large square number weights w This result settles longstanding open question namely whether wellknown Ow log w bound known hardthreshold nets also held general sigmoidal nets Implications number samples needed valid generalization discussed',\n",
              " 'Novel online learning algorithms self adaptive learning rates parameters blind separation signals proposed The main motivation development new learning rules improve convergence speed reduce crosstalking especially nonstationary signals Furthermore discovered conditions proposed neural network models associated learning algorithms exhibit random switch attention ie ability chaotic random switching crossover output signals way specified separated signal may appear various outputs different time windows Validity performance dynamic properties proposed learning algorithms investigated computer simulation experiments',\n",
              " 'I present modular network architecture learning algorithm based incremental dynamic programming allows single learning agent learn solve multiple Markovian decision tasks MDTs significant transfer learning across tasks I consider class MDTs called composite tasks formed temporally concatenating number simpler elemental MDTs The architecture trained set composite elemental MDTs The temporal structure composite task assumed unknown architecture learns produce temporal decomposition It shown certain conditions solution composite MDT constructed computationally inexpensive modifications solutions constituent elemental MDTs',\n",
              " 'Scientists engineers face recurring problems constructing testing modifying numerical simulation programs The process coding revising simulators extremely timeconsuming almost always written conventional programming languages Scientists engineers therefore benefit software facilitates construction programs simulating physical systems Our research adapts methodology deductive program synthesis problem constructing numerical simulation codes We focused simulators represented second order functional programs composed numerical integration root extraction routines We developed system uses first order Horn logic synthesize numerical simulators built components Our approach based two ideas First axiomatize relationship integration differentiation We neither attempt require complete axiomatization mathematical analysis Second system uses representation functions reified objects Function objects encoded lambda expressions Our knowledge base includes axiomatization term equality lambda calculus It also includes axioms defining semantics numerical integration root extraction routines We use depth bounded SLD resolution construct proofs synthesize programs Our system successfully constructed numerical simulators computational design jet engine nozzles sailing yachts among others Our results demonstrate deductive synthesis techniques used construct numerical simulation programs realistic applications Ellman Murata Automatic design optimization highly sensitive problem formulation The choice objective function constraints design parameters dramatically impact computational cost optimization quality resulting design The best formulation varies one application another A design engineer usually know best formulation advance In order address problem developed system supports interactive formulation testing reformulation design optimization strategies Our system includes executable dataflow language representing optimization strategies The language allows engineer define multiple stages optimization using different approximations objective constraints different abstractions design space We also developed set transformations reformulate strategies represented language The transformations approximate objective constraint functions abstract reparameterize search spaces divide optimization process multiple stages The system applicable principle design problem expressed terms constrained op',\n",
              " 'Bayesian networks provide language qualitatively representing conditional independence properties distribution This allows natural compact representation distribution eases knowledge acquisition supports effective inference algorithms It wellknown however certain independencies capture qualitatively within Bayesian network structure independencies hold certain contexts ie given specific assignment values certain variables In paper propose formal notion contextspecific independence CSI based regularities conditional probability tables CPTs node We present technique analogous based dseparation determining independence holds given network We focus particular qualitative representation schemetreestructured CPTs capturing CSI We suggest ways representation used support effective inference algorithms In particular present structural decomposition resulting network improve performance clustering algorithms alternative algorithm based cutset conditioning',\n",
              " 'The eligibility trace one basic mechanisms used reinforcement learning handle delayed reward In paper introduce new kind eligibility trace replacing trace analyze theoretically show results faster reliable learning conventional trace Both kinds trace assign credit prior events according recently occurred conventional trace gives greater credit repeated events Our analysis conventional replacetrace versions oine TD algorithm applied undiscounted absorbing Markov chains First show methods converge repeated presentations training set predictions two well known Monte Carlo methods We analyze relative efficiency two Monte Carlo methods We show method corresponding conventional TD biased whereas method corresponding replacetrace TD unbiased In addition show method corresponding replacing traces closely related maximum likelihood solution tasks mean squared error always lower long run Computational results confirm analyses show applicable generally In particular show replacing traces significantly improve performance reduce parameter sensitivity MountainCar task full reinforcementlearning problem continuous state space using featurebased function approximator',\n",
              " 'Reading studied decades variety cognitive disciplines yet theories exist sufficiently describe explain people accomplish complete task reading realworld texts In particular type knowledge intensive reading known creative reading largely ignored past research We argue creative reading aspect practically reading experiences result theory overlooks insufficient We built results psychology artificial intelligence education order produce functional theory complete reading process The overall framework describes set tasks necessary reading performed Within framework developed theory creative reading The theory implemented ISAAC Integrated Story Analysis And Creativity system reading system reads science fiction stories',\n",
              " 'We study problem combining updates special instance theory change counterfactual conditionals propositional knowledgebases Intuitively update means world described knowledgebase changed This opposed revisions another instance theory change knowledge static world changes A counterfactual implication statement form If A case B would also case negation A may derivable current knowledge We present decidable logic called VCU update counterfactual implication connectives object language Our update operator generalization operators previously proposed studied literature We show operator satisfies certain postulates set forth reasonable update The logic VCU extension D K Lewis logic VCU counterfactual conditionals The semantics VCU multimodal propositional calculus based possible worlds The infamous Ramsey Rule becomes derivation rule sound complete axiomatization We show Gardenfors Triviality Theorem impossibility combine theory change counterfactual conditionals via Ramsey Rule hold logic It thus seen Triviality Theorem applies revision operators updates fl A preliminary version paper presented Second International Conference Principles Knowledge Representation Reasoning Cambridge Massachusetts April The work partially performed author visiting Department Computer Science University Toronto',\n",
              " 'Many neural net learning algorithms aim finding simple nets explain training data The expectation simpler networks better generalization test data Occams razor Previous implementations however use measures simplicity lack power universality elegance based Kolmogorov complexity Solomonoffs algorithmic probability Likewise previous approaches especially Bayesian kind suffer problem choosing appropriate priors This paper addresses issues It first reviews basic concepts algorithmic complexity theory relevant machine learning SolomonoffLevin distribution universal prior deals prior problem The universal prior leads probabilistic method finding algorithmically simple problem solutions high generalization capability The method based Levin complexity timebounded generalization Kolmogorov complexity inspired Levins optimal universal search algorithm For given problem solution candidates computed efficient selfsizing programs influence runtime storage size The probabilistic search algorithm finds good programs ones quickly computing algorithmically probable solutions fitting training data Simulations focus task discovering algorithmically simple neural networks low Kolmogorov complexity high generalization capability It demonstrated method least certain toy problems computationally feasible lead generalization results unmatchable previous neural net algorithms Much remains done however make large scale applications incremental learning feasible',\n",
              " 'We studied problem generating expressive musical performances context tenor saxophone interpretations We done several recordings tenor sax playing different Jazz ballads different degrees expressiveness including inexpressive interpretation ballad These recordings analyzed using SMS spectral modeling techniques extract information related several expressive parameters This set parameters scores constitute set cases examples casebased system From set cases system infers set possible expressive transformations given new phrase applying similarity criteria based background musical knowledge new phrase set cases Finally SaxEx applies inferred expressive transformations new phrase using synthesis capabilities SMS',\n",
              " 'One surprising recurring phenomena observed experiments boosting test error generated classifier usually increase size becomes large often observed decrease even training error reaches zero In paper show phenomenon related distribution margins training examples respect generated voting classification rule margin example simply difference number correct votes maximum number votes received incorrect label We show techniques used analysis Vapniks support vector classifiers neural networks small weights applied voting methods relate margin distribution test error We also show theoretically experimentally boosting especially effective increasing margins training examples Finally compare explanation based biasvariance decomposition',\n",
              " 'Realworld learning tasks may involve highdimensional data sets arbitrary patterns missing data In paper present framework based maximum likelihood density estimation learning data sets We use mixture models density estimates make two distinct appeals ExpectationMaximization EM principle Dempster et al deriving learning algorithmEM used estimation mixture components coping missing data The resulting algorithm applicable wide range supervised well unsupervised learning problems Results classification benchmarkthe iris data setare presented',\n",
              " 'The hierarchical feature map system recognizes input story instance particular script classifying three levels scripts tracks role bindings The recognition taxonomy ie breakdown script tracks roles extracted automatically independently script examples script instantiations unsupervised selforganizing process The process resembles human learning differentiation frequently encountered scripts become gradually detailed The resulting structure hierachical pyramid feature maps The hierarchy visualizes taxonomy maps lay topology level The number input lines selforganization time considerably reduced compared ordinary singlelevel feature mapping The system recognize incomplete stories recover missing events The taxonomy also serves memory organization scriptbased episodic memory The maps assign unique memory location script instantiation The salient parts input data separated resources concentrated representing accurately',\n",
              " 'It shown static neural approaches adaptive target detection replaced efficient sequential alternative The latter inspired observation biological systems employ sequential eyemovements pattern recognition A system described builds adaptive model timevarying inputs artificial fovea controlled adaptive neural controller The controller uses adaptive model learning sequential generation fovea trajectories causing fovea move target visual scene The system also learns track moving targets No teacher provides desired activations eyemuscles various times The goal information shape target Since task rewardonlyatgoal task involves complex temporal credit assignment problem Some implications adaptive attentive systems general discussed',\n",
              " 'We present treestructured architecture supervised learning The statistical model underlying architecture hierarchical mixture model mixture coefficients mixture components generalized linear models GLIMs Learning treated maximum likelihood problem particular present ExpectationMaximization EM algorithm adjusting parameters architecture We also develop online learning algorithm parameters updated incrementally Comparative simulation results presented robot dynamics domain We want thank Geoffrey Hinton Tony Robinson Mitsuo Kawato Daniel Wolpert helpful comments manuscript This project supported part grant McDonnellPew Foundation grant ATR Human Information Processing Research Laboratories grant Siemens Corporation grant IRI National Science Foundation grant NJ Office Naval Research The project also supported NSF grant ASC support Center Biological Computational Learning MIT including funds provided DARPA HPCC program NSF grant ECS support Initiative Intelligent Control MIT Michael I Jordan NSF Presidential Young Investigator',\n",
              " 'The EM algorithm performs maximum likelihood estimation data variables unobserved We present function resembles negative free energy show M step maximizes function respect model parameters E step maximizes respect distribution unobserved variables From perspective easy justify incremental variant EM algorithm distribution one unobserved variables recalculated E step This variant shown empirically give faster convergence mixture estimation problem A variant algorithm exploits sparse conditional distributions also described wide range variant algorithms also seen possible',\n",
              " 'A network WilsonCowan oscillators constructed emergent properties synchronization desynchronization investigated computer simulation formal analysis The network twodimensional matrix oscillator coupled neighbors We show analytically chain locally coupled oscillators piecewise linear approximation WilsonCowan oscillator synchronizes present technique rapidly entrain finite numbers oscillators The coupling strengths change fast time scale based Hebbian rule A global separator introduced receives input sends feedback oscillator matrix The global separator used desynchronize different oscillator groups Unlike many models properties network emerge local connections preserve spatial relationships among components critical encoding Gestalt principles feature grouping The ability synchronize desynchronize oscillator groups within network offers promising approach pattern segmentation figureground segregation based oscillatory correlation',\n",
              " 'In paper I describe implementation probabilistic regression model BUGS BUGS program carries Bayesian inference statistical problems using simulation technique known Gibbs sampling It possible implement surprisingly complex regression models environment I demonstrate simultaneous inference interpolant inputdependent noise level',\n",
              " 'Classifying hand complex data coming psychology experiments long difficult task quantity data classify amount training may require One way alleviate problem use machine learning techniques We built classifier based decision trees reproduces classifying process used two humans sample data learns classify unseen data The automatic classifier proved accurate constant much faster classification hand',\n",
              " 'The estimation training methods neural network literature usually simple form gradient descent algorithm suitable implementation hardware using massively parallel computations For ordinary computers massively parallel optimization algorithms several SAS procedures usually far efficient This talk shows fit neural networks using SASOR R fl SASETS R fl SASSTAT R fl software',\n",
              " 'Selecting right reference class right interval faced conflicting candidates possibility establishing subset style dominance problem Kyburgs Evidential Probability system Various methods proposed Loui Kyburg solve problem way intuitively appealing justifiable within Kyburgs framework The scheme proposed paper leads stronger statistical assertions without sacrificing much intuitive appeal Kyburgs latest proposal',\n",
              " 'We apply reinforcement learning methods learn domainspecific heuristics job shop scheduling A repairbased scheduler starts criticalpath schedule incrementally repairs constraint violations goal finding short conflictfree schedule The temporal difference algorithm T D applied train neural network learn heuristic evaluation function states This evaluation function used onestep lookahead search procedure find good solutions new scheduling problems We evaluate approach synthetic problems problems NASA space shuttle payload processing task The evaluation function trained problems involving small number jobs tested larger problems The TD scheduler performs better best known existing algorithm taskZwebens iterative repair method based simulated annealing The results suggest reinforcement learning provide new method constructing highperformance scheduling systems',\n",
              " 'A neural network approach classic inverted pendulum task presented This task task keeping rigid pole hinged cart free fall plane roughly vertical orientation moving cart horizontally plane keeping cart within maximum distance starting position This task constitutes difficult control problem parameters cartpole system known precisely variable It also forms basis even complex controllearning problem controller must learn proper actions successfully balancing pole given current state system failure signal pole angle vertical becomes great cart exceeds one boundaries placed position The approach presented demonstrated effective realtime control small selfcontained minirobot specially outfitted task Origins details learning scheme specifics minirobot hardware results actual learning trials presented',\n",
              " 'Platts resourceallocation network RAN Platt b modified reinforcementlearning paradigm restart existing hidden units rather adding new units After restarting units continue learn via backpropagation The resulting restart algorithm tested Qlearning network learns solve inverted pendulum problem Solutions found faster average restart algorithm without',\n",
              " 'We propose new processing paradigm called Expandable Split Window ESW paradigm exploiting finegrain parallelism This paradigm considers window instructions possibly dependencies single unit exploits finegrain parallelism overlapping execution multiple windows The basic idea connect multiple sequential processors decoupled decentralized manner achieve overall multiple issue This processing paradigm shares number properties restricted dataflow machines derived sequential von Neumann architecture We also present implementation Expandable Split Window execution model preliminary performance results',\n",
              " 'Algorithms based Nested Generalized Exemplar NGE theory Salzberg classify new data points computing distance nearest generalized exemplar ie either point axisparallel rectangle They combine distancebased character nearest neighbor NN classifiers axisparallel rectangle representation employed many rulelearning systems An implementation NGE compared knearest neighbor kNN algorithm domains found significantly inferior kNN Several modifications NGE studied understand cause poor performance These show performance substantially improved preventing NGE creating overlapping rectangles still allowing complete nesting rectangles Performance improved modifying distance metric allow weights features Salzberg Best results obtained study weights computed using mutual information features output class The best version NGE developed batch algorithm BNGE FW MI usertunable parameters BNGE FW MI performance comparable firstnearest neighbor algorithm also incorporating feature weights However knearest neighbor algorithm still significantly superior BNGE FW MI domains inferior We conclude even improvements NGE approach sensitive shape decision boundaries classification problems In domains decision boundaries axisparallel NGE approach produce excellent generalization interpretable hypotheses In domains tested NGE algorithms require much less memory store generalized exemplars required NN algorithms',\n",
              " 'Selecting good model set input points cross validation computationally intensive process especially number possible models number training points high Techniques gradient descent helpful searching space models problems local minima importantly lack distance metric various models reduce applicability search methods Hoeffding Races technique finding good model data quickly discarding bad models concentrating computational effort differentiating better ones This paper focuses special case leaveoneout cross validation applied memorybased learning algorithms also argue applicable class model selection problems',\n",
              " 'In many learning problems learning system presented values features actually irrelevant concept trying learn The FOCUS algorithm due Almuallim Dietterich performs explicit search smallest possible input feature set S permits consistent mapping features S output feature The FOCUS algorithm also seen algorithm learning determinations functional dependencies suggested Another algorithm learning determinations appears The FOCUS algorithm superpolynomial runtime Almuallim Dietterich leave open question tractability underlying problem In paper problem shown NPcomplete We also describe briefly experiments demonstrate benefits determination learning show finding lowestcardinality determinations easier practice finding minimal determi Define MINFEATURES problem follows given set X examples composed binary value specifying value target feature vector binary values specifying values features number n determine whether exists feature set S We show MINFEATURES NPcomplete reducing VERTEXCOVER MINFEATURES The VERTEXCOVER problem may stated question given graph G vertices V edges E subset V V size edge E connected least one vertex V We may reduce instance VERTEXCOVER instance MINFEATURES mapping edge E example X one input feature every vertex V In proof reported result reduction set covering The proof therefore fails show NPcompleteness nations',\n",
              " 'An unsupervised learning algorithm multilayer network stochastic neurons described Bottomup recognition connections convert input representations successive hidden layers topdown generative connections reconstruct representation one layer representation layer In wake phase neurons driven recognition connections generative connections adapted increase probability would reconstruct correct activity vector layer In sleep phase neurons driven generative connections recognition connections adapted increase probability would produce Supervised learning algorithms multilayer neural networks face two problems They require teacher specify desired output network require method communicating error information connections The wakesleep algorithm avoids problems When external teaching signal matched goal required force hidden units extract underlying structure In wakesleep algorithm goal learn representations economical describe allow input reconstructed accurately We quantify goal imagining communication game vector raw sensory inputs communicated receiver first sending hidden representation sending difference input vector topdown reconstruction hidden representation The aim learning minimize description length total number bits would required communicate input vectors way No communication actually takes place minimizing description length would required forces network learn economical representations capture underlying regularities data correct activity vector layer',\n",
              " 'Properly structured software libraries crucial success software reuse Specifically structure software library ought reect functional similarity stored software components order facilitate retrieval process We propose application artificial neural network technology achieve structured library In detail utilize artificial neural network adhering unsupervised learning paradigm The distinctive feature model make semantic relationship stored software components geographically explicit Thus actual user software library gets notion semantic relationship components terms geographical closeness',\n",
              " 'Learning fundamental component intelligence key consideration designing cognitive architectures Soar Laird et al This chapter considers question constitutes appropriate generalpurpose learning mechanism We interested mechanisms might explain reproduce rich variety learning capabilities humans ranging learning perceptualmotor skills ride bicycle learning highly cognitive tasks play chess Research learning fields cognitive science artificial intelligence neurobiology statistics led identification two distinct classes learning methods inductive analytic Inductive methods neural network Backpropagation learn general laws finding statistical correlations regularities among large set training examples In contrast analytical methods ExplanationBased Learning acquire general laws many fewer training examples They rely instead prior knowledge analyze individual training examples detail use analysis distinguish relevant example features irrelevant The question considered chapter best combine inductive analytical learning architecture seeks cover range learning exhibited intelligent systems humans We present specific learning mechanism Explanation Based Neural Network learning EBNN blends two types learning present experimental results demonstrating ability learn control strategies mobile robot using',\n",
              " 'Simulation plays important role stochastic geometry related fields simplest random set models tend intractable analysis Many simulation algorithms deliver approximate samples random set models example simulating equilibrium distribution Markov chain spatial birthanddeath process The samples usually fail exact algorithm simulates Markov chain long finite time thus convergence equilibrium approximate The seminal work Propp Wilson made important contribution simulation proposing coupling method Coupling Past CFTP delivers perfect say exact simulations Markov chains In paper introduce new idea perfect simulation illustrate using two common models stochastic geometry dead leaves model Boolean model conditioned cover finite set points',\n",
              " 'In recent years casebased reasoning demonstrated highly useful problem solving complex domains Also mixed paradigm approaches emerged combining CBR induction techniques aiming verifying knowledge andor building efficient case memory However complex domains induction whole problem space often possible time consuming In paper approach presented owing close interaction CBR part attempts induce rules particular context ie problem solved CBRoriented system These rules may used indexing purposes similarity assessment order support CBR process future',\n",
              " 'This paper demonstrates tandem use finite automata learning algorithm utility planner adversarial robotic domain For many applications robot agents need predict movement objects environment plan avoid When robot reasoning model object machine learning techniques used generate one In project learn DFA model adversarial robot use automaton predict next move adversary The robot agent plans path avoid adversary predicted location fulfilling goal requirements',\n",
              " 'Claudia Cargnoni Dipartimento Statistico Universita di Firenze Firenze Italy Peter Muller Assistant Professor Mike West Professor Institute Statistics Decision Sciences Duke University Durham NC Research Cargnoni performed visiting ISDS Muller West partially supported NSF grant DMS',\n",
              " 'Our theoretical understanding properties genetic algorithms GAs used function optimization GAFOs strong would like Traditional schema analysis provides first order insights doesnt capture nonlinear dynamics GA search process well Markov chain theory used primarily steady state analysis GAs In paper explore use transient Markov chain analysis model understand behavior finite population GAFOs observed transition steady states This approach appears provide new insights circumstances GAFOs perform well Some preliminary results presented initial evaluation merits approach provided',\n",
              " 'In paper consider application training noise multilayer perceptron input variables relevance determination Noise injection modified order penalize irrelevant features The proposed algorithm attractive requires tuning single parameter This parameter controls penalization inputs together complexity model After presentation method experimental evidences given simulated data sets',\n",
              " 'COINS Technical Report January Abstract In paper present new multivariate decision tree algorithm LMDT combines linear machines decision trees LMDT constructs test decision tree training linear machine eliminating irrelevant noisy variables controlled manner To examine LMDTs ability find good generalizations present results variety domains We compare LMDT empirically univariate decision tree algorithm observe multivariate tests appropriate bias given data set LMDT finds small accurate trees',\n",
              " 'Reinforcement learning RL modelfree tuning adaptation method control dynamic systems Contrary supervised learning based usually gradient descent techniques RL require model sensitivity function process Hence RL applied systems poorly understood uncertain nonlinear reasons untractable conventional methods In reinforcement learning overall controller performance evaluated scalar measure called reinforcement Depending type control task reinforcement may represent evaluation recent control action often entire sequence past control moves In latter case RL system learns predict outcome individual control action This prediction used adjust parameters controller The mathematical background RL closely related optimal control dynamic programming This paper gives comprehensive overview RL methods presents application attitude control satellite Some well known applications literature reviewed well',\n",
              " 'A biologically motivated mechanism selforganizing neural network modifiable lateral connections presented The weight modification rules purely activitydependent unsupervised local The lateral interaction weights initially random develop Mexican hat shape around neuron At time external input weights selforganize form topological map input space The algorithm demonstrates selforganization bootstrap using input information Predictions algorithm agree well experimental observations development lateral connections cortical feature maps',\n",
              " 'Some main users statistical methods economists social scientists epidemiologists discovering fields rest statistical causal foundations The blurring foundations years follows lack mathematical notation capable distinguishing causal equational relationships By providing formal natural explication relations graphical methods potential revolutionize statistics used knowledgerich applications Statisticians response beginning realize causality metaphysical deadend meaningful concept clear mathematical underpinning The paper surveys developments outlines future challenges',\n",
              " 'This paper describes new method inducing logic programs examples attempts integrate best aspects existing ILP methods single coherent framework In particular combines bottomup method similar Golem topdown method similar Foil It also includes method predicate invention similar Champ elegant solution noisy oracle problem allows system learn recursive programs without requiring complete set positive examples Systematic experimental comparisons Golem Foil range problems used clearly demonstrate advantages approach',\n",
              " 'We present deterministic techniques computing upper lower bounds marginal probabilities sigmoid noisyOR networks These techniques become useful size network clique size precludes exact computations We illustrate tightness bounds numerical experi ments',\n",
              " 'MIT Computational Cognitive Science Technical Report Abstract We develop recursive nodeelimination formalism efficiently approximating large probabilistic networks No constraints set network topologies Yet formalism straightforwardly integrated exact methods whenever arebecome applicable The approximations use controlled maintain consistently upper lower bounds desired quantities times We show Boltzmann machines sigmoid belief networks combination ie chain graphs handled within framework The accuracy methods verified exper imentally',\n",
              " 'We prove lower bound ln ffi VCdimC number random examples required distributionfree learning concept class C VCdimC VapnikChervonenkis dimension ffi accuracy confidence parameters This improves previous best lower bound ln ffi VCdimC comes close known general upper bound O ffi VCdimC ln consistent algorithms We show many interesting concept classes including kCNF kDNF bound actually tight within constant factor',\n",
              " 'The ability handle temporal variation important dealing realworld dynamic signals In many applications inputs come fixedrate sequences rather signals time scales vary one instance next thus modeling dynamic signals requires ability recognize sequences also ability handle temporal changes signal This paper discusses Tau Net neural network modeling dynamic signals application speech In Tau Net sequence learning accomplished using combination prediction recurrence timedelay connections Temporal variability modeled adaptable time constants network adjusted respect prediction error Adapting time constants changes time scale network adapted value networks time constant provides measure temporal variation signal Tau Net applied several simple signals sets sine waves differing frequency phase multidimensional signal representing walking gait children energy contour simple speech utterance Tau Net also shown work voicing distinction task using synthetic speech data In paper Tau Net applied two speakerindependent tasks vowel recognition faeiyuxg consonant recognition fptkg using speech data taken TIMIT database It shown Tau Nets trained mediumrate tokens achieved performance networks without time constants trained tokens rates performed better networks without time constants trained mediumrate tokens Our results demonstrate Tau Nets ability identify vowels consonants variable speech rates extrapolating rates represented training set',\n",
              " 'Backpropagation learning BP known serious limitations generalising knowledge certain types learning material BPSOM extension BP overcomes limitations BPSOM combination multilayered feedforward network MFN trained BP Kohonens selforganising maps SOMs In earlier reports shown BPSOM improved generalisation performance whereas decreased simultaneously number necessary hidden units without loss generalisation performance These two effects use SOM learning training MFNs In paper focus two additional effects First show BPSOM training activations hidden units MFNs tend oscillate among limited number discrete values Second identify SOM elements adequate organisers instances task hand We visualise effects argue lead intelligible neural networks employed basis automatic rule extraction',\n",
              " 'The discrimination powers Multilayer perceptron MLP Learning Vector Quantisation LVQ networks compared overlapping Gaussian distributions It shown analytically Monte Carlo studies MLP network handles high dimensional problems efficient way LVQ This mainly due sigmoidal form MLP transfer function also fact MLP uses hyperplanes efficiently Both algorithms equally robust limited training sets learning curves fall like M M training set size compared theoretical predictions statistical estimates VapnikChervonenkis bounds',\n",
              " 'In article approximate rate convergence Gibbs sampler normal approximation target distribution Based approximation consider many implementational issues Gibbs sampler eg updating strategy parameterization blocking We give theoretical results justify approximation illustrate methods number realistic examples',\n",
              " 'Instancebased learning methods explicitly remember data receive They usually training phase prediction time perform computation Then take query search database similar datapoints build online local model local average local regression predict output value In paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large We present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning Earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure Our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously This permits us query database exibility conventional linear search greatly reduced computational cost',\n",
              " 'We implemented reinforcement learning architecture reactive component two layer control system simulated race car We found separating layers expedited gradually improving competition multagent interaction We ran experiments test tuning decomposition coordination low level behaviors We extended control system allow passing cars tested ability avoid collisions The best design used reinforcement learning separate networks behavior coarse coded input simple rule based coordination mechanism',\n",
              " 'This study concerned whether possible detect information contained training data background knowledge relevant solving learning problem whether irrelevant information eliminated preprocessing starting learning process A case study data preprocessing hybrid genetic algorithm shows elimination irrelevant features substantially improve efficiency learning In addition costsensitive feature elimination effective reducing costs induced hypotheses',\n",
              " 'Hierarchical genetic programming HGP approaches rely discovery modification use new functions accelerate evolution This paper provides qualitative explanation improved behavior HGP based analysis evolution process dual perspective diversity causality From static point view use HGP approach enables manipulation population higher diversity programs Higher diversity increases exploratory ability genetic search process demonstrated theoretical experimental fitness distributions expanded structural complexity individuals From dynamic point view analysis causality crossover operator suggests HGP discovers exploits useful structures bottomup hierarchical manner Diversity causality complementary affecting exploration exploitation genetic search Unlike machine learning techniques need extra machinery control tradeoff HGP automatically trades exploration exploitation',\n",
              " 'Previous neural network learning algorithms sequence processing computationally expensive perform poorly comes long time lags This paper first introduces simple principle reducing descriptions event sequences without loss information A consequence principle unexpected inputs relevant This insight leads construction neural architectures learn divide conquer recursively decomposing sequences I describe two architectures The first functions selforganizing multilevel hierarchy recurrent networks The second involving two recurrent networks tries collapse multilevel predictor hierarchy single recurrent net Experiments show system require less computation per time step many fewer training sequences conventional training algorithms recurrent nets',\n",
              " 'A neural network model selforganization ocular dominance lateral connections binocular input presented The selforganizing process results network afferent weights neuron organize smooth hillshaped receptive fields primarily one retinas neurons common eye preference form connected intertwined patches lateral connections primarily link regions eye preference Similar selforganization cortical structures observed experimentally strabismic kittens The model shows patterned lateral connections cortex may develop based correlated activity explains lateral connection patterns follow receptive field properties ocular dominance',\n",
              " 'Relaxation oscillations exhibiting one time scale arise naturally many physical systems This paper proposes method numerically integrate large systems relaxation oscillators The numerical technique called singular limit method derived analysis relaxation oscillations singular limit In limit system evolution gives rise time instants fast dynamics takes place intervals slow dynamics takes place A full description method given LEGION locally excitatory globally inhibitory oscillator networks fast dynamics characterized jumping leads dramatic phase shifts captured method iterative operation slow dynamics entirely solved The singular limit method evaluated computer experiments produces remarkable speedup compared methods integrating systems The speedup makes possible simulate largescale oscillator networks',\n",
              " 'A selforganizing model spiking neurons dynamic thresholds lateral excitatory inhibitory connections presented tested image segmentation task The model integrates two previously separate lines research modeling visual cortex Laterally connected selforganizing maps used model afferent structures lateral connections could selforganize inputdriven Hebbian adaptation Spiking neurons leaky integrator synapses used model image segmentation binding synchronization desynchronization neuronal activity Although approaches differ model neuron overall layout laterally connected twodimensional network This paper shows selforganization segmentation achieved network thus presenting unified model development func tional dynamics primary visual cortex',\n",
              " 'The full Bayesian method applying neural networks prediction problem set priorhyperprior structure net perform necessary integrals However integrals tractable analytically Markov Chain Monte Carlo MCMC methods slow especially parameter space highdimensional Using Gaussian processes approximate weight space integral analytically small number hyperparameters need integrated MCMC methods We applied idea classification problems obtaining ex cellent results realworld problems investigated far',\n",
              " 'Recently Propp Wilson proposed algorithm called Coupling Past CFTP allows approximate perfect ie exact simulation stationary distribution certain finite state space Markov chains Perfect Sampling using CFTP successfully extended context point processes amongst authors Haggstrom et al In Gibbs sampling applied bivariate point process penetrable spheres mixture model However general running time CFTP terms number transitions independent state sampled Thus impatient user aborts long runs may introduce subtle bias user impatience bias Fill introduced exact sampling algorithm finite state space Markov chains contrast CFTP unbiased user impatience Fills algorithm form rejection sampling similar CFTP requires sufficient monotonicity properties transition kernel used We show Fills version rejection sampling extended infinite state space context produce exact sample penetrable spheres mixture process related models Following use Gibbs sampling make use partial order mixture model state space Thus',\n",
              " 'Cells visual cortex selective ocular dominance orientation input also size spatial frequency The simulations reported paper show size selectivity could develop Hebbian selforganization receptive fields different sizes could organize columns like orientation ocular dominance The lateral connections network selforganize cooperatively simultaneously receptive field sizes produce patterns lateral connectivity closely follow receptive field organization Together previous work ocular dominance orientation selectivity results suggest single Hebbian selforganizing process give rise major receptive field properties visual cortex also structured patterns lateral interactions verified experimentally others predicted model The model also suggests functional role selforganized structures The afferent receptive fields develop sparse coding visual input recurrent lateral interactions eliminate redundancies cortical activity patterns allowing cortex efficiently process massive amounts visual information',\n",
              " 'A pilot study described practical application artificial neural networks The limit cycle attitude control satellite selected test case One sources limit cycle position dependent error observed attitude A Reinforcement Learning method selected able adapt controller cost function optimised An estimate cost function learned neural critic In approach estimated cost function directly represented function parameters linear controller The critic implemented CMAC network Results simulations show method able find optimal parameters without unstable behaviour In particular case large discontinuities attitude measurements method shows clear improvement compared conventional approach RMS attitude error decreases approximately',\n",
              " 'In nutshell describe generic ILP problem following given set E positive negative examples target predicate background knowledge B world usually logic program including facts auxiliary predicates task find logic program H hypothesis positive examples deduced B H negative example In paper review results achieved area discuss techniques used Moreover prove following new results Predicates described nonrecursive local clauses k literals PAClearnable distribution This generalizes previous result valid constrained clauses Predicates described k nonrecursive local clauses PAClearnable distribution This generalizes previous result non construc tive valid class distributions Finally introduce believe first theoretical framework learning Prolog clauses presence errors To purpose introduce new noise model call fixed attribute noise model learning propositional concepts Boolean domain This new noise model interest',\n",
              " 'The ExpectationMaximization algorithm given Dempster et al enjoyed considerable popularity solving MAP estimation problems This note gives simple derivation algorithm due Luttrell better illustrates convergence properties algorithm variants The algorithm illustrated two examples pooling data multiple noisy sources fitting mixture density',\n",
              " 'We consider problem incorporate prior knowledge supervised learning techniques We set problem framework regularization theory consider case know approximated function radial symmetry The problem solved two alternative ways use invariance constraint regularization theory framework derive rotation invariant version Radial Basis Functions use radial symmetry create new virtual examples given data set We show two apparently different methods learning',\n",
              " 'I present computational results suggesting gainadaptation algorithms based part connectionist learning methods may improve least squares classical parameterestimation methods stochastic timevarying linear systems The new algorithms evaluated respect classical methods along three dimensions asymptotic error computational complexity required prior knowledge system The new algorithms order complexity LMS methods On n dimensionality system whereas leastsquares methods Kalman filter On The new methods also improve Kalman filter require complete statistical model system varies time In simple computational experiment new methods shown produce asymptotic error levels near optimal Kalman filter significantly leastsquares LMS methods The new methods may perform better even Kalman filter error filters model system varies time',\n",
              " 'Windowing proposed procedure efficient memory use ID decision tree learning algorithm However previous work shown windowing may often lead decrease performance In work try argue separateandconquer rule learning algorithms appropriate windowing divideandconquer algorithms learn rules independently less susceptible changes class distributions In particular present new windowing algorithm achieves additional gains efficiency exploiting property separateandconquer algorithms While presented algorithm suitable redundant noisefree data sets also briefly discuss problem noisy data windowing present preliminary ideas might solved extension algorithm introduced paper',\n",
              " 'This article describes comprehensive approach automatic theory revision Given imperfect theory approach combines explanation attempts incorrectly classified examples order identify failing portions theory For theory fault correlated subsets examples used inductively generate correction Because corrections focused tend preserve structure original theory Because system starts approximate domain theory general fewer training examples required attain given level performance classification accuracy compared purely empirical system The approach applies classification systems employing propositional Hornclause theory The system tested variety application domains results presented problems domains molecular biology plant disease diagnosis',\n",
              " 'Suppose one wishes sample density x using Markov chain Monte Carlo MCMC An auxiliary variable u conditional distribution ujx defined giving joint distribution x u xujx A MCMC scheme samples joint distribution lead substantial gains efficiency compared standard approaches The revolutionary algorithm Swendsen Wang one example In addition reviewing SwendsenWang algorithm generalizations paper introduces new auxiliary variable method called partial decoupling Two applications Bayesian image analysis considered The first binary classification problem partial decoupling performs SW single site Metropolis The second PET reconstruction uses gray level prior Geman McClure A generalized SwendsenWang algorithm developed problem reduces computing time point MCMC viable method posterior exploration',\n",
              " 'In paper analyse theoretical properties slice sampler We find algorithm extremely robust geometric ergodicity properties For case one auxiliary variable demonstrate algorithm stochastically monotone deduce analytic bounds total variation distance stationarity method using FosterLyapunov drift condition methodology',\n",
              " 'This paper studies well combination simulated annealing ADFs solves genetic programming GP style program discovery problems On suite composed evenkparity problems k analyses performance simulated annealing ADFs compared using ADFs In contrast GP results suite simulated annealing run ADFs problem size increases advantage using standard GP program representation marginal When performance simulated annealing compared GP algorithm using ADFs evenparity problem GP advantageous evenparity problem SA GP equal evenparity problem SA advantageous',\n",
              " 'Most learning algorithms work effectively training data contain completely specified labeled samples In many diagnostic tasks however data include values attributes model blocking process hides values attributes learner While blockers remove values critical attributes handicap learner paper instead focuses blockers remove irrelevant attribute values ie values needed classify instance given values unblocked attributes We first motivate formalize model superfluousvalue blocking demonstrate omissions useful proving certain classes seem hard learn general PAC model viz decision trees DNF formulae trivial learn setting We also show model extended deal theory revision ie modifying existing formula blockers occasionally include superfluous values exclude required values cor ruptions training data',\n",
              " 'This paper presents approach automatic discovery functions Genetic Programming The approach based discovery useful building blocks analyzing evolution trace generalizing blocks define new functions finally adapting problem representation onthefly Adaptating representation determines hierarchical organization extended function set enables restructuring search space solutions found easily Measures complexity solution trees defined adaptive representation framework The minimum description length principle applied justify feasibility approaches based hierarchy discovered functions suggest alternative ways defining problems fitness function Preliminary empirical results presented',\n",
              " 'A decision problem associated fundamental nonconvex model linearly inseparable pattern sets shown NPcomplete Another nonconvex model employs norm instead norm solved polynomial time solving n linear programs n usually small dimensionality pattern space An effective LPbased finite algorithm proposed solving latter model The algorithm employed obtain nonconvex piecewiselinear function separating points representing measurements made fine needle aspirates taken benign malignant human breasts A computer program trained samples correctly diagnosed new samples encountered currently use University Wisconsin Hospitals Introduction The fundamental problem wish address',\n",
              " 'Many connectionist approaches musical expectancy music composition let question What next overshadow equally important question When next One escape latter question one temporal structure considering perception musical meter We view perception metrical structure dynamic process temporal organization external musical events synchronizes entrains listeners internal processing mechanisms This article introduces novel connectionist unit based upon mathematical model entrainment capable phase frequencylocking periodic components incoming rhythmic patterns Networks units selforganize temporally structured responses rhythmic patterns The resulting network behavior embodies perception metrical structure The article concludes discussion implications approach theories metrical structure musical expectancy',\n",
              " 'Over years several packages developed provide workbench genetic algorithm GA research Most packages use generational model inspired GENESIS A adopted steadystate model used Genitor Unfortunately deficiencies working orderbased problems packing routing scheduling This paper describes LibGA developed specifically orderbased problems also works easily kinds problems It offers easy use userfriendly interface allows comparisons made generational steadystate genetic algorithms particular problem It includes variety genetic operators reproduction crossover mutation LibGA makes easy use operators new ways particular applications develop include new operators Finally offers unique new feature dynamic generation gap',\n",
              " 'Human episodic memory provides seemingly unlimited storage everyday experiences retrieval system allows us access experiences partial activation components The system believed consist fast temporary storage hippocampus slow longterm storage within neocortex This paper presents neural network model hippocampal episodic memory inspired Damasios idea Convergence Zones The model consists layer perceptual feature maps binding layer A perceptual feature pattern coarse coded binding layer stored weights layers A partial activation stored features activates binding pattern turn reactivates entire stored pattern For many configurations model theoretical lower bound memory capacity derived order magnitude higher number units model several orders magnitude higher number bindinglayer units Computational simulations indicate average capacity order magnitude larger theoretical lower bound making connectivity layers sparser causes even increase capacity Simulations also show descriptive binding patterns used errors tend plausible patterns confused similar patterns slight cost capacity The convergencezone episodic memory therefore accounts immediate storage associative retrieval capability large capacity hippocampal memory shows memory encoding areas much smaller perceptual maps consist rather coarse computational units sparsely connected perceptual maps',\n",
              " 'Empirical Learning Results POLLYANNA The value empirical learning demonstrated results testing theory space search TSS component POLLYANNA Empirical data shows approximations generated generic simplifying assumptions widely varying levels accuracy efficiency The candidate theory space includes theories Pareto optimal combinations accuracy efficiency well others nonoptimal Empirical learning thus needed separate optimal theories nonoptimal ones It works filter process generating approximations generic simplifying assumptions Empirical tests serve additional purpose well Theory space search collects data precisely characterizes tradeoff accuracy efficiency among candidate approximate theories The tradeoff data used select theory best balances competing objectives accuracy efficiency manner appropriate intended performance context The feasibility empirical learning also addressed results testing theory space search component POLLYANNA In order empirical testing feasible candidate approximate theories must operationally usable Candidate hearts theories generated POLLYANNA shown operationally usable experimental results theory space search TSS phase learning They run real machine producing results compared training examples Feasibility also depends information computation costs empirical testing Information costs result need supply system training examples Computation costs result need execute candidate theories Both types costs grow numbers candidate theories tested Experimental results show empirical testing POLLYANNA limited computation costs executing candidate theories information costs obtaining many training examples POLLYANNA contrasts respect traditional inductive learning systems The feasibility empirical learning depends also intended performance context resources available context learning Measurements theory space search phase indicate TSS algorithms performing exhaustive search would feasible hearts domain although may feasible applications TSS algorithms avoid exhaustive search hold considerably promise',\n",
              " 'In paper adopt generalsum stochastic games framework multiagent reinforcement learning Our work extends previous work Littman zerosum stochastic games broader framework We design multiagent Qlearning method framework prove converges Nash equilibrium specified conditions This algorithm useful finding optimal strategy exists unique Nash equilibrium game When exist multiple Nash equilibria game algorithm combined learning techniques find optimal strategies',\n",
              " 'Rising operating costs structural transformations resizing globalization companies world brought focus emerging discipline knowledge management concerned making knowledge pay Corporate memories form important part knowledge management initiatives company In paper discuss viewing corporate memories distributed case libraries benefit existing techniques distributed casebased reasoning resource discovery exploitation previous expertise We present two techniques developed context multiagent casebased reasoning accessing exploiting past experience corporate memory resources The first approach called Negotiated Retrieval deals retrieving assembling case pieces different resources corporate memory form good overall case The second approach based Federated Peer Learning deals two modes cooperation called DistCBR ColCBR let agent exploit experience expertise peer agents achieve local task fl The first author would like acknowledge support National Science Foundation Grant Nos IRI EEC The second authors research reported paper developed IIIA inside ANALOG Project funded Spanish CICYT grant The content paper necessarily reflect position policy US Government Kingdom Spain Government Catalonia Government official endorsement inferred',\n",
              " 'When learning reasoning failures knowledge system behaves powerful lever deciding went wrong system deciding system needs learn A number benefits arise systems possess knowledge operation knowledge Abstract knowledge cognition used select diagnosis repair strategies among alternatives Specific kinds selfknowledge used distinguish failure hypothesis candidates Making selfknowledge explicit also facilitate use knowledge across domains provide principled way incorporate new learning strategies To illustrate advantages selfknowledge learning provide implemented examples two different systems A plan execution system called RAPTER story understanding system called MetaAQUA',\n",
              " 'Theory revision integrates inductive learning background knowledge combining training examples coarse domain theory produce accurate theory There two challenges theory revision theoryguided systems face First representation language appropriate initial theory may inappropriate improved theory While original representation may concisely express initial theory accurate theory forced use representation may bulky cumbersome difficult reach Second theory structure suitable coarse domain theory may insufficient finetuned theory Systems produce small local changes theory limited value accomplishing complex structural alterations may required Consequently advanced theoryguided learning systems require flexible representation flexible structure An analysis various theory revision systems theoryguided learning systems reveals specific strengths weaknesses terms two desired properties Designed capture underlying qualities system new system uses theoryguided constructive induction Experiments three domains show improvement previous theoryguided systems This leads study behavior limitations potential theoryguided constructive induction',\n",
              " 'If experiment requires statistical analysis establish result one better experiment Ernest Rutherford Most proponents cold fusion reporting excess heat electrolysis experiments claiming one main characteristics cold fusion irreproducibility JR Huizenga Cold Fusion p Abstract Amid ever increasing research various aspects neural computing much progress evident theoretical advances empirical studies On empirical side wealth data experimental studies reported It however clear best report neural computing experiments may replicated interested researchers In particular nature iterative learning randomised initial architecture backpropagation training multilayer perceptron precise replication reported result virtually impossible The outcome experimental replication reported results touchstone scientific method option researchers popular subfield neural computing In paper address issue replicability experiments based backpropagation training multilayer perceptrons although many results applicable subfield plagued characteristics First attempt produce complete abstract specification neural computing experiment From specification identify full range parameters needed support maximum replicability use show absolute replicability option practice We propose statistical framework support replicability We demonstrate framework empirical studies replicability respect experimental controls validity implementations backpropagation algorithm Finally suggest degree replicability neural computing experiment estimated reflected claimed precision empirical results reported',\n",
              " 'In paper propose unsupervised neural network allowing robot learn sensorimotor associations delayed reward The robot task learn meaning pictograms order survive maze First introduce new neural conditioning rule PCR Probabilistic Conditioning Rule allowing test hypotheses associations visual categories movements given time span Second describe real maze experiment mobile robot We propose neural architecture solve problem discuss difficulty build visual categories dynamically associating movements Third propose use algorithm simulation order test exhaustively We give results different kind mazes compare system adapted version Qlearning algorithm Finally conclude showing limitations approaches take account intrinsic complexity reasonning based image recognition',\n",
              " 'We present framework analysis synthesis acoustical instruments based datadriven probabilistic inference modeling Audio time series boundary conditions played instrument recorded nonlinear mapping control data audio space inferred using general inference framework ClusterWeighted Modeling The resulting model used realtime synthesis audio sequences new input data',\n",
              " 'In many realworld domains task machine learning algorithms learn theory predicting numerical values In particular several standard test domains used Inductive Logic Programming ILP concerned predicting numerical values examples relational mostly nondeterminate background knowledge However far ILP algorithm except one predict numbers cope nondeterminate background knowledge The exception covering algorithm called FORS In paper present Structural Regression Trees SRT new algorithm applied class problems integrating statistical method regression trees ILP SRT constructs tree containing literal atomic formula negation conjunction literals node assigns numerical value leaf SRT provides comprehensible results purely statistical methods applied class problems ILP systems handle Experiments several realworld domains demonstrate approach competitive existing methods indicating advantages expense predictive accuracy',\n",
              " 'A quantitative practical Bayesian framework described learning mappings feedforward networks The framework makes possible objective comparisons solutions using alternative network architectures objective stopping rules network pruning growing procedures objective choice magnitude type weight decay terms additive regularisers penalising large weights etc measure effective number welldetermined parameters model quantified estimates error bars network parameters network output objective comparisons alternative learning interpolation models splines radial basis functions The Bayesian evidence automatically embodies Occams razor penalising overflexible overcomplex models The Bayesian approach helps detect poor underlying assumptions learning models For learning models well matched problem good correlation generalisation ability This paper makes use Bayesian framework regularisation model comparison described companion paper Bayesian interpolation MacKay This framework due Gull Skilling Gull Bayesian evidence obtained',\n",
              " 'Simultaneous multithreading technique permits multiple independent threads issue multiple instructions cycle In previous work demonstrated performance potential simultaneous multithreading based somewhat idealized model In paper show throughput gains simultaneous multithreading achieved without extensive changes conventional wideissue superscalar either hardware structures sizes We present architecture simultaneous multithreading achieves three goals minimizes architectural impact conventional superscalar design minimal performance impact single thread executing alone achieves significant throughput gains running multiple threads Our simultaneous multithreading architecture achieves throughput instructions per cycle fold improvement unmodified superscalar similar hardware resources This speedup enhanced advantage multithreading previously unexploited architectures ability favor fetch issue threads efficiently using processor cycle thereby providing best instructions processor',\n",
              " 'The theory revision problem problem best go revising deficient domain theory using information contained examples expose inaccuracies In paper present approach theory revision problem propositional domain theories The approach described called PTR uses probabilities associated domain theory elements numerically track ow proof theory This allows us measure precise role clause literal allowing preventing desired undesired derivation given example This information used efficiently locate repair awed elements theory PTR proved converge theory correctly classifies examples shown experimentally fast accurate even deep theories',\n",
              " 'New methodology fully Bayesian mixture analysis developed making use reversible jump Markov chain Monte Carlo methods capable jumping parameter subspaces corresponding different numbers components mixture A sample full joint distribution unknown variables thereby generated used basis thorough presentation many aspects posterior distribution The methodology applied analysis univariate normal mixtures using hierarchical prior model offers approach dealing weak prior information avoiding mathematical pitfalls using improper priors mixture context',\n",
              " 'Northeastern University College Computer Science Technical Report NUCCS fl We gratefully acknowledge substantial contributions effort provided Andy Barto sparked original interest questions whose continued encouragement insightful comments criticisms helped us greatly Recent discussions Satinder Singh Vijay Gullapalli also helpful impact work Special thanks also Rich Sutton influenced thinking subject numerous ways This work supported Grant IRI National Science Foundation U S Air Force',\n",
              " 'Active learning differs passive learning examples learning algorithm assumes least control part input domain receives information In situations active learning provably powerful learning examples alone giving better generalization fixed number training examples In paper consider problem learning binary concept absence noise Valiant We describe formalism active concept learning called selective sampling show may approximately implemented neural network In selective sampling learner receives distribution information environment queries oracle parts domain considers useful We test implementation called SGnetwork three domains observe significant improvement generalization',\n",
              " 'High performance architectures always deal performancelimiting impact branch operations Microprocessor designs going deal problem well move towards deeper pipelines support multiple instruction issue Branch prediction schemes often used alleviate negative impact branch operations allowing speculative execution instructions unresolved branch Another technique eliminate branch instructions altogether Predication remove forward branch instructions translating instructions following branch predicate form This paper analyzes variety existing predication models eliminating branch operations effect elimination branch prediction schemes existing processors including single issue architectures simple prediction mechanisms newer multiissue designs correspondingly sophisticated branch predictors The effect branch prediction accuracy branch penalty basic block size studied hhhhhhhhhhhhhhhhhhhhhhhh',\n",
              " 'This paper describes model complementarity rules precedents classification task Under model precedents assist rulebased reasoning operationalizing abstract rule antecedents Conversely rules assist casebased reasoning case elaboration process inferring case facts order increase similarity cases term reformulation process replacing term whose precedents weakly match case terms whose precedents strongly match case Fully exploiting complementarity requires control strategy characterized impartiality absence arbitrary ordering restrictions use rules precedents An impartial control strategy implemented GREBE domain Texas workers compensation law In preliminary evaluation GREBEs performance found good slightly better performance law students task A case classified belonging particular category relating description criteria category membership The justifications warrants Toulmin relate case category vary widely generality antecedents For example consider warrants classifying case legal category negligence A rule An action negligent actor fails use reasonable care failure proximate cause injury general antecedent terms eg breach reasonable care Conversely precedent Dr Jones negligent failed count sponges surgery result left sponge Smith specific antecedent terms eg failure count sponges Both types warrants used classification systems relate cases categories Classification systems used precedents help match antecedents rules cases Completing match difficult terms antecedent opentextured ie significant uncertainty whether match specific facts Gardner McCarty Sridharan This problem results generality gap separating abstract terms specific facts Porter et al Precedents opentextured term ie past cases term applied used bridge gap Unlike rule antecedents antecedents precedents level generality cases generality gap exists precedents new cases Precedents therefore reduce problem matching specific case facts opentextured terms problem matching two sets specific facts For example injured employees entitlement workers compensation depends whether injured activity furtherance employment Determining whether particular case classified compensable injury therefore requires matching specific facts case eg John injured automobile accident driving office opentextured term activity furtherance employment The gap generality case description abstract term makes match problematical However completing match may much easier precedents term activity furtherance employment eg Marys injury compensable occurred driving work activity furtherance employment Bills injury compensable occurred driving house deliver pizza activity furtherance employment In case Johns driving office closely matches Marys driving work',\n",
              " 'We introduce modelbased average reward Reinforcement Learning method called Hlearning compare discounted counterpart Adaptive RealTime Dynamic Programming simulated robot scheduling task We also introduce extension Hlearning automatically explores unexplored parts state space always choosing greedy actions respect current value function We show Autoexploratory Hlearning performs better original Hlearning previously studied exploration methods random recencybased counterbased exploration',\n",
              " 'This paper proposes using fuzzy logic techniques dynamically control parameter settings genetic algorithms GAs We describe Dynamic Parametric GA GA uses fuzzy knowledgebased system control GA parameters We introduce technique automatically designing tuning fuzzy knowledgebase system using GAs Results initial experiments show performance improvement simple static GA One Dynamic Parametric GA system designed automatic method demonstrated improvement application included design phase may indicate general applicability Dynamic Parametric GA wide range ap plications',\n",
              " 'In previous work Olshausen Field algorithm described learning linear sparse codes trained natural images produces set basis functions spatially localized oriented bandpass ie waveletlike This note shows algorithm may interpreted within maximumlikelihood framework Several useful insights emerge connection makes explicit relation statistical independence ie factorial coding shows formal relationship algorithm Bell Sejnowski suggests adapt parameters previously fixed This report describes research done within Center Biological Computational Learning Department Brain Cognitive Sciences Massachusetts Institute Technology This research sponsored Individual National Research Service Award BAO NIMH FMH grant National Science Foundation contract ASC award includes funds ARPA provided HPCC program CBCL',\n",
              " 'We study layered belief networks binary random variables conditional probabilities Prchildjparents depend monotonically weighted sums parents For networks give efficient algorithms computing rigorous bounds marginal probabilities evidence output layer Our methods apply generally computation upper lower bounds well generic transfer function parameterizations conditional probability tables sigmoid noisyOR We also prove rates convergence accuracy bounds function network size Our results derived applying theory large deviations weighted sums parents node network Bounds marginal probabilities computed two contributions one assuming weighted sums fall near mean values assuming This gives rise interesting tradeoff probable explanations evidence improbable deviations mean In networks child N parents gap upper lower bounds behaves sum two terms one order p In addition providing rates convergence large networks methods also yield efficient algorithms approximate inference fixed networks',\n",
              " 'Feature selection proven valuable technique supervised learning improving predictive accuracy reducing number attributes considered task We investigate potential similar benefits unsupervised learning task conceptual clustering The issues raised feature selection absence class labels discussed implementation sequential feature selection algorithm based existing conceptual clustering system described Additionally present second implementation employs technique improving efficiency search optimal description compare performance algorithms',\n",
              " 'Robust flexible sufficiently general vision systems recognition description complex dimensional objects require adequate armamentarium representations learning mechanisms This paper briefly analyzes strengths weaknesses different learning paradigms symbol processing systems connectionist networks statistical syntactic pattern recognition systems possible candidates providing capabilities points several promising directions integrating multiple paradigms synergistic fashion towards goal',\n",
              " 'A selforganizing neural network sequence classification called SARDNET described analyzed experimentally SARDNET extends Kohonen Feature Map architecture activation retention decay order create unique distributed response patterns different sequences SARDNET yields extremely dense yet descriptive representations sequential input training iterations The network proven successful mapping arbitrary sequences binary real numbers well phonemic representations English words Potential applications include isolated spoken word recognition cognitive science models sequence processing',\n",
              " 'LIACC Technical Report Abstract In paper address problem acquiring knowledge integration Our aim construct integrated knowledge base several separate sources The objective integration construct one system exploits knowledge available good performance The aim paper discuss methodology knowledge integration present concrete results In experiments performance integrated theory exceeded performance individual theories quite significant amount Also performance fluctuate much experiments repeated These results indicate knowledge integration complement existing ML methods',\n",
              " 'In introduction define term bias used machine learning systems We motivate importance automated methods evaluating selecting biases using framework bias selection search bias metabias spaces Recent research field machine learning bias summarized',\n",
              " 'A method initial results comparative study ABSTRACT A standard approach determining decision trees learn examples A disadvantage approach decision tree learned difficult modify suit different decision making situations Such problems arise example attribute assigned node measured significant change costs measuring attributes frequency distribution events different decision classes An attractive approach resolving problem learn store knowledge form decision rules generate whenever needed decision tree suitable given situation An additional advantage approach facilitates building compact decision trees much simpler logically equivalent conventional decision trees compact trees meant decision trees may contain branches assigned set values nodes assigned derived attributes ie attributes logical mathematical functions original ones The paper describes efficient method AQDT takes decision rules generated AQtype learning system AQ AQ builds decision tree optimizing given optimality criterion The method work two modes standard mode produces conventional decision trees compact mode produces compact decision trees The preliminary experiments AQDT shown decision trees generated decision rules conventional compact outperformed generated examples wellknown C program terms simplicity predictive accuracy',\n",
              " 'OGI CSE Technical Report Abstract Smoothing regularizers radial basis functions studied extensively general smoothing regularizers projective basis functions PBFs widelyused sigmoidal PBFs heretofore proposed We derive new classes algebraicallysimple th order smoothing regularizers networks projective basis functions f W x P N fi fl u general transfer functions g These simple algebraic forms RW enable direct enforcement smoothness without need costly Monte Carlo integrations SW The regularizers tested illustrative sample problems compared quadratic weight decay The new regularizers shown yield better generalization errors',\n",
              " 'We address problem musical variation identification different musical sequences variations implications mental representations music According reductionist theories listeners judge structural importance musical events forming mental representations These judgments may result production reduced memory representations retain musical gist In study improvised music performance pianists produced variations melodies Analyses musical events retained across variations provided support reductionist account structural importance A neural network trained produce reduced memory representations melodies represented structurally important events efficiently others Agreement among musicians improvisations network model musictheoretic predictions suggest perceived constancy across musical variation natural result reductionist mechanism producing memory representations',\n",
              " 'Ensemble learning variational free energy minimization tool introduced neural networks Hinton van Camp learning described terms optimization ensemble parameter vectors The optimized ensemble approximation posterior probability distribution parameters This tool applied variety statistical inference problems In paper I study linear regression model parameters hyperparameters I demonstrate evidence approximation optimization regularization constants derived detail free energy minimization view point',\n",
              " 'The self regenerative MCMC tool constructing Markov chain given stationary distribution constructing auxiliary chain stationary distribution Elements auxiliary chain picked suitable random number times resulting chain stationary distribution Sahu Zhigljavsky In article provide generic adaptation scheme algorithm The adaptive scheme use knowledge stationary distribution gathered far update course simulation This method easy implement often leads considerable improvement We obtain theoretical results adaptive scheme Our proposed methodology illustrated number realistic examples Bayesian computation performance compared available MCMC techniques In one applications develop nonlinear dynamics model modeling predatorprey relationships wild',\n",
              " 'Conceptual analogy CA approach integrates conceptualization ie memory organization based prior experiences analogical reasoning Borner It implemented prototypically tested support design process building engineering Borner Janetzko Borner There number features distinguish CA standard approaches CBR AR First CA automatically extracts knowledge needed support design tasks ie complex case representations relevance object features relations proper adaptations attributevalue representations prior layouts Secondly effectively determines similarity complex case representations terms adaptability Thirdly implemented integrated highly interactive adaptive system architecture allows incremental knowledge acquisition user support This paper surveys basic assumptions psychological results influenced development CA It sketches knowledge representation formalisms employed characterizes subprocesses needed integrate memory organization analogical reasoning',\n",
              " 'Even sophisticated branchprediction techniques necessarily suffer mispredictions even relatively small mispredict rates hurt performance substantially currentgeneration processors In paper investigate schemes improving performance face imperfect branch predictors processor simultaneously execute code taken nottaken outcomes branch This paper presents data regarding limits multipath execution considers fetchbandwidth needs multipath execution discusses various dynamic confidenceprediction schemes gauge likelihood branch mispredictions Our evaluations consider executing along several paths Using paths relatively simple confidence predictor multipath execution garners speedups compared singlepath case average speedup SPECint suite While associated increases instructionfetchbandwidth requirements surprising less expected result significance separate returnaddress stack forked path Overall results indicate multipath execution offers significant improvements singlepath performance could especially useful combined multithreading hardware costs amortized approaches',\n",
              " 'This paper presents algorithms robustness analysis Bayesian networks global neighborhoods Robust Bayesian inference calculation bounds posterior values given perturbations probabilistic model We present algorithms robust inference including expected utility expected value variance bounds global perturbations modeled contaminated constant density ratio constant density bounded total variation classes distributions c fl Carnegie Mellon University',\n",
              " 'This paper describes selflearning control system mobile robot Based local sensor data robot taught avoid collisions obstacles The feedback control system binaryvalued external reinforcement signal indicates whether collision occured A reinforcement learning scheme used find correct mapping input sensor space output steering signal space An adaptive quantisation scheme introduced discrete division input space built scratch system',\n",
              " 'Rules extracted trained feedforward networks used explanation validation crossreferencing network output decisions This paper introduces rule evaluation ordering mechanism orders rules extracted feedforward networks based three performance measures Detailed experiments using three rule extraction techniques applied Wisconsin breast cancer database illustrate power proposed methods Moreover method integrating output decisions extracted rulebased system corresponding trained network proposed The integrated system provides improvements',\n",
              " 'Standard methods inducing structure weight values recurrent neural networks fit assumed class architectures every task This simplification necessary interactions network structure function well understood Evolutionary computation includes genetic algorithms evolutionary programming populationbased search method shown promise complex tasks This paper argues genetic algorithms inappropriate network acquisition describes evolutionary program called GNARL simultaneously acquires structure weights recurrent networks This algorithms empirical acquisition method allows emergence complex behaviors topologies potentially excluded artificial architectural constraints imposed standard network induction methods',\n",
              " 'A new mechanism genetic encoding neural networks proposed loosely based marker structure biological DNA The mechanism allows aspects network structure including number nodes connectivity evolved genetic algorithms The effectiveness encoding scheme demonstrated object recognition task requires artificial creatures whose behaviour driven neural network develop highlevel finitestate exploration discrimination strategies The task requires solving sensorymotor grounding problem ie developing functional understanding effects creatures movement sensory input',\n",
              " 'We study multivariate smoothing spline estimate function several variables based ANOVA decomposition sums main effect functions one variable twofactor interaction functions two variables etc We derive Bayesian confidence intervals components decomposition demonstrate even multiple smoothing parameters efficiently computed using publicly available code RKPACK originally designed compute estimates We carry small Monte Carlo study see closely actual properties componentwise confidence intervals match nominal confidence levels Lastly analyze lake acidity data function calcium concentration latitude longitude using polynomial thin plate spline main effects model',\n",
              " 'appear Cowan J Tesauro G Alspector J eds Advances Neural Information Processing Systems San Francisco CA Morgan Kaufmann Publishers This paper written tersely accomodate page limitation Presented refereed poster session Conference Neural Information Processing SystemsNatural Synthetic November Denver CO Supported NSF grants DMS DMS National Eye InstituteNIH grants EY EY',\n",
              " 'Virtually largescale sequencing projects use automatic sequenceassembly programs aid determination DNA sequences The computergenerated assemblies require substantial handediting transform submissions GenBank As size sequencing projects increases becomes essential improve quality automated assemblies timeconsuming handediting may reduced Current ABI sequencing technology uses base calls made fluorescentlylabeled DNA fragments run gels We present new representation fluorescent trace data associated individual base calls This representation used fragment assembly improve quality assemblies We demonstrate one use endtrimming suboptimal data results significant improvement quality subsequent assemblies',\n",
              " 'Extensive research done extracting parallelism single instruction stream processors This paper presents results investigation ways modify MIMD architectures allow extract instruction level parallelism achieved current superscalar VLIW machines A new architecture proposed utilizes advantages multiple instruction stream design addressing limitations prevented MIMD architectures performing ILP operation A new code scheduling mechanism described support new architecture partitioning instructions across multiple processing elements order exploit level parallelism',\n",
              " 'This paper describes single chip Multiple Instruction Stream Computer MISC capable extracting instruction level parallelism broad spectrum programs The MISC architecture uses multiple asynchronous processing elements separate program streams executed parallel integrates conflictfree message passing system lowest level processor design facilitate low latency intraMISC communication This approach allows increased machine parallelism minimal code expansion provides alternative approach single instruction stream multiissue machines SuperScalar VLIW',\n",
              " 'In paper define examine two versions bridge problem The first variant bridge problem determistic model agent knows superset transitions priori probabilities transitions intact In second variant transitions break fixed probability time step These problems applicable planning uncertain domains well packet routing computer network We show agent act optimally models reduction Markov decision processes We describe methods solving note methods intractable reasonably sized problems Finally suggest neurodynamic programming method value function approximation types models',\n",
              " 'If several mental states reliably distinguished recognizing patterns EEG paralyzed person could communicate device like wheelchair composing sequencesof mental states In article report study comparing four representations EEG signals classification twolayer neural network sigmoid activation functions The neural network implemented CNAPS server processor SIMD architecture Adaptive Solutions Inc gaining fold decrease training time Sun',\n",
              " 'Recurrent perceptron classifiers generalize classical perceptron model They take account correlations dependences among input coordinates arise linear digital filtering This paper provides tight bounds sample complexity associated fitting models experimental data',\n",
              " 'I present general taxonomy neural net architectures processing timevarying patterns This taxonomy subsumes many existing architectures literature points several promising architectures yet examined Any architecture processes timevarying patterns requires two conceptually distinct components shortterm memory holds relevant past events associator uses shortterm memory classify predict My taxonomy based characterization shortterm memory models along dimensions form content adaptability Experiments predicting future values financial time series US dollarSwiss franc exchange rates presented using several alternative memory models The results experiments serve baseline sophisticated architectures compared Neural networks proven promising alternative traditional techniques nonlinear temporal prediction tasks eg Curtiss Brandemuehl Kreider Lapedes Farber Weigend Huberman Rumelhart However temporal prediction particularly challenging problem conventional neural net architectures algorithms well suited patterns vary time The prototypical use neural nets structural pattern recognition In task collection featuresvisual semantic otherwiseis presented network network must categorize input feature pattern belonging one classes For example network might trained classify animal species based set attributes describing living creatures tail lives water carnivorous network could trained recognize visual patterns twodimensional pixel array letter fA B Zg In tasks network presented relevant information simultaneously In contrast temporal pattern recognition involves processing patterns evolve time The appropriate response particular point time depends current input potentially previous inputs This illustrated Figure shows basic framework temporal prediction problem I assume time quantized discrete steps sensible assumption many time series interest intrinsically discrete continuous series sampled fixed interval The input time denoted xt For univariate series input',\n",
              " 'DISLEX artificial neural network model mental lexicon It built test computationally whether lexicon could consist separate feature maps different lexical modalities lexical semantics connected ordered pathways In model orthographic phonological semantic feature maps associations formed unsupervised process based cooccurrence lexical symbol meaning After model organized various damage lexical system simulated resulting dyslexic categoryspecific aphasic impairments similar observed human patients',\n",
              " 'In paper describe new selforganizing decomposition technique learning highdimensional mappings Problem decomposition performed errordriven manner resulting subtasks patches equally well approximated Our method combines unsupervised learning scheme Feature Maps Koh nonlinear approximator Backpropagation RHW The resulting learning system stable effective changing environments plain backpropagation much powerful extended feature maps proposed RS RMS Extensions method give rise active exploration strategies autonomous agents facing unknown environments The appropriateness general purpose method demonstrated ex ample mathematical function approximation',\n",
              " 'Irrelevant features weakly relevant features may reduce comprehensibility accuracy concepts induced supervised learning algorithms We formulate search feature subset abstract search problem probabilistic estimates Searching space using evaluation function random variable requires trading accuracy estimates increased state exploration We show recent feature subset selection algorithms machine learning literature fit search problem simple hill climbing approaches conduct small experiment using bestfirst search technique',\n",
              " 'As field Genetic Programming GP matures breadth application increases need parallel implementations becomes absolutely necessary The transputerbased system recently presented Koza one rare parallel implementations Until today implementation proposed parallel GP using SIMD architecture except dataparallel approach although others exploited workstation farms pipelined supercomputers One reason certainly apparent difficulty dealing parallel evaluation different Sexpressions single instruction executed time every processor The aim paper present implementation parallel GP SIMD system processor efficiently evaluate different Sexpression We implemented approach MasPar MP computer present timing results To extent SIMD machines like MasPar available offer costeffective cycles scien tific experimentation useful approach',\n",
              " 'Reinforcement learning problem generating optimal behavior sequential decisionmaking environment given opportunity interacting Many algorithms solving reinforcementlearning problems work computing improved estimates optimal value function We extend prior analyses reinforcementlearning algorithms present powerful new theorem provide unified analysis valuefunctionbased reinforcementlearning algorithms The usefulness theorem lies allows asynchronous convergence complex reinforcementlearning algorithm proven verifying simpler synchronous algorithm converges We illustrate application theorem analyzing convergence Qlearning modelbased reinforcement learning Qlearning multistate updates Qlearning Markov games risksensitive reinforcement learning',\n",
              " 'Hyperspectral image sensors provide images large number contiguous spectral channels per pixel enable information different materials within pixel obtained The problem spectrally unmixing materials may viewed specific case blind source separation problem data consists mixed signals case minerals goal determine contribution mineral mix without prior knowledge minerals mix The technique Independent Component Analysis ICA assumes spectral components close statistically independent provides unsupervised method blind source separation We introduce contextual ICA context hyperspectral data analysis apply method mineral data synthetically mixed minerals real image signatures',\n",
              " 'Partially observable Markov decision processes POMDPs allow one model complex dynamic decision control problems include action outcome uncertainty imperfect observability The control problem formulated dynamic optimization problem value function combining costs rewards multiple steps In paper propose analyse test various incremental methods computing bounds value function control problems infinite discounted horizon criteria The methods described tested include novel incremental versions gridbased linear interpolation method simple lower bound method Sondiks updates Both work arbitrary points belief space enhanced various heuristic point selection strategies Also introduced new method computing initial upper bound fast informed bound method This method able improve significantly standard commonly used upper bound computed MDPbased method The quality resulting bounds tested maze navigation problem states actions observations',\n",
              " 'The energy prediction competition involved prediction series building energy loads series environmental input variables Nonlinear regression using neural networks popular technique modeling tasks Since obvious large timewindow inputs appropriate preprocessing inputs best viewed regression problem many possible input variables may actually irrelevant prediction output variable Because finite data set show random correlations irrelevant inputs output conventional neural network even regularisation weight decay set coefficients junk inputs zero Thus irrelevant variables hurt models performance The Automatic Relevance Determination ARD model puts prior regression parameters embodies concept relevance This done simple soft way introducing multiple regularisation constants one associated input Using Bayesian methods regularisation constants junk inputs automatically inferred large preventing inputs causing significant overfitting',\n",
              " 'Several different approaches used describe concepts supervised learning tasks In paper describe two approaches prototypebased incremental neural networks casebased reasoning approaches We show improve prototypebased neural network model storing specific instances CBR memory system This leads us propose coprocessing hybrid model classification',\n",
              " 'Extensive research done extracting parallelism single instruction stream processors This paper presents investigation ways modify MIMD architectures allow extract instruction level parallelism achieved current superscalar VLIW machines A new architecture proposed utilizes advantages multiple instruction stream design addressing limitations prevented MIMD architectures performing ILP operation A new code scheduling mechanism described support new architecture partitioning instructions across multiple processing elements order exploit level parallelism',\n",
              " 'A performance prediction method presented indicating performance range MIMD parallel processor systems neural network simulations The total execution time parallel application modeled sum calculation communication times The method scalable based times measured one processor one communication link performance speedup efficiency predicted larger processor system It validated quantitatively applying two popular neural networks backpropagation Kohonen selforganizing feature map decomposed GCel transputer system Agreement model measurements within',\n",
              " 'Algorithms learning classification trees successes artificial intelligence statistics many years This paper outlines tree learning algorithm derived using Bayesian statistics This introduces Bayesian techniques splitting smoothing tree averaging The splitting rule similar Quinlans information gain smoothing averaging replace pruning Comparative experiments reimplementations minimum encoding approach Quinlans C Quinlan et al Breiman et als CART Breiman et al show full Bayesian algorithm produce Publication This paper final draft submitted publication Statistics Computing journal version minor changes appeared Volume pages accurate predictions versions approaches though pay computational price',\n",
              " 'pomdps general models sequential decisions actions observations probabilistic Many problems interest formulated pomdps yet use pomdps limited lack effective algorithms Recently started change number problems robot navigation planning beginning formulated solved pomdps The advantage pomdp approach clean semantics ability produce principled solutions integrate physical information gathering actions In paper pursue approach context two learning tasks learning sort vector numbers learning decision trees data Both problems formulated pomdps solved general pomdp algorithm The main lessons results use suitable heuristics representations allows solution sorting classification pomdps nontrivial sizes quality resulting solutions competitive best algorithms problematic aspects decision tree learning test misclassification costs noisy tests missing values naturally accommodated',\n",
              " 'Given arbitrary learning situation difficult determine appropriate learning strategy The goal research provide general representation processing framework introspective reasoning strategy selection The learning framework introspective system perform reasoning task As system also records trace reasoning along results reasoning If reasoning failure occurs system retrieves applies introspective explanation failure order understand error repair knowledge base A knowledge structure called MetaExplanation Pattern used explain conclusions derived conclusions fail If reasoning represented explicit declarative manner system examine reasoning analyze reasoning failures identify needs learn select appropriate learning strategies order learn required knowledge without overreli ance programmer',\n",
              " 'We describe ongoing project develop adaptive training system ATS dynamically models students learning processes provide specialized tutoring adapted students knowledge state learning style The student modeling component ATS MLModeler uses machine learning ML techniques emulate students novicetoexpert transition MLModeler infers learning methods student used reach current knowledge state comparing students solution trace expert solution generating plausible hypotheses misconceptions errors student made A casebased approach used generate hypotheses incorrectly applying analogy overgeneralization overspecialization The student expert models use networkbased representation includes abstract concepts relationships well strategies problem solving Fuzzy methods used represent uncertainty student model This paper describes design ATS MLModeler gives detailed example system would model tutor student typical session The domain use example highschool level chemistry',\n",
              " 'Many algorithms parameters set user For machine learning algorithms parameter setting nontrivial task influence knowledge model returned algorithm Parameter values usually set approximately according characteristics target problem obtained different ways The usual way use background knowledge target problem perform testing experiments The paper presents approach automated model selection based local optimization uses empirical evaluation constructed concept description guide search The approach tested using inductive concept learning system Magnus',\n",
              " 'This paper presents method learning logic programs without explicit negative examples exploiting assumption output completeness A mode declaration supplied target predicate training input assumed accompanied legal outputs Any outputs generated incomplete program implicitly represent negative examples however large numbers ground negative examples never need generated This method incorporated two ILP systems Chillin IFoil use intensional background knowledge Tests two natural language acquisition tasks caserole mapping pasttense learning illustrate advantages approach',\n",
              " 'Instancebased learning methods explicitly remember data receive They usually training phase prediction time perform computation Then take query search database similar datapoints build online local model local average local regression predict output value In paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large We present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning Earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure Our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously This permits us query database exibility conventional linear search greatly reduced computational cost',\n",
              " 'In paper introduce new agglomerative clustering algorithm pattern cluster represented collection fuzzy hyperboxes Initially number hyperboxes calculated represent pattern samples Then algorithm applies multiresolution techniques progressively combine hyperboxes hierarchial manner Such agglomerative scheme found yield encouraging results realworld clustering problems',\n",
              " 'This paper introduces randomized technique partitioning examples using oblique hyperplanes Standard decision tree techniques ID descendants partition set points axisparallel hyperplanes Our method contrast attempts find hyperplanes orientation The purpose general technique find smaller equally accurate decision trees created methods We tested algorithm real simulated data found cases produces surprisingly small trees without losing predictive accuracy Small trees allow us turn obtain simple qualitative descriptions problem domain',\n",
              " 'This paper introduces ICET new algorithm costsensitive classification ICET uses genetic algorithm evolve population biases decision tree induction algorithm The fitness function genetic algorithm average cost classification using decision tree including costs tests features measurements costs classification errors ICET compared three algorithms costsensitive classification EG CSID IDX also C classifies without regard cost The five algorithms evaluated empirically five realworld medical datasets Three sets experiments performed The first set examines baseline performance five algorithms five datasets establishes ICET performs significantly better competitors The second set tests robustness ICET variety conditions shows ICET maintains advantage The third set looks ICETs search bias space discovers way improve search',\n",
              " 'This paper highlights role mathematical programming particularly linear programming training neural networks A neural network description given terms separating planes input space suggests use linear programming determining planes A standard description terms mean square error output space also given leads use unconstrained minimization techniques training neural network The linear programming approach demonstrated brief description system breast cancer diagnosis use last four years major medical facility',\n",
              " 'Dissatisfaction existing standard casebased reasoning CBR systems prompted us investigate make systems creative broadly would mean creative This paper discusses three research goals understanding creative processes better investigating role cases CBR creative problem solving understanding framework supports interesting kind casebased reasoning In addition discusses methodological issues study creativity particular use CBR research paradigm exploring creativity',\n",
              " 'This work presents application machine learning characterizing important property natural DNA sequences compositional inhomogeneity Compositional segments often correspond meaningful biological units Taking account inhomogeneity prerequisite successful recognition functional features DNA sequences especially proteincoding genes Here present technique DNA segmentation using hidden Markov models A DNA sequence represented chain homogeneous segments described one statistically discriminated hidden states whose contents form firstorder Markov chain The technique used describe compare chromosomes I IV completely sequenced Saccharomyces cerevisiae yeast genome Our results indicate existence well separated states gives support isochore theory We also explore models likelihood landscape analyze dynamics optimization process thus addressing problem reliability obtained optima efficiency algorithms',\n",
              " 'Weight modifications traditional neural nets computed hardwired algorithms Without exception previous weight change algorithms many specific limitations Is principle possible overcome limitations hardwired algorithms allowing neural nets run improve weight change algorithms This paper constructively demonstrates answer principle yes I derive initial gradientbased sequence learning algorithm selfreferential recurrent network speak weight matrix terms activations It uses input output units observing errors explicitly analyzing modifying weight matrix including parts weight matrix responsible analyzing modifying weight matrix The result first introspective neural net explicit potential control adaptive parameters A disadvantage algorithm high computational complexity per time step independent sequence length equals On conn logn conn n conn number connections Another disadvantage high number local minima unusually complex error surface The purpose paper however come efficient introspective selfreferential weight change algorithm show algorithms possible',\n",
              " 'This paper discusses problem implement manytomany multiassociative mapping within connectionist models Traditional symbolic approaches work explicit representation alternatives via stored links implicitly enumerative algorithms Classical pattern association models ignore issue generating multiple outputs single input pattern recent research recurrent networks promising field clearly focused upon multiassociativity goal In paper define multiassociative memory MM several possible variants discuss utility general cognitive modeling We extend sequential cascaded networks Pollack fit task perform several initial experiments demonstrate feasibility concept',\n",
              " 'Human visual systems maintain stable internal representation scene even though image retina constantly changing eye movements Such stabilization theoretically effected dynamic shifts receptive field RF neurons visual system This paper examines neural circuit learn generate shifts The shifts controlled eye position signals compensate movement retinal image caused eye movements The development neural shifter circuit Olshausen Anderson Van Essen modeled using triadic connections These connections gated signals indicate direction gaze eye position signals In simulations neural model exposed sequences stimuli paired appropriate eye position signals The initially',\n",
              " 'This paper tries identify rules factors predictive outcome international conflict management attempts We use C advanced Machine Learning algorithm generating decision trees prediction rules cases CONFMAN database The results show simple patterns rules often understandable also reliable complex rules Simple decision trees able improve chances correctly predicting outcome conflict management attempt This suggests mediation repetitive conflicts per se results achieved far',\n",
              " 'c fl UWCC COMMA Technical Report No February x No part article may reproduced commercial purposes Abstract A technique described allows unimodal function optimization methods extended efficiently locate optima multimodal problems We describe algorithm based traditional genetic algorithm GA This involves iterating GA uses knowledge gained one iteration avoid researching subsequent iterations regions problem space solutions already found This achieved applying fitness derating function raw fitness function fitness values depressed regions problem space solutions already found Consequently likelihood discovering new solution iteration dramatically increased The technique may used various styles GA optimization methods simulated annealing The effectiveness algorithm demonstrated number multimodal test functions The technique least fast fitness sharing methods It provides speedup p problem p optima depending value p convergence time complexity',\n",
              " 'In paper examine intuition TD meant operate approximating asynchronous value iteration We note important class discrete acyclic stochastic tasks value iteration inefficient compared DAGSP algorithm essentially performs one sweep instead many working backwards goal The question address paper whether analogous algorithm used large stochastic state spaces requiring function approximation We present algorithm analyze give comparative results TD several domains state Using VI solve MDPs belonging either special classes quite inefficient since VI performs backups entire space whereas backups useful improving V fl frontier alreadycorrect notyetcorrect V fl values In fact classical algorithms problem classes compute V fl efficiently explicitly working backwards deterministic class Dijkstras shortestpath algorithm acyclic class DirectedAcyclicGraphShortestPaths DAGSP DAGSP first topologically sorts MDP producing linear ordering states every state x precedes states reachable x Then runs list reverse performing one backup per state Worstcase bounds VI Dijkstra DAGSP deterministic domains X states A actionsstate Although presents DAGSP deterministic acyclic problems applies straightforwardly',\n",
              " 'Automatic design optimization highly sensitive problem formulation The choice objective function constraints design parameters dramatically impact computational cost optimization quality resulting design The best formulation varies one application another A design engineer usually know best formulation advance In order address problem developed system supports interactive formulation testing reformulation design optimization strategies Our system includes executable dataflow language representing optimization strategies The language allows engineer define multiple stages optimization using different approximations objective constraints different abstractions design space We also developed set transformations reformulate strategies represented language The transformations approximate objective constraint functions abstractor reparameterize search spaces divide optimization process multiple stages The system applicable principle design problem expressed terms constrained optimization however expect system useful design artifact governed algebraic ordinary differential equations We tested system problems racing yacht design jet engine nozzle design We report experimental results demonstrating reformulation techniques significantly improve performance automatic design optimization Our research demonstrates viability reformulation methodology combines symbolic program transformation numerical experimentation It important first step research program aimed automating entire strategy formulation process fl Fully accepted Research Engineering Design',\n",
              " 'The classification performance neural network combined sixband LandsatTM oneband ERSSAR PRI imagery scene carried Different combinations data either raw segmented filtered using available ground truth polygons training test sets created The training sets used learning test sets used verification neural network The different combinations evaluated',\n",
              " 'The problem modeling complicated data sequences DNA speech often arises practice Most algorithms select hypothesis within model class assuming observed sequence direct output underlying generation process In paper consider case output passes memoryless noisy channel observation In particular show class Markov chains variable memory length learning affected factors despite superpolynomial still small practical cases Markov models variable memory length probabilistic finite suffix automata introduced learning theory Ron Singer Tishby also described polynomial time learning algorithm We present modification algorithm uses noisecorrupted sample knowledge noise structure The algorithm still viable noise known exactly good estimation available Finally experimental results presented removing noise corrupted English text measure performance learning algorithm affected size noisy sample noise rate',\n",
              " 'We present evaluate implemented system rapidly easily build intelligent software agents Webbased tasks Our design centered around two basic functions ScoreThisLink ScoreThisPage If given highly accurate functions standard heuristic search would lead efficient retrieval useful information Our approach allows users tailor systems behavior providing approximate advice functions This advice mapped neural network implementations two functions Subsequent reinforcements Web eg dead links ratings retrieved pages user wishes provide respectively used refine link pagescoring functions Hence architecture provides appealing middle ground nonadaptive agent programming languages systems solely learn user preferences users ratings pages We describe internal representation Web pages major predicates advice language advice mapped neural networks mechanisms refining advice based subsequent feedback We also present case study provide simple advice specialize generalpurpose system homepage finder An empirical study demonstrates approach leads effective homepage finder leading commercial Web search site',\n",
              " 'In concept learning objects domain grouped together based similarity determined attributes used describe Existing concept learners require set attributes known advance presented entirety learning begins Additionally systems possess mechanisms altering attribute set concepts learned Consequently veridical attribute set relevant task concepts used must supplied onset learning turn usefulness concepts limited task attributes originally selected In order efficiently accommodate changing contexts concept learner must able alter set descriptors without discarding prior knowledge domain We introduce notion attributeincrementation dynamic modification attribute set used describe instances problem domain We implemented capability concept learning system evaluated along several dimensions using existing concept formation system com parison',\n",
              " 'It shown Bayesian inference data modeled mixture distribution feasibly performed via Monte Carlo simulation This method exhibits true Bayesian predictive distribution implicitly integrating entire underlying parameter space An infinite number mixture components accommodated without difficulty using prior distribution mixing proportions selects reasonable subset components explain finite training set The need decide correct number components thereby avoided The feasibility method shown empirically simple classification task',\n",
              " 'This article presents new reinforcement learning method called SANE Symbiotic Adaptive NeuroEvolution evolves population neurons genetic algorithms form neural network capable performing task Symbiotic evolution promotes cooperation specialization results fast efficient genetic search discourages convergence suboptimal solutions In inverted pendulum problem SANE formed effective networks times faster Adaptive Heuristic Critic times faster Qlearning GENITOR neuroevolution approach without loss generalization Such efficient learning combined domain assumptions make SANE promising approach broad range reinforcement learning problems including many realworld applications',\n",
              " 'The paper concerns probabilistic evaluation plans presence unmeasured variables plan consisting several concurrent sequential actions We establish graphical criterion recognizing effects given plan predicted passive observations measured variables When criterion satisfied closedform expression provided probability plan achieve specified goal',\n",
              " 'We introduce technique enhance ability dynamic ILP processors exploit speculatively executed parallelism Existing branch prediction mechanisms used establish dynamic window ILP extracted limited abilities create large accurate dynamic window ii initiate large number instructions window every cycle iii traverse multiple branches control flow graph per prediction We introduce control flow prediction uses information control flow graph program overcome limitations We discuss information present control flow graph represented using multiblocks conveyed hardware using Control Flow Tables Control Flow Prediction Buffers We evaluate potential control flow prediction abstract machine dynamic ILP processing model Our results indicate control flow prediction powerful effective assist hardware making informed run time decisions program control flow',\n",
              " 'We develop mean field theory sigmoid belief networks based ideas statistical mechanics Our mean field theory provides tractable approximation true probability distribution networks also yields lower bound likelihood evidence We demonstrate utility framework benchmark problem statistical pattern recognitionthe classification handwritten digits',\n",
              " 'Many learning experience systems use information extracted problem solving experiences modify performance element PE forming new element PE solve similar problems efficiently However transformations improve performance one set problems degrade performance sets new PE always better original PE depends distribution problems We therefore seek performance element whose expected performance distribution optimal Unfortunately actual distribution needed determine element optimal usually known Moreover task finding optimal element even knowing distribution intractable interesting spaces elements This paper presents method palo sidesteps problems using set samples estimate unknown distribution using set transformations hillclimb local optimum This process based mathematically rigorous form utility analysis particular uses statistical techniques determine whether result proposed transformation better original system We also present efficient way implementing learning system context general class performance elements include empirical evidence approach work effectively fl Much work performed University Toronto supported Institute Robotics Intelligent Systems operating grant National Science Engineering Research Council Canada We also gratefully acknowledge receiving many helpful comments William Cohen Dave Mitchell Dale Schuurmans anonymous referees',\n",
              " 'Compositional QLearning CQL Singh modular approach learning perform composite tasks made several elemental tasks reinforcement learning Skills acquired performing elemental tasks also applied solve composite tasks Individual skills compete right act winning skills included decomposition composite task We extend original CQL concept two ways general reward function agent one actuator We use CQL architecture acquire skills performing composite tasks simulated twolinked manipulator large state action spaces The manipulator nonlinear dynamical system require endeffector specific positions workspace Fast function approximation Qmodules achieved use array Cerebellar Model Articulation Controller CMAC Albus Our research interests involve scaling machine learning methods especially reinforcement learning autonomous robot control We interested function approximators suitable reinforcement learning problems large state spaces Cerebellar Model Articulation Controller CMAC Albus permit fast online learning good local generalization In addition interested task decomposition reinforcement learning use hierarchical modular function approximator architectures We examining effectiveness modified Hierarchical Mixtures Experts HME Jordan Jacobs approach reinforcement learning since original HME developed mainly supervised learning batch learning tasks The incorporation domain knowledge reinforcement learning agents important way extending capabilities Default policies specified domain knowledge also used restrict size stateaction space leading faster learning We investigating use QLearning Watkins planning tasks using classifier system Holland encode necessary conditionaction rules Jordan M Jacobs R Hierarchical mixtures experts EM algorithm Technical Report MIT Computational Cognitive Science',\n",
              " 'The processing performed feedforward neural network often interpreted use decision hyperplanes layer The adaptation process however normally explained using picture gradient descent error landscape In paper dynamics decision hyperplanes used model adaptation process A electromechanical analogy drawn dynamics hyperplanes determined interaction forces hyperplanes particles represent patterns Relaxation system determined increasing hyperplane inertia mass This picture used clarify dynamics learning go way explaining learning deadlocks escaping certain local minima Furthermore network plasticity introduced dynamic property system reduction necessary consequence information storage Hyperplane inertia used explain avoid destructive relearning trained networks',\n",
              " 'Modifications Recursive AutoAssociative Memory presented allow store deeper complex data structures previously reported These modifications include adding extra layers compressor reconstructor networks employing integer rather realvalued representations preconditioning weights presetting representations compatible The resulting system tested data set syntactic trees extracted Penn Treebank',\n",
              " 'The problem combining preferences arises several applications combining results different search engines This work describes efficient algorithm combining multiple preferences We first give formal framework problem We describe analyze new boosting algorithm combining preferences called RankBoost We also describe efficient implementation algorithm restricted case We discuss two experiments carried assess performance RankBoost In first experiment used algorithm combine different search strategies query expension given domain For task compare performance RankBoost individual search strategies The second experiment collaborativefiltering task specifically making movie recommendations Here present results comparing RankBoost nearest neighbor regression algorithms',\n",
              " 'This paper shows decision trees used improve performance casebased learning CBL systems We introduce performance task machine learning systems called semiflexible prediction lies classification task performed decision tree algorithms flexible prediction task performed conceptual clustering systems In semiflexible prediction learning improve prediction specific set features known priori rather single known feature classification arbitrary set features conceptual clustering We describe one task natural language processing present experiments compare solutions problem using decision trees CBL hybrid approach combines two In hybrid approach decision trees used specify features included knearest neighbor case retrieval Results experiments show hybrid approach outperforms decision tree casebased approaches well two casebased systems incorporate expert knowledge case retrieval algorithms Results clearly indicate decision trees used improve performance CBL systems without reliance potentially expensive expert knowledge',\n",
              " 'Technical Report No Department Statistics University Toronto We describe linear network models correlations realvalued visible variables using one realvalued hidden variables factor analysis model This model seen linear version Helmholtz machine parameters learned using wakesleep method learning primary generative model assisted recognition model whose role fill values hidden variables based values visible variables The generative recognition models jointly learned wake sleep phases using delta rule This learning procedure comparable simplicity Ojas version Hebbian learning produces somewhat different representation correlations terms principal components We argue simplicity wakesleep learning makes factor analysis plau sible alternative Hebbian learning model activitydependent cortical plasticity',\n",
              " 'A Bayesian method estimating amino acid distributions states hidden Markov model HMM protein family columns multiple alignment family introduced This method uses Dirichlet mixture densities priors amino acid distributions These mixture densities determined examination previously constructed HMMs multiple alignments It shown Bayesian method improve quality HMMs produced small training sets Specific experiments EFhand motif reported priors shown produce HMMs higher likelihood unseen data fewer false positives false negatives database search task',\n",
              " 'This paper proposes simple cost model machine learning applications based notion net present value The model extends unifies models used Pazzani et al Masand PiatetskyShapiro It attempts answer question Should given machine learning system prototype stage fielded The models inputs systems confusion matrix cash flow matrix application cost per decision onetime cost deploying system rate return investment Like Provost Fawcetts ROC convex hull method present model used decisionmaking even input variables known exactly Despite simplicity number nontrivial consequences For example free lunch theorems learning theory longer apply',\n",
              " 'This paper demonstrates use graphs mathematical tool expressing independenices formal language communicating processing causal information statistical analysis We show complex information external interventions organized represented graphically conversely graphical representation used facilitate quantitative predictions effects interventions We first review Markovian account causation show directed acyclic graphs DAGs offer economical scheme representing conditional independence assumptions deducing displaying logical consequences assumptions We introduce manipulative account causation show DAG defines simple transformation tells us probability distribution change result external interventions system Using transformation possible quantify nonexperimental data effects external interventions specify conditions randomized experiments necessary Finally paper offers graphical interpretation Rubins model causal effects demonstrates equivalence manipulative account causation We exemplify tradeoffs two approaches deriving nonparametric bounds treatment effects conditions imperfect compliance',\n",
              " 'In casebased design adaptation design case new design requirements plays important role If sufficient adapt predefined set design parameters task easily automated If however farreaching creative changes required current systems provide limited success This paper describes approach creative design adaptation based notion creativity goal oriented shift focus search process An evolving representation used restructure search space designs similar example case lie focus search This focus used starting point create new designs',\n",
              " 'We consider novel nonlinear model time series analysis The study model emphasizes theoretical aspects well practical applicability The architecture model demonstrated sufficiently rich sense approximating unknown functional forms yet retains simple intuitive characteristics linear models A comparison established nonlinear models emphasized theoretical issues backed prediction results benchmark time series well computer generated data sets Efficient estimation algorithms seen applicable made possible mixture based structure model Large sample properties estimators discussed well well specified well misspecified settings We also demonstrate inference pertaining data structure may made parameterization model resulting better intuitive understanding structure performance model',\n",
              " 'The coverage learning algorithm number concepts learned algorithm samples given size This paper asks whether good learning algorithms designed maximizing coverage The paper extends previous upper bound coverage Boolean concept learning algorithm describes two algorithmsMultiBalls LargeBallwhose coverage approaches upper bound Experimental measurement coverage ID FRINGE algorithms shows coverage far bound Further analysis LargeBall shows although learns many concepts seem interesting concepts Hence coverage maximization alone appear yield practicallyuseful learning algorithms The paper concludes definition coverage within bias suggests way coverage maximization could applied strengthen weak preference biases',\n",
              " 'At previous FOGA workshop presented initial results using Markov models analyze transient behavior genetic algorithms GAs used function optimizers GAFOs In paper states Markov model ordered via simple mathematically convenient lexicographic ordering used initially Nix Vose In paper explore alternative orderings states based interesting semantic properties average fitness degree homogeneity average attractive force etc We also explore lumping techniques reducing size state space Analysis reordered lumped Markov models provides new insights transient behavior GAs general GAFOs particular',\n",
              " 'An important aspect creative design concept emergence Though emergence important mechanism either well understood limited domain shapes This deficiency compensated considering definitions emergent behaviour Artificial Life ALife research community With new insights proposed computational technique called evolving representations design genes extended emergent behaviour We demonstrate emergent behaviour coevolutionary model design This coevolutionary approach design allows solution space structure space evolve response problem space behaviour space Since behaviour space active participant behaviour may emerge new structures end design process This paper hypothesizes emergent behaviour identified using technique The floor plan example Gero Schnier extended demonstrate behaviour emerge coevolutionary design process',\n",
              " 'We investigate learnability PAC model data used learning attributes labels either corrupted incomplete In order prove main results define new complexity measure statistical query SQ learning algorithms The view SQ algorithm maximum queries algorithm number input bits query depends We show restricted view SQ algorithm class general sufficient condition learnability models attribute noise covered missing attributes We show since algorithms question statistical also simultaneously tolerate classification noise Classes results hold therefore learned simultaneous attribute noise classification noise include kDNF ktermDNF DNF representations conjunctions relevant variables uniform distribution decision lists These noise models first PAC models training data attributes labels may corrupted random process Previous researchers shown class kDNF learnable attribute noise attribute noise rate known exactly We show attribute noise learnability results either without classification noise also hold exact noise rate known provided learner instead polynomially good approximation noise rate In addition show results also hold one noise rate distinct noise rate attribute Our results learning random covering require learner told even approximation covering rate addition hold setting distinct covering rates attribute Finally give lower bounds number examples required learning presence attribute noise covering',\n",
              " 'This study describes new Hidden Markov Model HMM system segmenting uncharacterized genomic DNA sequences exons introns intergenic regions Separate HMM modules designed trained specific regions DNA exons introns intergenic regions splice sites The models tied together form biologically feasible topology The integrated HMM trained set eukaryotic DNA sequences tested using segment separate set sequences The resulting HMM system called VEIL Viterbi ExonIntron Locator obtains overall accuracy test data total bases correctly labelled correlation coefficient Using stringent test exact exon prediction VEIL correctly located ends coding exons exons predicts exactly correct These results compare favorably best previous results gene structure prediction demonstrate benefits using HMMs problem',\n",
              " 'Recently increased interest lifelong machine learning methods transfer knowledge across multiple learning tasks Such methods repeatedly found outperform conventional singletask learning algorithms learning tasks appropriately related To increase robustness approaches methods desirable reason relatedness individual learning tasks order avoid danger arising tasks unrelated thus potentially misleading This paper describes taskclustering TC algorithm TC clusters learning tasks classes mutually related tasks When facing new learning task TC first determines related task cluster exploits information selectively task cluster An empirical study carried mobile robot domain shows TC outperforms nonselective counterpart situations small number tasks relevant',\n",
              " 'Belief revision belief update proposed two types belief change serving different purposes Belief revision intended capture changes agents belief state reflecting new information static world Belief update intended capture changes belief response changing world We argue belief revision belief update restrictive routine belief change involves elements We present model generalized update allows updates response external changes inform agent prior beliefs This model update combines aspects revision update providing realistic characterization belief change We show certain assumptions original update postulates satisfied We also demonstrate plain revision plain update special cases model way formally verifies intuition revision suitable static belief change',\n",
              " 'We propose Bayesian framework regression problems covers areas usually dealt function approximation An online learning algorithm derived solves regression problems Kalman filter Its solution always improves increasing model complexity without risk overfitting In infinite dimension limit approaches true Bayesian posterior The issues prior selection overfitting also discussed showing commonly held beliefs misleading The practical implementation summarised Simulations using popular publicly available data sets used demonstrate method highlight important issues concerning choice priors',\n",
              " 'This report deals efficient mapping sparse neural networks CNS We develop parallel vector code idealized sparse network determine performance three memory systems We use code evaluate memory systems one implemented prototype pinpoint bottlenecks current CNS design',\n",
              " 'In nature genotype many organisms exhibits diploidy ie includes two copies every gene In paper describe results simulations comparing behavior haploid diploid populations ecological neural networks living fixed changing environments We show diploid genotypes create variability fitness population haploid genotypes buffer better environmental change consequence one wants obtain good results average peak fitness single population one choose diploid population appropriate mutation rate Some results simulations parallel biological findings',\n",
              " 'The problem belief changehow agent revise beliefs upon learning new informationhas active area research philosophy artificial intelligence Many approaches belief change proposed literature Our goal introduce yet another approach examine carefully rationale underlying approaches already taken literature highlight view methodological problems literature The main message study belief change carefully must quite explicit ontology scenario underlying belief change process This something missing previous work focus postulates Our analysis shows must pay particular attention two issues often taken granted The first model agents epistemic state Do use set beliefs richer structure ordering worlds And use set beliefs language beliefs expressed The second status observations Are observations known true believed In latter case firm belief For example argue even postulates called beyond controversy unreasonable agents beliefs include beliefs epistemic state well external world Issues status observations arise particularly consider iterated belief revision must confront possibility revising',\n",
              " 'The study belief change active area philosophy AI In recent years two special cases belief change belief revision belief update studied detail Roughly speaking revision treats surprising observation sign previous beliefs wrong update treats surprising observation indication world changed In general would expect agent making observation may want revise earlier beliefs assume change occurred world We define novel approach belief change allows us applying ideas probability theory qualitative settings The key idea use qualitative Markov assumption says state transitions independent We show recent approach modeling qualitative uncertainty using plausibility measures allows us make qualitative Markov assumption relatively straightforward way show Markov assumption used provide attractive beliefchange model',\n",
              " 'In reinforcement learning frequently necessary resort approximation true optimal value function Here investigate benefits online search cases We examine local searches agent performs finitedepth lookahead search global searches agent performs search trajectory way current state goal state The key success methods lies taking value function gives rough solution hard problem finding good trajectories every single state combining online search gives accurate solution easier problem finding good trajectory specifically current state',\n",
              " 'Probabilistic contextfree grammars PCFGs provide simple way represent particular class distributions sentences contextfree language Efficient parsing algorithms answering particular queries PCFG ie calculating probability given sentence finding likely parse applied variety patternrecognition problems We extend class queries answered several ways allowing missing tokens sentence sentence fragment supporting queries intermediate structure presence particular nonterminals flexible conditioning variety types evidence Our method works constructing Bayesian network represent distribution parse trees induced given PCFG The network structure mirrors chart standard parser generated using similar dynamicprogramming approach We present algorithm constructing Bayesian networks PCFGs show queries patterns queries network correspond interesting queries PCFGs',\n",
              " 'This paper presents recent developments toward formalism combines useful properties logic probabilities Like logic formalism admits qualitative sentences provides symbolic machinery deriving deductively closed beliefs like probability permits us express ifthen rules different levels firmness retract beliefs response changing observations Rules interpreted orderofmagnitude approximations conditional probabilities impose constraints rankings worlds Inferences supported unique priority ordering rules syntactically derived knowledge base This ordering accounts rule interactions respects specificity considerations facilitates construction coherent states beliefs Practical algorithms developed analyzed testing consistency computing rule ordering answering queries Imprecise observations incorporated using qualitative versions Jeffreys Rule Bayesian updating result coherent belief revision embodied naturally tractably Finally causal rules interpreted imposing Markovian conditions constrain world rankings reflect modularity causal organizations These constraints shown facilitate reasoning causal projections explanations actions change',\n",
              " 'Clay evolutionary architecture autonomous robots integrates motor schemabased control reinforcement learning Robots utilizing Clay benefit realtime performance motor schemas continuous dynamic environments taking advantage adaptive reinforcement learning Clay coordinates assemblages groups motor schemas using embedded reinforcement learning modules The coordination modules activate specific assemblages based presently perceived situation Learning occurs robot selects assemblages samples reinforcement signal time Experiments robot soccer simulation illustrate performance utility system',\n",
              " 'Most known learning algorithms dynamic neural networks nonstationary environments need global computations perform credit assignment These algorithms either local time local space Those algorithms local time space usually deal sensibly hidden units In contrast far judge learning rules biological systems many hidden units local space time In paper propose parallel online learning algorithm performs local computations yet still designed deal hidden units units whose past activations hidden time The approach inspired Hollands idea bucket brigade classifier systems transformed run neural network fixed topology The result feedforward recurrent neural dissipative system consuming weightsubstance permanently trying distribute substance onto connections appropriate way Simple experiments demonstrating feasability algorithm reported',\n",
              " 'Although creativity largely studied problem solving contexts creativity consists generative component comprehension component In particular creativity essential part reading understanding natural language stories We formalized understanding process developed algorithm capable producing creative understanding behavior We also created novel knowledge organization scheme assist process Our model creativity implemented portion ISAAC Integrated Story Analysis And Creativity reading system system models creative reading science fiction stories',\n",
              " 'Creative designers often see solutions pending design problems everyday objects surrounding This often lead innovation insight sometimes revealing new functions purposes common design pieces process We interested modeling serendipitous recognition solutions pending problems context creative mechanical design This paper characterizes ability analyzing observations made placing context forms recognition We propose computational model capture explore serendipitous recognition based ideas reconstructive dynamic memory situation assessment casebased reasoning',\n",
              " 'In paper analyze two wellknown measures attribute selection decision tree induction informativity gini index In particular interested influence different methods estimating probabilities two measures The results experiments show different measures obtained different probability estimation methods determine preferential order attributes given node Therefore determine structure constructed decision tree This feature beneficial especially realworld applications several different trees often required',\n",
              " 'We consider learning situations function used classify examples may switch back forth small number different concepts course learning We examine several models situations oblivious models switches made independent selection examples adversarial models single adversary controls concept switches example selection We show relationships benign models pconcepts Kearns Schapire present polynomialtime algorithms learning switches two kDNF formulas For adversarial model present model success patterned popular competitive analysis used studying online algorithms We describe randomized query algorithm adversarial switches two monotone disjunctions competitive total number mistakes plus queries high probability bounded number switches plus fixed polynomial n number variables We also use notions described provide sufficient conditions learning pconcept class decision rule implies able learn class model probability',\n",
              " 'Case based systems typically retrieve cases case base applying similarity measures The measures usually constructed ad hoc manner This report presents toolbox systematic construction similarity measures In addition paving way design methodology similarity measures systematic approach facilitates identification opportunities parallelisation case base retrieval',\n",
              " 'In real world applications software engineers recognise use memory must organised via data structures software using data must independant data structures implementation details They achieve using abstract data structures records files buffers We demonstrate genetic programming automatically implement simple abstract data structures considering detail task evolving list We show general reasonably efficient implementations automatically generated simple primitives A model maintaining evolved code demonstrated using list problem Much published work genetic programming GP evolves functions without sideeffects learn patterns test data In contrast human written programs often make extensive explicit use memory Indeed memory form required programming system Turing Complete ie possible write computable program system However inclusion memory make interactions parts programs much complex make harder produce programs Despite shown GP automatically create programs explicitly use memory Teller In normal genetic programming considerable benefits found adopting structured approach For example Koza shows introduction evolvable code modules automatically defined functions ADFs greatly help GP reach solution We suggest corresponding structured approach use data similarly significant advantage GP Earlier work demonstrated genetic programming automatically generate simple abstract data structures namely stacks queues Langdon That GP evolve programs organise memory accessed via simple read write primitives data structures used external software without needing know implemented This chapter shows possible evolve list data structure basic primitives Aho Hopcroft Ullman suggest three different ways implement list experiments show GP evolve implementation This requires list components agree one implementation coevolve together Section describes GP architecture including use Pareto multiple component fitness scoring measures aimed speeding GP search The evolved solutions described Section Section presents candidate model maintaining evolved software This followed discussion learned conclusions drawn',\n",
              " 'Report SYCON ABSTRACT We pursue particular approach analog computation based dynamical systems type used neural networks research Our systems fixed structure invariant time corresponding unchanging number neurons If allowed exponential time computation turn unbounded power However polynomialtime constraints limits capabilities though powerful Turing Machines A similar restricted model shown polynomialtime equivalent classical digital computation previous work Moreover precise correspondence nets standard nonuniform circuits equivalent resources consequence one lower bound constraints compute This relationship perhaps surprising since analog devices change manner input size We note networks likely solve polynomially NPhard problems equality p np model implies almost complete collapse standard polynomial hierarchy',\n",
              " 'We introduce convergence diagnostic procedure MCMC operates estimating total variation distances distribution algorithm certain numbers iterations The method advantages many existing methods terms applicability utility computational expense interpretability It used assess convergence marginal joint posterior densities show applied two commonly used MCMC samplers Gibbs Sampler Metropolis Hastings algorithm Illustrative examples highlight utility interpretability proposed diagnostic also highlight limitations',\n",
              " 'Because distance skull brain different resistivities electroencephalographic EEG data collected point human scalp includes activity generated within large brain area This spatial smearing EEG data volume conduction involve significant time delays however suggesting Independent Component Analysis ICA algorithm Bell Sejnowski suitable performing blind source separation EEG data The ICA algorithm separates problem source identification source localization First results applying ICA algorithm EEG eventrelated potential ERP data collected sustained auditory detection task show ICA training insensitive different random seeds ICA may used segregate obvious artifactual EEG components line muscle noise eye movements sources ICA capable isolating overlapping EEG phenomena including alpha theta bursts spatiallyseparable ERP components separate ICA channels Nonstationarities EEG behavioral state tracked using ICA via changes amount residual correlation ICAfiltered output channels',\n",
              " 'Miller G The magical number seven plus minus two Some limits capacity processing information The Psychological Review Schmidhuber J b Towards compositional learning dynamic neural networks Technical Report FKI Technische Universitat Munchen Institut fu Informatik ServanSchreiber D Cleermans A McClelland J Encoding sequential structure simple recurrent networks Technical Report CMUCS Carnegie Mellon University Computer Science Department',\n",
              " 'The standard approach decision tree induction topdown greedy algorithm makes locally optimal irrevocable decisions node tree In paper study alternative approach algorithms use limited lookahead decide test use node We systematically compare using large number decision trees quality decision trees induced greedy approach trees induced using lookahead The main results experiments greedy approach produces trees accurate trees produced much expensive lookahead step ii decision tree induction exhibits pathology sense lookahead produce trees larger less accurate trees produced without',\n",
              " 'This thesis presents machine learning model capable extracting discrete classes continuous valued input features This done using neurally inspired novel competitive classifier CC feeds discrete classifications forward supervised machine learning model The supervised learning model uses discrete classifications perhaps information available solve problem The supervised learner generates feedback guide CC potentially useful classifications continuous valued input features Two supervised learning models combined CC creating ASOCSAFE IDAFE Both models simulated results analyzed Based results several areas future research proposed',\n",
              " 'We frequently called upon perform multiple tasks compete attention resource Often know optimal solution task isolation paper describe knowledge exploited efficiently find good solutions tasks parallel We formulate problem dynamically merging multiple Markov decision processes MDPs composite MDP present new theoreticallysound dynamic programming algorithm finding optimal policy composite MDP We analyze various aspects algorithm Every day faced problem multiple tasks parallel competes attention resource If running job shop must decide machines allocate jobs order jobs miss deadlines If mail delivery robot must find intended recipients mail simultaneously avoiding fixed obstacles walls mobile obstacles people still manage keep sufficiently charged Frequently know perform task isolation paper considers take information individual tasks combine efficiently find optimal solution entire set tasks parallel More importantly describe theoreticallysound algorithm merging dynamically new tasks new job arrival job shop assimilated online solution found ongoing set simultaneous tasks illustrate use simple merging problem',\n",
              " 'We consider problem fitting n fi n distance matrix D tree metric T Let distance closest tree metric min T fk T D k g First present On algorithm finding additive tree T k T D k giving first algorithm problem performance guarantee Second show N Phard find tree T k T D k lt',\n",
              " 'CaseBased Planning CBP provides way scaling domainindependent planning solve large problems complex domains It replaces detailed lengthy search solution retrieval adaptation previous planning experiences In general CBP demonstrated improve performance generative fromscratch planning However performance improvements provides dependent adequate judgements problem similarity In particular although CBP may substantially reduce planning effort overall subject misretrieval problem The success CBP depends retrieval errors relatively rare This paper describes design implementation replay framework casebased planner dersnlpebl dersnlpebl extends current CBP methodology incorporating explanationbased learning techniques allow explain learn retrieval failures encounters These techniques used refine judgements case similarity response feedback wrong decision made The failure analysis used building case library addition repairing cases Large problems split stored single goal subproblems Multigoal problems stored smaller cases fail merged full solution An empirical evaluation approach demonstrates advantage learning experienced retrieval failure',\n",
              " 'Modern processors improve instruction level parallelism speculation The outcome data control decisions predicted operations speculatively executed committed original predictions correct There number ways processor resources could used threading eager execution As use speculation increases believe processors need form speculation control balance benefits speculation possible activities Confidence estimation one technique exploited architects speculation control In paper introduce performance metrics compare confidence estimation mechanisms argue metrics appropriate speculation control We compare number confidence estimation mechanisms focusing mechanisms small implementation cost gain benefit exploiting characteristics branch predictors clustering mispredicted branches We compare performance different confidence estimation methods using detailed pipeline simulations Using simulations show improve confidence estimators providing better insight future investigations comparing applying confidence estimators',\n",
              " 'Relational learning algorithms special interest members machine learning community offer practical methods extending representations used algorithms solve supervised learning tasks Five approaches currently explored address issues involved using relational representations This paper surveys algorithms embodying approaches summarizes empirical evaluations highlights commonalities suggests potential directions future research',\n",
              " 'The learning process Boltzmann Machines computationally expensive The computational complexity exact algorithm exponential number neurons We present new approximate learning algorithm Boltzmann Machines based mean field theory linear response theorem The computational complexity algorithm cubic number neurons In absence hidden units show weights directly computed fixed point equation learning rules Thus case need use gradient descent procedure learning process We show solutions method close optimal solutions give significant improvement correlations play significant role Finally apply method pattern completion task show good performance networks neurons',\n",
              " 'This paper introduces methodology solving combinatorial optimization problems application reinforcement learning methods The approach applied cases several similar instances combinatorial optimization problem must solved The key idea analyze set training problem instances learn search control policy solving new problem instances The search control policy twin goals finding highquality solutions finding quickly Results applying methodology NASA scheduling problem show learned search control policy much effective best known nonlearning search procedurea method based simulated annealing',\n",
              " 'Markov decision processes MDPs undiscounted rewards represent important class problems decision control The goal learning MDPs find policy yields maximum expected return per unit time In large state spaces computing averages directly feasible instead agent must estimate stochastic exploration state space In case longer exploration times enable accurate estimates informed decisionmaking The learning curve MDP measures agents performance depends allowed exploration time T In paper analyze learning curves simple control problem undiscounted rewards In particular methods statistical mechanics used calculate lower bounds agents performance thermodynamic limit T N ff T N finite T number time steps allotted per policy evaluation N size state space In limit provide lower bound return policies appear optimal based imperfect statistics',\n",
              " 'One effectively utilize predicated execution improve branch handling instructionlevel parallel processors Although potential benefits predicated execution high tradeoffs involved design instruction set support predicated execution difficult On one end design spectrum architectural support full predicated execution requires increasing number source operands instructions Full predicate support provides flexibility largest potential performance improvements On end partial predicated execution support conditional moves requires little change existing architectures This paper presents preliminary study qualitatively quantitatively address benefit full partial predicated execution support With current compiler technology show compiler use partial full predication achieve speedup large controlintensive programs Some details code generation techniques shown provide insight benefit going partial full predication Preliminary experimental results encouraging partial predication provides average performance improvement issue processor predicate support full predication provides additional improvement',\n",
              " 'This paper studies selfdirected learning variant online learning model learner selects presentation order instances We give tight bounds complexity selfdirected learning concept classes monomials kterm DNF formulas orthogonal rectangles f ng These results demonstrate number mistakes selfdirected learning surprisingly small We prove model selfdirected learning powerful commonly used online query learning models Next explore relationship complexity selfdirected learning VapnikChervonenkis dimension Finally explore relationship Mitchells version space algorithm existence selfdirected learning algorithms make mistakes fl Supported part GE Foundation Junior Faculty Grant NSF Grant CCR Part research conducted author MIT Laboratory Computer Science supported NSF grant DCR grant Siemens Corporation Net address sgcswustledu',\n",
              " 'A lowerbound result power Abstract This paper presents lowerbound result computational power genetic algorithm context combinatorial optimization We describe new genetic algorithm merged genetic algorithm prove class monotonic functions algorithm finds optimal solution exponential convergence rate The analysis pertains ideal behavior algorithm main task reduces showing convergence probability distributions search space combinatorial structures optimal one We take exponential convergence indicative efficient solvability samplebounded algorithm although sampling theory needed better relate limit behavior actual behavior The paper concludes discussion immediate problems lie ahead genetic algorithm',\n",
              " 'In paper study forecasting model based mixture experts predicting French electric daily consumption energy We split task two parts Using mixture experts first model predicts electricity demand exogenous variables temperature degree cloud cover viewed nonlinear regression model mixture Gaussians Using single neural network second model predicts evolution residual error first one viewed nonlinear autoregression model We analyze splitting input space generated mixture experts model compare performance models presently used',\n",
              " 'Suttons TD metho aims provide represen tation cost function absorbing Mark ov chain transition costs A simple example given represen tation obtained dep ends For represen tation optimal resp ect least squares error criterion decreases towards represen tation becomes progressiv ely worse cases poor The example suggests need understand better circumstances TD Qlearning obtain satisfactory neural net workbased compact represen tations cost function A variation TD also prop osed performs b etter example',\n",
              " 'Casebased reasoning involves reasoning cases specific pieces experience reasoners anothers used solve problems We use term graphstructured representations capable expressing relations two objects case allow set relations used vary case case allow set possible relations expanded necessary describe new cases Such representations implemented example semantic networks lists concrete propositions logic We believe graphstructured representations offer significant advantages thus investigating ways implement representations efficiently We make casebased argument using examples two systems chiron caper show graphstructured representation supports two different kinds casebased planning two different domains We discuss costs associated graphstructured representations describe approach reducing costs imple mented caper',\n",
              " 'The advantage using linear regression leaves regression tree analysed paper It carried modification affects construction pruning interpretation regression tree The modification tested artificial reallife domains impact classification error stability induced trees considered The results show modification beneficial leads smaller classification errors induced regression trees The Bayesian approach estimation class distributions used experiments',\n",
              " 'General Theory Quantitative Results Abstract The human genotype represents ten billion binary informations whereas human brain contains million times billion synapses So differentiated brain structure essentially due selforganization Such selforganization relevant areas ranging medicine design intelligent complex systems Many brain structures emerge collective phenomenon microscopic neurosynaptic dynamics stochastic dynamics mimics neuronal action potentials synaptic dynamics modeled local coupling dynamics type Hebbrule synaptic efficiency increases coincident spiking pre postsynaptic neuron The microscopic dynamics transformed collective dynamics reminiscent hydrodynamics The theory models empirical findings quantitatively Topology preserving neuronal maps assumed Descartes selforganization suggested Weiss empirical observation reported Marshall shown neurosynaptically stable due ubiquitous infinitesimal short range electrical chemical leakage In visual cortex neuronal stimulus orientation preference emerges empirically measured orientation patterns determined Poisson equation electrostatics Poisson equation orientation pattern emergence derived Complex cognitive abilities emerge basic local synaptic changes regulated valuation emergent valuation attention attention focus combination subnetworks Altogether general theory presented emergence functionality synaptic growth neurobiological systems The theory provides transformation collective dynamics used quantitative modeling empirical data',\n",
              " 'A Methodology Evaluating Theory Revision Systems Results Abstract Theory revision systems learning systems goal making small changes original theory account new data A measure distance two theories proposed This measure corresponds minimum number edit operations literal level required transform one theory another By computing distance original theory revised theory claim theory revision system makes revisions theory may quantitatively evaluated We present data using accuracy distance metric Audrey II Audrey II fl',\n",
              " 'We present novel data mining approach based decomposition In order analyze given dataset method decomposes hierarchy smaller less complex datasets analyzed independently The method experimentally evaluated realworld housing loans allocation dataset showing decomposition discover meaningful intermediate concepts decompose relatively complex dataset datasets easy analyze comprehend derive classifier high classification accuracy We also show human interaction positive effect comprehensibility classification accuracy',\n",
              " 'Most empirical evaluations machine learning algorithms case studies evaluations multiple algorithms multiple databases Authors case studies implicitly explicitly hypothesize pattern results often suggests one algorithm performs significantly better others limited small number databases investigated instead holds general class learning problems However hypotheses rarely supported additional evidence leaves suspect This paper describes empirical method generalizing results case studies example application This method yields rules describing algorithms significantly outperform others dependent measures Advantages generalizing case studies limitations particular approach also described',\n",
              " 'Learning multiple descriptions class data shown reduce generalization error amount error reduction varies greatly domain domain This paper presents novel empirical analysis helps understand variation Our hypothesis amount error reduction linked degree descriptions class make errors correlated manner We present precise novel definition notion use twentynine data sets show amount observed error reduction negatively correlated degree descriptions make errors correlated manner We empirically show possible learn descriptions make less correlated errors domains many ties search evaluation measure eg information gain experienced learning The paper also presents results help understand multiple descriptions help irrelevant attributes much help large amounts class noise',\n",
              " 'This paper presents Plannett system combines artificial neural networks achieve expert level accuracy difficult scientific task recognizing volcanos radar images surface planet Venus Plannett uses ANNs vary along two dimensions set input features used train number hidden units The ANNs combined simply averaging output activations When Plannett used classification module threestage image analysis system called JAR tool endtoend accuracy sensitivity specificity good human planetary geologist fourimage test suite JARtoolPlannett also achieves best algorithmic accuracy images date',\n",
              " 'Planning learning multiple levels temporal abstraction key problem artificial intelligence In paper summarize approach problem based mathematical framework Markov decision processes reinforcement learning Conventional modelbased reinforcement learning uses primitive actions last one time step modeled independently learning agent These generalized macro actions multistep actions specified arbitrary policy way completing Macro actions generalize classical notion macro operator closed loop uncertain variable duration Macro actions needed represent commonsense higherlevel actions going lunch grasping object traveling distant city This paper generalizes prior work temporally abstract models Sutton extends prediction setting include actions control planning We define semantics models macro actions guarantees validity planning using models This paper present new results theory planning macro actions illustrates potential advantages gridworld task',\n",
              " 'This paper reviews five approximate statistical tests determining whether one learning algorithm outperforms another particular learning task These tests compared experimentally determine probability incorrectly detecting difference difference exists type I error Two widelyused statistical tests shown high probability Type I error certain situations never used These tests test difference two proportions b paireddifferences test based taking several random traintest splits A third test paireddifferences test based fold crossvalidation exhibits somewhat elevated probability Type I error A fourth test McNemars test shown low Type I error The fifth test new test xcv based iterations fold crossvalidation Experiments show test also acceptable Type I error The paper also measures power ability detect algorithm differences exist tests The crossvalidated test powerful The xcv test shown slightly powerful McNemars test The choice best test determined computational cost running learning algorithm For algorithms executed McNemars test test acceptable Type I error For algorithms executed ten times xcv test recommended slightly powerful directly measures variation due choice training set',\n",
              " 'Many classification algorithms passive assign classlabel instance based description given even description incomplete In contrast active classifier cost obtain values missing attributes deciding upon class label The expected utility using active classifier depends cost required obtain additional attribute values penalty incurred outputs wrong classification This paper considers problem learning nearoptimal active classifiers using variant probablyapproximatelycorrect PAC model After defining framework perhaps main contribution paper describe situation task achieved efficiently show task often intractable',\n",
              " 'Probabilistic inference algorithms finding probable explanation maximum aposteriori hypothesis maximum expected utility updating belief reformulated eliminationtype algorithm called bucket elimination This emphasizes principle common many algorithms appearing literature clarifies relationship nonserial dynamic programming algorithms We also present general way combining conditioning elimination within framework Bounds complexity given algorithms function problems struc ture',\n",
              " 'This article published Sociological Methodology edited Peter V Marsden Cambridge Mass Blackwells Adrian E Raftery Professor Statistics Sociology Department Sociology DK University Washington Seattle WA This research supported NIH grant RHD I would like thank Robert Hauser Michael Hout Steven Lewis Scott Long Diane Lye Peter Marsden Bruce Western Yu Xie two anonymous reviewers detailed comments earlier version I also grateful Clem Brooks Sir David Cox Tom DiPrete John Goldthorpe David Grusky Jennifer Hoeting Robert Kass David Madigan Michael Sobel Chris Volinsky helpful discussions correspondence',\n",
              " 'In paper propose family algorithms combining treeclustering conditioning trade space time Such algorithms useful reasoning probabilistic deterministic networks well accomplishing optimization tasks By analyzing problem structure possible select spectrum algorithm best meets given timespace specifica tion',\n",
              " 'In paper propose new approach probabilistic inference belief networks global conditioning simple generalization Pearls b method loopcutset conditioning We show global conditioning well loopcutset conditioning thought special case method Lauritzen Spiegelhalter refined Jensen et al b Nonetheless approach provides new opportunities parallel processing case sequential processing tradeoff time memory We also show hybrid method Suermondt others combining loopcutset conditioning Jensens method viewed within framework By exploring relationships methods develop unifying framework advantages approach combined successfully',\n",
              " 'The dynamics collective properties feedback networks spiking neurons investigated Special emphasis given potential computational role subthreshold oscillations It shown model systems integrateandfire neurons function associative memories two distinct levels On first level binary patterns represented spike activity fire fire On second level analog patterns encoded relative firing times individual spikes spikes underlying subthreshold oscillation Both coding schemes may coexist within network The results suggest cortical neurons may perform broad spectrum associative computations far beyond scope traditional firingrate picture',\n",
              " 'This paper considers new method maintaining diversity creating subpopulations standard generational evolutionary algorithm Unlike methods replaces concept distance individuals tag bits identify subpopulation individual belongs Two variations method presented illustrating feasibility approach',\n",
              " 'Ideally pattern recognition machines provide constant output inputs transformed group G desired invariances These invariances achieved enhancing training data include examples inputs transformed elements G leaving corresponding targets unchanged Alternatively cost function training include regularization term penalizes changes output input transformed group This paper relates two approaches showing precisely sense regularized cost function approximates result adding transformed distorted examples training data The cost function enhanced training set equivalent sum original cost function plus regularizer For unbiased models regularizer reduces intuitively obvious choice term penalizes changes output inputs transformed group For infinitesimal transformations coefficient regularization term reduces variance distortions introduced training data This correspondence provides simple bridge two approaches',\n",
              " 'A new method proposed exploiting causal independencies exact Bayesian network inference A Bayesian network viewed representing factorization joint probability multiplication set conditional probabilities We present notion causal independence enables one factorize conditional probabilities combination even smaller factors consequently obtain finergrain factorization joint probability The new formulation causal independence lets us specify conditional probability variable given parents terms associative commutative operator sum max contribution parent We start simple algorithm VE Bayesian network inference given evidence query variable uses factorization find posterior distribution query We show algorithm extended exploit causal independence Empirical studies based CPCS networks medical diagnosis show method efficient previous methods allows inference larger networks previous algorithms',\n",
              " 'Our goal develop hybrid cognitive model humans acquire skills complex cognitive tasks We pursuing goal designing hybrid computational architectures NRL Navigation task requires competent sensorimotor coordination In paper describe results directly fitting human execution data task We next present empirically compare two methods modeling control knowledge acquisition reinforcement learning novel variant action models human learning task The paper concludes experimental demonstration impact background knowledge system performance Our results indicate performance action models approach closely approximates rate human learning task reinforcement learning',\n",
              " 'The statistical query learning model viewed tool creating demonstrating existence noisetolerant learning algorithms PAC model The complexity statistical query algorithm conjunction complexity simulating SQ algorithms PAC model noise determine complexity noisetolerant PAC algorithms produced Although roughly optimal upper bounds shown complexity statistical query learning corresponding noisetolerant PAC algorithms optimal due inefficient simulations In paper provide improved simulations new variant statistical query model order overcome inefficiencies We improve time complexity classification noise simulation statistical query algorithms Our new simulation roughly optimal dependence noise rate We also derive simpler proof statistical queries simulated presence classification noise This proof makes fewer assumptions queries therefore allows one simulate general types queries We also define new variant statistical query model based relative error show variant natural strictly powerful standard additive error model We demonstrate efficient PAC simulations algorithms new model give general upper bounds learning relative error statistical queries PAC simulation We show statistical query algorithm simulated PAC model malicious errors way resultant PAC algorithm roughly optimal tolerable malicious error rate sample complexity Finally generalize types queries allowed statistical query model We discuss advantages allowing generalized queries show results improved simulations also hold queries This paper available Center Research Computing Technology Division Applied Sciences Harvard University technical report TR',\n",
              " 'This paper outlines problems may occur Reduced Error Pruning relational learning algorithms notably efficiency Thereafter new method Incremental Reduced Error Pruning proposed attempts address problems Experiments show many noisy domains method much efficient alternative algorithms along slight gain accuracy However experiments show well use algorithm recommended domains require specific concept description',\n",
              " 'One kind prosodic structure apparently underlies music language meter Yet detailed measurements music speech show nested periodicities define metrical structure noisy sense What kind system could produce perceive variable metrical timing And would take store particular metrical patterns longterm memory system We developed network coupled oscillators produces perceives metrical patterns pulses In addition beginning initial state biases learns prefer beat patterns like waltzes beat patterns Models general class could learn entrain musical patterns And given way process speech extract appropriate pulses model applicable metrical structure speech well Is language metrical Meter refers particular sorts patterns time abstract description patterns potentially cognitive representation In cases two hierarchical levels equally spaced events occur periods characterizing levels integral multiples usually The hierarchy implied standard Western musical notation different levels indicated kinds notes quarter notes half notes etc bars separating measures For example basic waltztime meter individual beats spacing grouped sets three every third one receiving stronger accent In meter hierarchy consisting faster periodic cycle beat level slower one measure level fast onset zero phase angle coinciding zero phase angle every third beat Metrical systems like seem underlie forms music around world often said underlie human speech well Jones Martin However awkward difficulty definition employs notion integer since data music speech show clearly perfect temporal ratios predicted definition observed performance In music performance various kinds systematic temporal deviations timing specified musical notation known',\n",
              " 'We propose model abduction based revision epistemic state agent Explanations must sufficient induce belief sentence explained instance observation ensure consistency beliefs manner adequately accounts factual hypothetical sentences Our model generate explanations nonmonotonically predict observation thus generalizing current accounts require deductive relationship explanation observation It also provides natural preference ordering explanations defined terms normality plausibility To illustrate generality approach reconstruct two key paradigms modelbased diagnosis abductive consistencybased diagnosis within framework This reconstruction provides alternative semantics extends systems accommodate predictive explanations semantic preferences explanations It also illustrates general information incorporated principled manner fl Some parts paper appeared preliminary form Abduction Belief Revision A Model Preferred Explanations Proc Eleventh National Conf Artificial Intelligence AAAI Washington DC pp',\n",
              " 'Report R ISRN SICSRSE ISSN Abstract The optimal probability activation corresponding performance studied three designs Sparse Distributed Memory namely Kanervas original design Jaeckels selectedcoordinates design Karlssons modifi cation Jaeckels design We assume hard locations Karlssons case masks storage addresses stored data randomly chosen consider different levels random noise reading address',\n",
              " 'Report R ISRN SICSRSE ISSN Abstract We consider sparse distributed memory randomly chosen hard locations unknown number T random data vectors stored A method given estimate T content memory high accuracy In fact estimate unbiased coefficient variation roughly inversely proportional p MU M number hard locations memory U length data accuracy made arbitrarily high making memory big enough A consequence good reading methods used without need special extra location introduced',\n",
              " 'We describe rankedmodel semantics ifthen rules admitting exceptions provides coherent framework many facets evidential causal reasoning Rule priorities automatically extracted form knowledge base facilitate construction retraction plausible beliefs To represent causation formalism incorporates principle Markov shielding imposes stratified set independence constraints rankings interpretations We show formalism resolves classical problems associated specificity prediction abduction offers natural way unifying belief revision belief update reasoning actions',\n",
              " 'We build mathematical connection ExpectationMaximization EM algorithm gradientbased approaches maximum likelihood learning finite Gaussian mixtures We show EM step parameter space obtained gradient via projection matrix P provide explicit expression matrix We analyze convergence EM terms special properties P provide new results analyzing effect P likelihood surface Based mathematical results present comparative discussion advantages disadvantages EM algorithms learning Gaussian mixture models',\n",
              " 'A first order regression algorithm capable handling realvalued continuous variables introduced applications presented Regressional learning assumes realvalued class discrete realvalued variables The algorithm combines regressional learning standard ILP concepts first order concept description background knowledge A clause generated successively refining initial clause adding literals form A v discrete attributes A v A v realvalued attributes background knowledge literals clause body The algorithm employs covering approach beam search heuristic impurity function stopping criteria based local improvement minimum number examples maximum clause length minimum local improvement minimum description length allowed error variable depth An outline algorithm results systems application artificial realworld domains presented The realworld domains comprise modelling water behavior surge tank modelling workpiece roughness steel grinding process modelling operators behavior process electrical discharge machining Special emphasis given evaluation obtained models domain experts comments aspects practical use induced knowledge The results obtained knowledge acquisition process show several important guidelines knowledge acquisition concerning mainly process interaction domain experts exposing primarily importance comprehensibility induced knowledge',\n",
              " 'We address problem measuring degree hemispheric organization asymmetry organization computational model bihemispheric cerebral cortex A theoretical framework measures developed used produce algorithms measuring degree organization symmetry lateralization topographic map formation The performance resulting measures tested several topographic maps obtained selforganization initially random network results compared subjective assessments made humans It found closest agreement human assessments obtained using organization measures based sigmoidtype error averaging Measures developed correct large constant displacements well curving hemispheric topographic maps',\n",
              " 'Learning structure temporallyextended sequences difficult computational problem fraction relevant information available instant Although variants back propagation principle used find structure sequences practice sufficiently powerful discover arbitrary contingencies especially spanning long temporal intervals involving high order statistics For example designing connectionist network music composition encountered problem net able learn musical structure occurs locally timeeg relations among notes within musical phrasebut structure occurs longer time periodseg relations among phrases To address problem require means constructing reduced description sequence makes global aspects explicit readily detectable I propose achieve using hidden units operate different time constants Simulation experiments indicate slower timescale hidden units able pick global structure structure simply learned standard Many patterns world intrinsically temporal eg speech music unfolding events Recurrent neural net architectures devised accommodate timevarying sequences For example architecture shown Figure map sequence inputs sequence outputs Learning structure temporallyextended sequences difficult computational problem input pattern may contain taskrelevant information instant Thus back propagation',\n",
              " 'An incremental higherorder nonrecurrent neuralnetwork combines two properties found useful sequence learning neuralnetworks higherorder connections incremental introduction new units The incremental higherorder neuralnetwork adds higher orders needed adding new units dynamically modify connection weights The new units modify weights next timestep information previous step Since theoretically unlimited number units added network information arbitrarily distant past brought bear prediction Temporal tasks thereby learned without use feedback contrast recurrent neuralnetworks Because recurrent connections training simple fast Experiments demonstrated speedups two orders magnitude recurrent networks',\n",
              " 'In complex models like hidden Markov chains convergence MCMC algorithms used approximate posterior distribution Bayes estimates parameters interest must controlled robust manner We propose paper series online controls rely classical nonparametric tests evaluate independence startup distribution stability Markov chain asymptotic normality These tests lead graphical control spreadsheets presented setup normal mixture hidden Markov chains compare full Gibbs sampler aggregated Gibbs sampler based forwardbackward formulae',\n",
              " 'Three different methods investigated determine ability detect classify various categories diffuse liver disease A statistical method ie discriminant analysis supervised neural network called backpropagation nonsupervised selforganizing feature map examined The investigation performed basis previously selected set acoustic image texture parameters The limited number patients successfully extended generating additional independent data identical statistical properties The generated data used training test sets The final test made original patient data validation set It concluded neural networks attractive alternative traditional statistical techniques dealing medical detection classification tasks Moreover use generated data training networks discriminant classifier shown justified profitable',\n",
              " 'Nonlinear extensions oneunit multiunit Principal Component Analysis PCA neural networks introduced earlier authors reviewed The networks nonlinear Hebbian learning rules related signal expansions like Projection Pursuit PP Independent Component Analysis ICA Separation results mixtures real world signals im ages given',\n",
              " 'Many hormones physiological processes vary circadian pattern Although sinecosine function used model patterns functional form appropriate asymmetry peak nadir phases In paper describe semiparametric periodic spline function fit circadian rhythms The model includes phase amplitude time magnitude peak nadir estimated We also describe tests fit components model Data experiment study immunological responses humans used demonstrate methods',\n",
              " 'We present highlevel decompositionbased algorithms largescale blockangular optimization problems containing integer variables demonstrate effectiveness solution largescale graph partitioning problems These algorithms combine subproblemcoordination paradigm lower bounds pricedirective decomposition methods knapsack genetic approaches utilization building blocks partial solutions Even graph partitioning problems requiring billions variables standard formulation approach produces highquality solutions measured deviations easily computed lower bound substantially outperforms widelyused graph partitioning techniques based heuristics spectral methods',\n",
              " 'Maps regional morbidity mortality rates useful tools determining spatial patterns disease Combined sociodemographic census information also permit assessment environmental justice ie whether certain subgroups suffer disproportionately certain diseases adverse effects harmful environmental exposures Bayes empirical Bayes methods proven useful smoothing crude maps disease risk eliminating instability estimates lowpopulation areas maintaining geographic resolution In paper extend existing hierarchical spatial models account temporal effects spatiotemporal interactions Fitting resulting highlyparametrized models requires careful implementation Markov chain Monte Carlo MCMC methods well novel techniques model evaluation selection We illustrate approach using dataset countyspecific lung cancer rates state Ohio period',\n",
              " 'A novel unsupervised neural network dimensionality reduction seeks directions emphasizing multimodality presented connection exploratory projection pursuit methods discussed This leads new statistical insight synaptic modification equations governing learning Bienenstock Cooper Munro BCM neurons The importance dimensionality reduction principle based solely distinguishing features demonstrated using phoneme recognition experiment The extracted features compared features extracted using backpropagation network',\n",
              " 'This paper reprinted Computational Learning Theory Natural Learning Systems vol T Petsche S Hanson J Shavlik eds Copyrighted MIT Press Abstract The ability inductive learning system find good solution given problem dependent upon representation used features problem A number factors including trainingset size ability learning algorithm perform constructive induction mediate effect input representation accuracy learned concept description We present experiments evaluate effect input representation generalization performance realworld problem finding genes DNA Our experiments demonstrate two different input representations task result significantly different generalization performance neural networks decision trees neural symbolic methods constructive induction fail bridge gap two representations We believe realworld domain provides interesting challenge problem machine learning subfield constructive induction relationship two representations well known conceptually representational shift involved constructing better representation imposing',\n",
              " 'In paper emphasize role selection evolutionary algorithms We briefly review common selection schemes fields Genetic Algorithms Evolution Strategies Genetic Programming However classify selection schemes according group evolutionary algorithm belong rather distinguish parent selection schemes global competition replacement schemes local competition replacement schemes This paper intend fully review analyse presented selection schemes tries short reference standard advanced selection schemes',\n",
              " 'Selfsupervised backpropagation unsupervised learning procedure feedforward networks desired output vector identical input vector For backpropagation able use powerful simulators running parallel machines Topologypreserving maps hand developed variant competitive learning procedure However degenerate case selfsupervised backpropagation version competitive learning A simple extension cost function backpropagation leads competitive version selfsupervised backpropagation used produce topographic maps We demonstrate approach applied Traveling Salesman Problem TSP The algorithm implemented using backpropagation simulator CLONES parallel machine RAP',\n",
              " 'This paper describes evolving computational model perception production simple rhythmic patterns The model consists network oscillators different resting frequencies couple input patterns Oscillators whose frequencies match periodicities input tend become activated Metrical structure represented explicitly network form clusters oscillators whose frequencies phase angles constrained maintain harmonic relationships characterize meter Rests rhythmic patterns represented explicit rest oscillators network become activated expected beat pattern fails appear The model makes predictions relative difficulty The nested periodicity defines musical probably also linguistic meter appears fundamental way people perceive produce patterns time Meter however sufficient describe patterns interesting memorable deviate metrical hierarchy The simplest deviations rests gaps one levels hierarchy would normally beat When beats removed regular intervals match period level metrical hierarchy call simple rhythmic pattern Figure shows example simple rhythmic pattern Below grid representation meter behind pattern patterns effect deviations periodicity input',\n",
              " 'In paper generalize several results uniform approximation orders radial basis functions Buhmann Dyn Levin Dyn Ron L p approximation orders These results apply particular approximants spaces spanned translates radial basis functions scattered centres Examples results apply include quasiinterpolation leastsquares approximation radial function spaces',\n",
              " 'The paper studies L IR norm approximations space spanned discrete set translates basis function Attention restricted functions whose Fourier transform smooth IR n singularity origin Examples basis functions thinplate splines multiquadrics well types radial basis functions employed Approximation Theory The approximation problem wellunderstood case set points ffi used translating forms lattice IR many optimal quasioptimal approximation schemes already found literature In contrast mostly specific results known set ffi scattered points The main objective paper provide general tool extending approximation schemes use integer translates basis function nonuniform case We introduce single relatively simple conversion method preserves approximation orders provided large number schemes presently literature precisely almost stationary schemes In anticipation future introduction new schemes uniform grids effort made impose mild conditions function still allow unified error analysis hold In course discussion recent results BuDL scattered center approximation reproduced improved upon',\n",
              " 'An upper bound L p approximation power p provided principal shiftinvariant spaces derived mild assumptions generator It applies stationary nonstationary ladders shown apply spaces generated exponential box splines polyharmonic splines multiquadrics Gauss kernel',\n",
              " 'In speeduplearning problems full descriptions operators always known explanationbased learning EBL reinforcement learning RL applied This paper shows methods involve fundamentally process propagating information backward goal toward starting state RL performs propagation statebystate basis EBL computes weakest preconditions operators hence performs propagation regionbyregion basis Based observation RL form asynchronous dynamic programming paper shows develop dynamic programming version EBL call ExplanationBased Reinforcement Learning EBRL The paper compares batch online versions EBRL batch online versions RL standard EBL The results show EBRL combines strengths EBL fast learning ability scale large state spaces strengths RL learning optimal policies Results shown chess endgames synthetic maze tasks',\n",
              " 'In paper present extensions kmeans algorithm vector quantization permit efficient use image segmentation pattern classification tasks It shown introducing state variables correspond certain statistics dynamic behavior algorithm possible find representative centers lower dimensional manifolds define boundaries classes clouds multidimensional multiclass data permits one example find class boundaries directly sparse data eg image segmentation tasks efficiently place centers pattern classification eg local Gaussian classifiers The state variables used define algorithms determining adaptively optimal number centers clouds data spacevarying density Some examples application extensions also given This report describes research done within CIMAT Guanajuato Mexico Center Biological Computational Learning Department Brain Cognitive Sciences Artificial Intelligence Laboratory This research sponsored grants Office Naval Research contracts NJ NJ grant National Science Foundation contract ASC grant National Institutes Health contract NIH SRR Additional support provided North Atlantic Treaty Organization ATR Audio Visual Perception Research Laboratories Mitsubishi Electric Corporation Sumitomo Metal Industries Siemens AG Support AI Laboratorys artificial intelligence research provided ONR contract NJ JL Marroquin supported part grant Consejo Nacional de Ciencia Tecnologia Mexico',\n",
              " 'The limitations using selforganizing maps SOM either clusteringvector quantization VQ multidimensional scaling MDS discussed reviewing recent empirical findings relevant theory SOMs remaining ability VQ MDS time challenged new combined technique online Kmeans clustering plus Sammon mapping cluster centroids SOM shown perform significantly worse terms quantization error recovering structure clusters preserving topology comprehensive empirical study using series multivariate normal clustering problems',\n",
              " 'While exploring find better solutions agent performing online reinforcement learning RL perform worse acceptable In cases exploration might unsafe even catastrophic results often modeled terms reaching failure states agents environment This paper presents method uses domain knowledge reduce number failures exploration This method formulates set actions RL agent composes control policy ensure exploration conducted policy space excludes unacceptable policies The resulting action set abstract relationship task solved common many applications RL Although cost added safety learning may result suboptimal solution argue appropriate tradeoff many problems We illustrate method domain motion planning',\n",
              " 'In learning problems connectionist network trained finite sized training set better generalization performance often obtained unneeded weights network eliminated One source unneeded weights comes inclusion input variables provide little information output variables We propose method identifying eliminating input variables The method first determines relationship input output variables using nonparametric density estimation measures relevance input variables using information theoretic concept mutual information We present results method simple toy problem nonlinear time series',\n",
              " 'In work applying genetic algorithms populations neural networks real distinction genotype phenotype In nature information contained genotype mapping genetic information phenotype usually much complex The genotypes many organisms exhibit diploidy ie include two copies gene two copies identical sequences therefore functional difference products usually proteins expressed phenotypic feature termed dominant one one recessive expressed In paper review literature use diploidy dominance operators genetic algorithms present new results obtained simulations changing environments finally discuss results simulations parallel biological findings',\n",
              " 'The Multiscalar architecture advocates distributed processor organization tasklevel speculation exploit high degrees instruction level parallelism ILP sequential programs without impeding improvements clock speeds The main goal paper understand key implications architectural features distributed processor organization tasklevel speculation compiler task selection point view performance We identify fundamental performance issues control ow speculation data communication data dependence speculation load imbalance task overhead We show issues intimately related key characteristics tasks task size intertask control ow intertask data dependence We describe compiler heuristics select tasks favorable characteristics We report experimental results show heuristics successful boosting overall performance establishing larger ILP windows',\n",
              " 'Technical Report January Abstract This paper introduces Introspection Approach method learning agent employing reinforcement learning decide ask training agent instruction When using approach find number trainers responses produced significantly faster learners learner ask aid randomly Guidance received via approach informative random guidance Thus reduce interaction training agent learning agent without reducing speed learner develops policy In fact intelligent learner asks help even increase learning speed level trainer interaction',\n",
              " 'We present method feature construction selection finds minimal set conjunctive features appropriate perform classification task For problems bias appropriate method outperforms constructive induction algorithms able achieve higher classification accuracy The application method search minimal multilevel boolean expressions presented analyzed help examples',\n",
              " 'In paper discuss Bayesian approach finding latent classes data In approach use finite mixture models describe underlying structure data demonstrate possibility use full joint probability models raises interesting new prospects exploratory data analysis The concepts methods discussed illustrated case study using data set recent educational study The Bayesian classification approach described implemented presents appealing addition standard toolbox exploratory data analysis educational data',\n",
              " 'We present two additions hierarchical mixture experts HME architecture We view HME tree structured classifier Firstly applying likelihood splitting criteria expert HME grow tree adaptively training Secondly considering probable path tree may prune branches away either temporarily permanently become redundant We demonstrate results growing pruning algorithms show significant speed ups efficient use parameters conventional algorithms discriminating two interlocking spirals classifying bit parity patterns',\n",
              " 'Systems interacting realworld data must address issues raised possible presence errors observations makes In paper first present framework discussing imperfect data resulting problems may cause We distinguish two categories errors data random errors noise systematic errors examine relationship task describing observations way also useful helping future problemsolving learning tasks Secondly proceed examine techniques currently used AI research recognising errors',\n",
              " 'Genetic Programming applied task evolving general iterative sorting algorithms A connection size generality discovered Adding inverse size fitness measure along correctness decreases size resulting evolved algorithms also dramatically increases generality thus effectiveness evolution process In addition variety differing problem formulations investigated relative probability success reported An example evolved sort problem formulation presented initial attempt made understand variations difficulty resulting differing problem formulations',\n",
              " 'Irrelevant redundant features may reduce predictive accuracy comprehensibility induced concepts Most common Machine Learning approaches selecting good subset relevant features rely crossvalidation As alternative present application particular Minimum Description Length MDL measure task feature subset selection Using MDL principle allows taking account available data The new measure informationtheoretically plausible yet still simple therefore efficiently computable We show empirically new method judging value feature subsets efficient performs least well methods based crossvalidation Domains large number training examples large number possible features yield biggest gains efficiency Thus new approach seems scale better large learning problems previous methods',\n",
              " 'rules Rivest Inductive algorithms AQ CN learn decision lists incrementally one rule time Such algorithms face rule overlap problem classification accuracy decision list depends overlap learned rules Thus even though rules learned isolation evaluated concert Existing algorithms solve problem adopting greedy iterative structure Once rule learned training examples match rule removed training set We propose novel solution problem composing decision lists homogeneous rules rules whose classification accuracy change position decision list We prove problem finding maximally accurate decision list reduced problem finding maximally accurate homogeneous rules We report performance algorithm data sets UCI repository MONKs problems',\n",
              " 'Methods build function approximators example data gained considerable interest past Especially methodologies build models allow interpretation attracted attention Most existing algorithms however either complicated use infeasible highdimensional problems This article presents efficient easy use algorithm construct fuzzy graphs example data The resulting fuzzy graphs based locally independent fuzzy rules operate solely selected important attributes This enables application fuzzy graphs also problems high dimensional spaces Using illustrative examples real world data set demonstrated resulting fuzzy graphs offer quick insights structure example data underlying model',\n",
              " 'We describe methodology enabling intelligent teaching system make high level strategy decisions basis low level student modeling information This framework less costly construct superior hand coding teaching strategies responsive learners needs In order accomplish reinforcement learning used learn associate superior teaching actions certain states students knowledge Reinforcement learning RL shown flexible handling noisy data need expert domain knowledge A drawback RL often needs significant number trials learning We propose offline learning methodology using sample data simulated students small amounts expert knowledge bypass problem',\n",
              " 'Any intelligent system whether human robotic must capable dealing patterns time Temporal pattern processing achieved system shortterm memory capacity STM different representations maintained time In work propose neural model wherein STM realized leaky integrators selforganizing system The model exhibits compositionality ability extract construct progressively complex structured associations hierarchical manner starting basic primitive temporal elements An important feature proposed model use temporal correlations express dynamic bindings',\n",
              " 'In tasks requiring sustained attention human alertness varies minute time scale This serious consequences occupations ranging air traffic control monitoring nuclear power plants Changes electroencephalographic EEG power spectrum accompany fluctuations level alertness assessed measuring simultaneous changes EEG performance auditory monitoring task By combining power spectrum estimation principal component analysis artificial neural networks show continuous accurate noninvasive near realtime estimation operators global level alertness feasible using EEG measures recorded two central scalp sites This demonstration could lead practical system noninvasive monitoring cognitive state human operators attentioncritical settings',\n",
              " 'This paper presents exact solutions convergent approximations inferences Bayesian networks associated finitely generated convex sets distributions Robust Bayesian inference calculation bounds posterior values given perturbations probabilistic model The paper presents exact inference algorithms analyzes circumstances exact inference becomes intractable Two classes algorithms numeric approximations developed transformations original model The first transformation reduces robust inference problem estimation probabilistic parameters Bayesian network The second transformation uses Lavines bracketing algorithm generate sequence maximization problems Bayesian network The analysis extended contaminated lower density bounded belief function subsigma density bounded total variation density ratio classes distributions c fl Carnegie Mellon University',\n",
              " 'One fundamental problems learning identifying members two different classes For example diagnose cancer one must learn discriminate benign malignant tumors Through examination tumors previously determined diagnosis one learns function distinguishing benign malignant tumors Then acquired knowledge used diagnose new tumors The perceptron simple biologically inspired model twoclass learning problem The perceptron trained constructed using examples two classes Then perceptron used classify new examples We describe geometrically perceptron capable learning Using duality develop framework investigating different methods training perceptron Depending define best perceptron different minimization problems developed training perceptron The effectiveness methods evaluated empirically four practical applications breast cancer diagnosis detection heart disease political voting habits sonar recognition This paper assume prior knowledge machine learning pattern recognition',\n",
              " 'I define latent variable model form neural network target outputs specified inputs unspecified Although inputs missing still possible train model placing simple probability distribution unknown inputs maximizing probability data given parameters The model discover description data terms underlying latent variable space lower dimensionality I present preliminary results application models protein data',\n",
              " 'This paper studies aspects two categories usually differ like relevance generalization role loss function presents unifying formalism types information identified answers generalized questions shows kind generalized information necessary enable learning aims put usual training data prior information equal footing discussing possibilities variants measurement control generalized questions including examples smoothness symmetries reviews shortly measurement linguistic concepts based fuzzy priors principles combine preprocessors uses Bayesian decision theoretic framework contrasting parallel inverse decision problems proposes problems nonapproximation aspects Bayesian two step approximation consisting posterior maximization subsequent risk minimization analyses empirical risk minimization aspect nonlocal information compares Bayesian two step approximation empirical risk minimization including interpretations Occams razor formulates examples stationarity conditions maximum posterior approximation nonlocal nonconvex priors leading inhomogeneous nonlinear equations similar example equations scattering theory physics In summary paper focuses dependencies answers different questions Because training examples alone dependencies enable generalization emphasizes need empirical measurement control explicit treatment theory This report describes research done within Center Biological Computational Learning Department Brain Cognitive Sciences Massachusetts Institute Technology This research sponsored grant National Science Foundation contract ASC grant ONRARPA contract NJ The author supported Postdoctoral Fellowship Le Deutsche Forschungsgemeinschaft NSFCISE Postdoctoral Fellowship',\n",
              " 'We present alternative cellular encoding technique Gruau evolving graph network structures via genetic programming The new technique called edge encoding uses edge operators rather node operators cellular encoding While cellular encoding edge encoding produce possible graphs two encodings bias genetic search process different ways may therefore useful different set problems The problems techniques may used think edge encoding may particularly useful include evolution recurrent neural networks finite automata graphbased queries symbolic knowledge bases In preliminary report present technical description edge encoding initial comparison cellular encoding Experimental investigation relative merits encoding schemes currently progress',\n",
              " 'We present technique evaluating classifications geometric comparison rule sets Rules represented objects ndimensional hyperspace The similarity classes computed overlap geometric class descriptions The system produces correlation matrix indicates degree similarity pair classes The technique applied classifications generated different algorithms different numbers classes different attribute sets Experimental results case study medical domain included',\n",
              " 'As natural resources become less abundant naturally become interested adept utilisation waste materials In bringing bear ploy key importance learning I argue paper In Truth Trash model learning viewed process uses environmental feedback assemble fortuitous sensory predispositions sensory trash useful information vehicles ie truthful indicators salient phenomena The main aim show computer implementation model used enhance learning strategic abilities simulated football playing mobot',\n",
              " 'This paper concerns probabilistic evaluation effects actions presence unmeasured variables We show identification causal effect singleton variable X set variables Y accomplished systematically time polynomial number variables graph When causal effect identifiable closedform expression obtained probability action achieve specified goal set goals',\n",
              " 'VISOR large connectionist system shows visual schemas learned represented used mechanisms natural neural networks Processing VISOR based cooperation competition parallel bottomup topdown activation schema representations Simulations show VISOR robust noise variations inputs parameters It indicate confidence analysis pay attention important minor differences use context recognize ambiguous objects Experiments also suggest representation learning stable behavior consistent human processes priming perceptual reversal circular reaction learning The schema mechanisms VISOR serve starting point building robust highlevel vision systems perhaps schemabased motor control natural language processing systems well',\n",
              " 'We present framework characterizing Bayesian classification methods This framework thought spectrum allowable dependence given probabilistic model Naive Bayes algorithm restrictive end learning full Bayesian networks general extreme While much work carried along two ends spectrum surprising little done along middle We analyze assumptions made one moves along spectrum show tradeoffs model accuracy learning speed become critical consider variety data mining domains We present general induction algorithm allows traversal spectrum depending available computational power carrying induction show application number domains different properties',\n",
              " 'Traits acquired members evolving population lifetime adaptive processes learning become genetically specified later generations Thus change level learning population evolutionary time This paper explores idea well benefits gained learning may also costs paid ability learn It costs supply selection pressure genetic assimilation acquired traits Two models presented attempt illustrate assertion The first uses Kauffmans NK fitness landscapes show effect explicit implicit costs assimilation learnt traits A characteristic hump observed graph level plasticity population showing learning first selected evolution progresses The second model practical example neural network controllers evolved small mobile robot Results experiment also show hump',\n",
              " 'The evolution population guided phenotypic traits acquired members population lifetime This phenomenon known Baldwin Effect speed evolutionary process traits initially acquired become genetically specified later generations This paper presents conditions genetic assimilation take place As well benefits lifetime adaptation give population may cost paid adaptive ability It evolutionary tradeoff costs benefits provides selection pressure acquired traits become genetically specified It also noted genotypic space evolution operates phenotypic space adaptive processes learning operate general different nature To guarantee acquired characteristic become genetically specified spaces must property neighbourhood correlation means small distance two individuals phenotypic space implies small distance two individuals genotypic space',\n",
              " 'Decision Trees widely used classificationregression tasks They relatively much faster build compared Neural Networks understandable humans In normal decision trees based input vector one branch followed In Probabilistic OPtion trees based input vector follow subtrees probability These probabilities learned system Probabilistic decisions likely useful boundary classes submerge noise input data In addition provide us confidence measure We allow option nodes trees Again instead uniform voting learn weightage every subtree',\n",
              " 'The fundamental backpropagation BP algorithm training artificial neural networks cast deterministic nonmonotone perturbed gradient method Under certain natural assumptions series learning rates diverging series squares converging established every accumulation point online BP iterates stationary point BP error function The results presented cover serial parallel online BP modified BP momentum term BP weight decay',\n",
              " 'Recurrent neural networks trained behave like deterministic finitestate automata DFAs show deteriorating performance tested long strings This deteriorating performance attributed instability internal representation learned DFA states The use sigmoidal discriminant function together recurrent structure contribute instability We prove simple algorithm construct secondorder recurrent neural networks sparse interconnection topology sigmoidal discriminant function internal DFA state representations stable ie constructed network correctly classifies strings arbitrary length The algorithm based encoding strengths weights directly neural network We derive relationship weight strength number DFA states robust string classification For DFA n states input alphabet symbols constructive algorithm generates programmed neural network On neurons Omn weights We compare algorithm methods proposed literature',\n",
              " 'Report SYCON Recent Results Lyapunovtheoretic Techniques Nonlinear Stability ABSTRACT This paper presents Converse Lyapunov Function Theorem motivated robust control analysis design Our result based upon generalizes various aspects wellknown classical theorems In unified natural manner includes arbitrary bounded disturbances acting system deals global asymptotic stability results smooth infinitely differentiable Lyapunov functions applies stability respect necessarily compact invariant sets As corollary obtained Converse Theorem show wellknown Lyapunov sufficient condition inputtostate stability also necessary settling positively open question raised several authors past years',\n",
              " 'Technical Report CSTR UMIACSTR University Maryland College Park MD Abstract The extraction symbolic knowledge trained neural networks direct encoding partial knowledge networks prior training important issues They allow exchange information symbolic connectionist knowledge representations The focus paper quality rules extracted recurrent neural networks Discretetime recurrent neural networks trained correctly classify strings regular language Rules defining learned grammar extracted networks form deterministic finitestate automata DFAs applying clustering algorithms output space recurrent state neurons Our algorithm extract different finitestate automata consistent training set network We compare generalization performances different models trained network introduce heuristic permits us choose among consistent DFAs model best approximates learned regular grammar',\n",
              " 'Jobshop scheduling important task manufacturing industries We interested particular task scheduling payload processing NASAs space shuttle program This paper summarizes previous work formulating task solution reinforcement learning algorithm T D A shortcoming previous work reliance handengineered input features This paper shows extend timedelay neural network TDNN architecture apply irregularlength schedules Experimental tests show TDNNT D network match performance previous handengineered system The tests also show neural network approaches significantly outperform best previous nonlearning solution problem terms quality resulting schedules number search steps required construct',\n",
              " 'Report SYCON ABSTRACT This paper deals simulation Turing machines neural networks Such networks made interconnections synchronously evolving processors updates state according sigmoidal linear combination previous states units The main result states one may simulate Turing machines nets linear time In particular possible give net made processors computes universal partialrecursive function This update Report SYCON new results include simulation linear time binarytape machines opposed unary alphabets used previous version',\n",
              " 'In paper develop new LRTAbased algorithms variety tasks analyze complexity The LRTA algorithm realtime search algorithm developed Korf It used reach stationary moving goal state identify shortest paths given start state stationary goal state algorithm reset start state reaches goal state Our algorithms search horizon one require internal memory must able store information states For example bidirectional LRTS algorithm determines optimal universal plans ie finds optimal paths states set stationary goal states even reset actions available We show tasks studied paper solved LRTAbased algorithms On action executions state spaces size n',\n",
              " 'In paper consider problem approximating function belonging function space linear combination n translates given function G Using lemma Jones Barron show possible define function spaces functions G rate convergence zero error O p n number dimensions The apparent avoidance curse dimensionality due fact function spaces constrained dimension increases Examples include spaces Sobolev type number weak derivatives required larger number dimensions We give results approximation L norm L norm The interesting feature results thanks constructive nature Jones Barrons lemma iterative procedure defined achieve rate This paper describes research done within Center Biological Information Processing Department Brain Cognitive Sciences Artificial Intelligence Laboratory Department Mathematics University Trento Italy Gabriele Anzellotti Department Mathematics University Trento Italy This research sponsored grant Office Naval Research ONR Cognitive Neural Sciences Division Artificial Intelligence Center Hughes Aircraft Corporation S Support A I Laboratorys artificial intelligence research provided Advanced Research Projects Agency Department Defense Army contract DACAC part ONR contract NK c fl Massachusetts Institute Technology',\n",
              " 'University Wisconsin Computer Sciences Technical Report September Abstract In explanationbased learning specific problems solution generalized form later used solve conceptually similar problems Most research explanationbased learning involves relaxing constraints variables explanation specific example rather generalizing graphical structure explanation However precludes acquisition concepts iterative recursive process implicitly represented explanation fixed number applications This paper presents algorithm generalizes explanation structures reports empirical results demonstrate value acquiring recursive iterative concepts The BAGGER algorithm learns recursive iterative concepts integrates results multiple examples extracts useful subconcepts generalization On problems learning recursive rule appropriate system produces result standard explanationbased methods Applying learned recursive rules requires minor extension PROLOGlike problem solver namely ability explicitly call specific rule Empirical studies demonstrate generalizing structure explanations helps avoid recently reported negative effects learning',\n",
              " 'We consider Gibbs sampler applied uniform distribution bounded region R R We show convergence properties Gibbs sampler depend greatly smoothness boundary R Indeed sufficiently smooth boundaries sampler uniformly ergodic jagged boundaries sampler could fail even geometrically ergodic',\n",
              " 'In learning examples often useful expand attributevector representation intermediate concepts The usual advantage structuring learning problem makes learning easier improves comprehensibility induced descriptions In paper develop technique discovering useful intermediate concepts class attributes realvalued The technique based decomposition method originally developed design switching circuits recently extended handle incompletely specified multivalued functions It also applied machine learning tasks In paper introduce modifications needed decompose real functions present symbolic form The method evaluated number test functions The results show method correctly decomposes fairly complex functions The decomposition hierarchy depend given repertoir basic functions background knowledge',\n",
              " 'Uncertainty sampling methods iteratively request class labels training instances whose classes uncertain despite previous labeled instances These methods greatly reduce number instances expert need label One problem approach classifier best suited application may expensive train use selection instances We test use one classifier highly efficient probabilistic one select examples training another C rule induction program Despite chosen heterogeneous approach uncertainty samples yielded classifiers lower error rates random samples ten times larger',\n",
              " 'Certain causal models involving unmeasured variables induce independence constraints among observed variables imply nevertheless inequality constraints observed distribution This paper derives general formula inequality constraints induced instrumental variables exogenous variables directly affect variables With help formula possible test whether model involving instrumental variables may account data conversely whether given vari able deemed instrumental',\n",
              " 'Bayesian confidence intervals smoothing spline often used distinguish two curves In paper provide asymptotic formula sample size calculations based Bayesian confidence intervals Approximations simulations special functions indicate asymptotic formula reasonably accurate Key Words Bayesian confidence intervals sample size smoothing spline fl Address Department Statistics Applied Probability University California Santa Barbara CA Tel Fax Email yuedongpstatucsbedu Supported National Institute Health Grants R EY P DK P HD',\n",
              " 'We describe several improvements Freund Schapires AdaBoost boosting algorithm particularly setting hypotheses may assign confidences predictions We give simplified analysis AdaBoost setting show analysis used find improved parameter settings well refined criterion training weak hypotheses We give specific method assigning confidences predictions decision trees method closely related one used Quinlan This method also suggests technique growing decision trees turns identical one proposed Kearns Mansour We focus next apply new boosting algorithms multiclass classification problems particularly multilabel case example may belong one class We give two boosting methods problem One leads new method handling singlelabel case simpler effective techniques suggested Freund Schapire Finally give experimental results comparing algorithms discussed paper',\n",
              " 'Evolutionary Algorithms direct random search algorithms imitate principles natural evolution method solve adaptation learning tasks general As several features common observed genetic phenotypic level living species In paper algorithms capability adaptation learning wider sense demonstrated focused Genetic Algorithms illustrate learning process population level first level learning Evolution Strategies demonstrate learning process metalevel strategy parameters second level learning',\n",
              " 'We examine novel addition known methods learning Bayesian networks data improves quality learned networks Our approach explicitly represents learns local structure conditional probability distributions CPDs quantify networks This increases space possible models enabling representation CPDs variable number parameters The resulting learning procedure induces models better emulate interactions present data We describe theoretical foundations practical aspects learning local structures provide empirical evaluation proposed learning procedure This evaluation indicates learning curves characterizing procedure converge faster number training instances standard procedure ignores local structure CPDs Our results also show networks learned local structures tend complex terms arcs yet require fewer parameters',\n",
              " 'This paper contains method bound test errors voting committees members chosen pool trained classifiers There many prospective committees validating directly achieve useful error bounds Because fewer classifiers prospective committees better validate classifiers individually use linear programming infer committee error bounds We test method using credit card data Also extend method infer bounds classifiers general',\n",
              " 'An accurate simulation heating coil used compare performance PI controller neural network trained predict steadystate output PI controller neural network trained minimize nstep ahead error coil output set point reinforcement learning agent trained minimize sum squared error time Although PI controller works well task neural networks result improved performance',\n",
              " 'The CN algorithm induces ordered list classification rules examples using entropy search heuristic In short paper describe two improvements algorithm Firstly present use Laplacian error estimate alternative evaluation function secondly show unordered well ordered rules generated We experimentally demonstrate significantly improved performances resulting changes thus enhancing usefulness CN inductive tool Comparisons Quinlans C also made',\n",
              " 'Neural computation also called connectionism parallel distributed processing neural network modeling brainstyle computation grown rapidly last decade Despite explosion ultimately impressive applications dire need concise introduction theoretical perspective analyzing strengths weaknesses connectionist approaches establishing links disciplines statistics control theory The Introduction Theory Neural Computation Hertz Krogh Palmer subsequently referred HKP written perspective physics home discipline authors The book fulfills mission introduction neural network novices provided background calculus linear algebra statistics It covers number models often viewed disjoint Critical analyses fruitful comparisons models',\n",
              " 'Controlflow misprediction penalties major impediment high performance wideissue superscalar processors In paper present Selective Eager Execution SEE execution model overcome misspeculation penalties executing paths diffident branches We present microarchitecture PolyPath processor extension aggressive superscalar outoforder architecture The PolyPath architecture uses novel instruction tagging register renaming mechanism execute instructions multiple paths simultaneously processor pipeline retaining maximum resource availability singlepath code sequences Results executiondriven pipelinelevel simulations show SEE improve performance much go benchmark average SPECint compared normal superscalar outoforder speculative execution monopath processor Moreover architectural model elegant practical implement using small amount additional state control logic',\n",
              " 'This paper describes competitive tree learning algorithm derived first principles The algorithm approximates Bayesian decision theoretic solution learning task Comparative experiments algorithm several mature AI statistical families tree learning algorithms currently use show derived Bayesian algorithm consistently good better although sometimes computational cost Using strategy design algorithms many supervised model learning tasks given probabilistic representation kind knowledge learned As illustration second learning algorithm derived learning Bayesian networks data Implications incremental learning use multiple models also discussed',\n",
              " 'We address problem finding subset features allows supervised induction algorithm induce small highaccuracy concepts We examine notions relevance irrelevance show definitions used machine learning literature adequately partition features useful categories relevance We present definitions irrelevance two degrees relevance These definitions improve understanding behavior previous subset selection algorithms help define subset features sought The features selected depend features target concept also induction algorithm We describe method feature subset selection using crossvalidation applicable induction algorithm discuss experiments conducted ID C artificial real datasets',\n",
              " 'We describe machine learning method predicting value realvalued function given values multiple input variables The method induces solutions samples form ordered disjunctive normal form DNF decision rules A central objective method representation induction compact easily interpretable solutions This rulebased decision model extended search efficiently similar cases prior approximating function values Experimental results realworld data demonstrate new techniques competitive existing machine learning statistical methods sometimes yield superior regression performance',\n",
              " 'This work presents hybrid branch predictor scheme uses limited form dual path execution along dynamic branch prediction improve execution times The ability execute paths conditional branch enables branch penalty minimized however relying exclusively dual path execution infeasible due instruction fetch rates far exceed capability pipeline retire single branch others must processed By using confidence information available dynamic branch prediction state tables limited form dual path execution becomes feasible This reduces burden branch predictor allowing predictions low confidence avoided In study present new approach gather branch prediction confidence little overhead use confidence mechanism determine whether dual path execution branch prediction used Comparing hybrid predictor model dynamic branch predictor shows dramatic decrease misprediction rate translates reduction runtime These results imply dual path execution often thought excessively resource consuming method may worthy approach restricted appropriate predicting set',\n",
              " 'This paper presents Threaded MultiPath Execution TME exploits existing hardware Simultaneous Multithreading SMT processor speculatively execute multiple paths execution When fewer threads SMT processor hardware contexts threaded multipath execution uses spare contexts fetch execute code along less likely path hardtopredict branches This paper describes hardware mechanisms needed enable SMT processor efficiently spawn speculative threads threaded multipath execution The Mapping Synchronization Bus described enables spawning multiple paths Policies examined deciding branches fork managing competition primary alternate path threads critical resources Our results show TME increases single program performance SMT eight thread contexts average depending misprediction penalty programs high misprediction rate',\n",
              " 'In paper review research machine learning relation computational models human learning We focus initially concept induction examining five main approaches problem consider complex issue learning sequential behaviors After compare rhetoric sometimes appears machine learning psychological literature growing evidence different theoretical paradigms typically produce similar results In response suggest concrete computational models currently dominate field may less useful simulations operate abstract level We illustrate point abstract simulation explains challenging phenomenon area category learning conclude general observations abstract models',\n",
              " 'The function unknown biological sequence often accurately inferred identifying sequences homologous original sequence Given query set known homologs exist least three general classes techniques finding additional homologs pairwise sequence comparisons motif analysis hidden Markov modeling Pair wise sequence comparisons typically employed single query sequence known Hidden Markov models HMMs hand usually trained sets sequences Motif based methods fall two extremes',\n",
              " 'Future research directions Knowledge Discovery Databases KDD include ability extract overlying concept relating useful data Current limitations involve search complexity find concept means useful The Pattern Theory research crosses natural way aforementioned domain The goal paper threefold First present new approach problem learning Discovery robust pattern finding Second explore current limitations Pattern Theoretic approach applied general KDD problem Third exhibit performance experimental results binary functions compare results C This new approach learning demonstrates powerful method finding patterns robust manner',\n",
              " 'Prerequisites An understanding dynamic programming edit distance approach pairwise sequence alignment useful parts Also familiarity use Internet resources would helpful part For former see Chapters latter see Chapter Hypertext Book GNAVSNS Biocomputing Course httpwwwtechfakunibielefelddebcdCurricwelcomehtml General Rationale You understand Multiple Alignment considered challenging problem study approaches try reduce number steps needed calculate optimal solution study fast heuristics In case study involving immunoglobulin sequences study multiple alignments obtained WWW servers recapitulating results original paper Revision History Version Sep Expanded Ex Updated Ex Revised Solution Sheet Ex Marked Exercises A submitted Instructor Various minor clarifications content',\n",
              " 'This article describes new system induction oblique decision trees This system OC combines deterministic hillclimbing two forms randomization find good oblique split form hyperplane node decision tree Oblique decision tree methods tuned especially domains attributes numeric although adapted symbolic mixed symbolicnumeric attributes We present extensive empirical studies using real artificial data analyze OCs ability construct oblique trees smaller accurate axisparallel counterparts We also examine benefits randomization construction oblique decision trees',\n",
              " 'ExplanationBased Reinforcement Learning EBRL introduced Dietterich Flann way combining ability Reinforcement Learning RL learn optimal plans generalization ability ExplanationBased Learning EBL Dietterich Flann We extend work domains agent must order achieve sequence subgoals optimal fashion Hierarchical EBRL effectively learn optimal policies sequential task domains even subgoals weakly interact We also show planner achieve individual subgoals available method converges even faster',\n",
              " 'Discretization continuously valued data useful necessary tool many learning paradigms assume nominal data A list objectives efficient effective discretization presented A paradigm called BRACE Boundary Ranking And Classification Evaluation attempts meet objectives presented along algorithm follows paradigm The paradigm meets many objectives potential extension meet remainder Empirical results promising For reasons BRACE potential effective efficient method discretization continuously valued data A advantage BRACE general enough extended types clusteringunsupervised learning',\n",
              " 'Naive Bayesian classifiers make independence assumptions perform remarkably well data sets poorly others We explore ways improve Bayesian classifier searching dependencies among attributes We propose evaluate two algorithms detecting dependencies among attributes show backward sequential elimination joining algorithm provides improvement naive Bayesian classifier The domains improvement occurs domains naive Bayesian classifier significantly less accurate decision tree learner This suggests attributes used common databases independent conditioned class violations independence assumption affect accuracy classifier The Bayesian classifier Duda Hart probabilistic method classification It used determine probability example j belongs class C given values attributes example represented set n nominallyvalued attributevalue pairs form A V j P A k V k j jC may estimated training data To determine likely class test example probability class computed Equation A classifier created manner sometimes called simple Langley naive Kononenko Bayesian classifier One important evaluation metric machine learning methods predictive accuracy unseen examples This measured randomly selecting subset examples database use training examples reserving remainder used test examples In case simple Bayesian classifier training examples used estimate probabilities Equation used detected training data',\n",
              " 'Multiple sequence alignment distantly related viral proteins remains challenge currently available alignment methods The hidden Markov model approach offers new flexible method generation multiple sequence alignments The results studies attempting infer appropriate parameter constraints generation de novo HMMs globin kinase aspartic acid protease ribonuclease H sequences SAM HMMER methods described',\n",
              " 'Several recurrent networks proposed representations task formal language learning After training recurrent network next step understand information processing carried network Some researchers Giles et al Watrous Kuhn Cleeremans et al resorted extracting finite state machines internal state trajectories recurrent networks This paper describes two conditions sensitivity initial conditions frivolous computational explanations due discrete measurements Kolen Pollack allow extraction methods return illusionary finite state descriptions',\n",
              " 'In order useful learning algorithm must able generalize well faced inputs previously presented system A bias necessary generalization shown several researchers recent years bias lead strictly better generalization summed possible functions applications This paper provides examples illustrate fact also explains bias learning algorithm better another practice probability occurrence functions taken account It shows domain knowledge understanding conditions learning algorithm performs well used increase probability accurate generalization identifies several conditions considered attempting select appropriate bias particular problem',\n",
              " 'In paper introduce modelbased reinforcement learning method called Hlearning optimizes undiscounted average reward We compare three reinforcement learning methods domain scheduling Automatic Guided Vehicles transportation robots used modern manufacturing plants facilities The four methods differ along two dimensions They either modelbased modelfree optimize discounted total reward undiscounted average reward Our experimental results indicate Hlearning robust respect changes domain parameters many cases converges fewer steps better average reward per time step methods An added advantage unlike methods parameters tune',\n",
              " 'This paper presents Converse Lyapunov Function Theorem motivated robust control analysis design Our result based upon generalizes various aspects wellknown classical theorems In unified natural manner allows arbitrary bounded timevarying parameters system description deals global asymptotic stability results smooth infinitely differentiable Lyapunov functions applies stability respect necessarily compact invariant sets Introduction This work motivated problems robust nonlinear stabilization One main',\n",
              " 'Technical Report AI May Abstract An important often neglected problem field Artificial Intelligence grounding systems environment representations manipulate inherent meaning system Since humans rely heavily semantics seems likely grounding crucial development truly intelligent behavior This study investigates use simulated robotic agents neural network processors part method ensure grounding Both topology weights neural networks optimized genetic algorithms Although comprehensive optimization difficult empirical evidence gathered shows method tractable quite fruitful In experiments agents evolved wallfollowing control strategy able transfer novel environments Their behavior suggests also learning build cognitive maps',\n",
              " 'ExplanationBased Learning Mitchell et al DeJong Mooney shown promise powerful analytical learning technique However EBL severely hampered requirement complete correct domain theory successful learning occur Clearly nontrivial domains developing domain theory nearly impossible task Therefore much research devoted understanding imperfect domain theory corrected extended system performance In paper present characterization problem use analyze past research area Past characterizations problem eg Mitchell et al Rajamoney DeJong viewed types performance errors caused faulty domain theory primary In contrast focus primarily types knowledge deficiencies present theory derive types performance errors result Correcting theory viewed search space possible domain theories variety knowledge sources used guide search We examine types knowledge used variety past systems purpose The hope analysis indicate need universal weak method domain theory correction different sources knowledge theory correction freely flexibly combined',\n",
              " 'We study task tnding maximal posteriori MAP instantiation Bayesian network variables given partial value assignment initial constraint This problem known NPhard concentrate stochastic approximation algorithm simulated annealing This stochastic algorithm realized sequential process set Bayesian network variables one variable allowed change time Consequently method become impractically slow number variables increases We present method mapping given Bayesian network massively parallel Bolztmann machine neural network architecture sense instead using normal sequential simulated annealing algorithm use massively parallel stochastic process Boltzmann machine architecture The neural network updating process provably converges state solves given MAP task',\n",
              " 'Parameterized heuristics offers elegant powerful theoretical framework design analysis autonomous adaptive communication networks Routing messages networks presents realtime instance multicriterion optimization problem dynamic uncertain environment This paper describes framework heuristic routing large networks The effectiveness heuristic routing mechanism upon Quo Vadis based described part simulation study within network grid topology A formal analysis underlying principles presented incremental design set heuristic decision functions used guide messages along nearoptimal eg minimum delay path large network This paper carefully derives properties heuristics set simplifying assumptions network topology load dynamics identify conditions guaranteed route messages along optimal path The paper concludes discussion relevance theoretical results presented paper design intelligent autonomous adaptive communication networks outline directions future research',\n",
              " 'Technical Report Department Statistics University Washington Derek Stanford Graduate Research Assistant Adrian E Raftery Professor Statistics Sociology Department Statistics University Washington Box Seattle WA USA Email stanfordstatwashingtonedu rafterystatwashingtonedu Web httpwwwstatwashingtoneduraftery This research supported ONR grants N N The authors grateful Simon Byers Gilles Celeux Christian Posse helpful discussions',\n",
              " 'We analyze algorithms predict binary value combining predictions several prediction strategies called experts Our analysis worstcase situations ie make assumptions way sequence bits predicted generated We measure performance algorithm difference expected number mistakes makes bit sequence expected number mistakes made best expert sequence expectation taken respect randomization predictions We show minimum achievable difference order square root number mistakes best expert give efficient algorithms achieve Our upper lower bounds matching leading constants cases We show leads certain kinds pattern recognitionlearning algorithms performance bounds improve best results currently known context We also compare analysis case log loss used instead expected number mistakes',\n",
              " 'This paper presents formalization novel approach structural similarity assessment adaptation casebased reasoning Cbr synthesis The approach informally presented exemplified implemented domain industrial building design Borner By relating approach existing theories provide foundation systematic evaluation appropriate usage Cases primary repository knowledge represented structurally using algebraic approach Similarity relations provide structure preserving case modifications modulo underlying algebra equational theory algebra available This representation modeled universe discourse enables theorybased inference adapted solutions The approach enables us incorporate formally generalization abstraction geometrical transformation combinations Cbr',\n",
              " 'A learning agent employing reinforcement learning hindered receives critics sparse weakly informative training information We present approach automated training agent may also provide occasional instruction learner form actions learner perform The learner access critics feedback trainers instruction In experiments vary level trainers interaction learner allowing trainer instruct learner almost every time step allowing trainer respond We also vary parameter controls learner incorporates trainers actions The results show significant reductions average number training trials necessary learn perform task',\n",
              " 'We present algorithm improving accuracy algorithms learning binary concepts The improvement achieved combining large number hypotheses generated training given learning algorithm different set examples Our algorithm based ideas presented Schapire paper The strength weak learnability represents improvement results The analysis algorithm provides general upper bounds resources required learning Valiants polynomial PAC learning framework best general upper bounds known today We show number hypotheses combined algorithm smallest number possible Other outcomes analysis results regarding representational power threshold circuits relation learnability compression method parallelizing PAC learning algorithms We provide extensions algorithms cases concepts binary case accuracy learning algorithm depends distribution instances',\n",
              " 'This paper proposes model ratio decidendi justification structure consisting series reasoning steps relate abstract predicates abstract predicates relate abstract predicates specific facts This model satisfies important set characteristics ratio decidendi identified jurisprudential literature In particular model shows theory case decided controls precedential effect By contrast purely exemplarbased model ratio decidendi fails account dependency precedential effect theory decision',\n",
              " 'In paper abstract computational principles underlying topographic maps discussed We give definition perfectly neighbourhood preserving map call topographic homeomorphism prove certain desirable properties It argued topographic homeomorphism exist usual case many equally valid choices available quantifying quality map We introduce particular measure encompasses several previous proposals discuss relation work This formulation problem sets within wellknown class quadratic assignment problems',\n",
              " 'This paper studies robustness pac learning algorithms instance space f g n examples corrupted purely random noise affecting instances labels In past conflicting results subject obtainedthe best agreement rule tolerate small amounts noise yet cases large amounts noise tolerated We show truth lies somewhere two alternatives For uniform attribute noise attribute flipped independently random probability present algorithm pac learns monomials unknown noise rate less Contrasting positive result show product random attribute noise attribute flipped randomly independently probability p nearly harmful malicious noiseno algorithm tolerate small amount noise fl Supported part GE Foundation Junior Faculty Grant NSF grant CCR Part research conducted author MIT Laboratory Computer Science supported NSF grant DCR grant Siemens Corporation Net address sgcswustledu',\n",
              " 'This paper describes research investigating behavioral specialization learning robot teams Each agent provided common set skills motor schemabased behavioral assemblages builds taskachieving strategy using reinforcement learning The agents learn individually activate particular behavioral assemblages given current situation reward signal The experiments conducted robot soccer simulations evaluate agents terms performance policy convergence behavioral diversity The results show many cases robots automatically diversify choosing heterogeneous behaviors The degree diversification performance team depend reward structure When entire team jointly rewarded penalized global reinforcement teams tend towards heterogeneous behavior When agents provided feedback individually local reinforcement converge identical policies',\n",
              " 'Product units provide method automatically learning higherorder input combinations required efficient synthesis Boolean logic functions neural networks Product units also higher information capacity sigmoidal networks However activation function received much attention literature A possible reason one encounters problems using standard backpropagation train networks containing units This report examines problems evaluates performance three training algorithms networks type Empirical results indicate error surface networks containing product units local minima corresponding networks summation units For reason combination local global training algorithms found provide reliable convergence We investigate hints added training algorithm By extracting common frequency input weights training frequency separately show convergence accelerated In order compare performance transfer functions product units implemented candidate units Cascade Correlation CC system Using candidate units resulted smaller networks trained faster standard three sigmoidal types one Gaussian transfer functions used This superiority confirmed pool candidate units four different nonlinear activation functions used compete addition network Extensive simulations showed problem implementing random Boolean logic functions product units always chosen transfer functions',\n",
              " 'Our goal develop cognitive model humans acquire skills complex cognitive tasks We pursuing goal designing computational architectures NRL Navigation task requires competent sensorimotor coordination In paper analyze NRL Navigation task depth We use data experiments human subjects learning task guide us constructing cognitive model skill acquisition task Verbal protocol data augments black box view provided execution traces inputs outputs Computational experiments allow us explore space alternative architectures task guided quality fit human performance data',\n",
              " 'We show paper AGM postulates week ensure rational preservation conditional beliefs belief revision thus permitting improper responses sequences observations We remedy weakness proposing four additional postulates sound relative qualitative version probabilistic conditioning Contrary AGM framework proposed postulates characterize belief revision process may depend elements epistemic state necessarily captured belief set We also show simple modification AGM framework allow belief revision function epistemic states We establish modelbased representation theorem characterizes proposed postulates constrains turn way entrenchment orderings may transformed iterated belief revision',\n",
              " 'Results presented demonstrate learning finetuning search strategies using connectionist mechanisms Previous studies strategy learning within symbolic productionrule formalism addressed finetuning behavior Here twolayer connectionist system presented develops search weak taskspecific strategy finetunes performance The system applied simulated realtime balancecontrol task We compare performance onelayer twolayer networks showing ability twolayer network discover new features thus enhance original representation critical solving balancing task',\n",
              " 'Following terminology used adaptive control distinguish indirect learning methods learn explicit models dynamic structure system controlled direct learning methods We compare existing indirect method uses conventional dynamic programming algorithm closely related direct reinforcement learning method applying methods infinite horizon Markov decision problem unknown statetransition probabilities The simulations show although direct method requires much less space dramatically less computation per control action learning ability task superior compares favorably complex indirect method Although results address methods performances compare problems become difficult suggest given fixed amount computational power available per control action may better use direct reinforcement learning method augmented indirect techniques devote available resources computationally costly indirect method Comprehensive answers questions raised study depend many factors making eco nomic context computation',\n",
              " 'We propose general framework study belief change We begin defining belief terms knowledge plausibility agent believes knows true worlds considers plausible We consider properties defining interaction knowledge plausibility show properties affect properties belief In particular show assuming two natural properties belief becomes KD operator Finally add time picture This gives us framework talk knowledge plausibility hence belief time extends framework Halpern Fagin HF modeling knowledge multiagent systems We show framework quite expressive lets us model natural way number different scenarios belief change For example show capture analogue prior probabilities updated conditioning In related paper show two best studied scenarios belief revision belief update fit framework',\n",
              " 'Markov chain Monte Carlo MCMC used evaluating expectations functions interest target distribution This done calculating averages sample path Markov chain stationary distribution For computational efficiency Markov chain rapidly mixing This sometimes achieved careful design transition kernel chain basis detailed preliminary exploratory analysis An alternative approach might allow transition kernel adapt whenever new features encountered MCMC run However adaptation occurs infinitely often stationary distribution chain may disturbed We describe framework based concept Markov chain regeneration allows adaptation occur infinitely often disturb stationary distribution chain consistency samplepath averages Key Words Adaptive method Bayesian inference Gibbs sampling Markov chain Monte Carlo',\n",
              " 'A traditional interpolation model characterized choice regularizer applied interpolant choice noise model Typically regularizer single regularization constant ff noise model single parameter fi The ratio fffi alone responsible determining globally attributes interpolant complexity flexibility smoothness characteristic scale length characteristic amplitude We suggest interpolation models able capture one flavour simplicity complexity We describe Bayesian models interpolant smoothness varies spatially We emphasize importance practical implementation concept conditional convexity designing models many hyperparameters We apply new models interpolation neuronal spike data demonstrate substantial improvement generalization error',\n",
              " 'Author paper coordinator Machine Learning project StatLog This project supported financially European Community The main aim StatLog evaluate different learning algorithms using real industrial commercial applications As industrial partner contributor DaimlerBenz introduced different applications StatLog among fault diagnosis letter digit recognition creditscoring prediction number registered trucks We learned lot lessons project effected application oriented research field Machine Learning ML DaimlerBenz We distinguished especially research necessary prepare MLalgorithms handle real industrial commercial applications In paper describe shortly DaimlerBenz applications StatLog discuss shortcomings applied MLalgorithms finally outline fields think research necessary',\n",
              " 'This paper describes application reinforcement learning RL difficult real world problem elevator dispatching The elevator domain poses combination challenges seen RL research date Elevator systems operate continuous state spaces continuous time discrete event dynamic systems Their states fully observable nonstationary due changing passenger arrival rates In addition use team RL agents responsible controlling one elevator car The team receives global reinforcement signal appears noisy agent due effects actions agents random nature arrivals incomplete observation state In spite complications show results simulation surpass best heuristic elevator control algorithms aware These results demonstrate power RL large scale stochastic dynamic optimization problem practical utility',\n",
              " 'This paper presents Fringe Exploration technique efficient exploration partially observable domains The key idea applicable many exploration techniques keep statistics space possible shortterm memories instead agents current state space Experimental results partially observable maze difficult driving task visual routines show dramatic performance improvements',\n",
              " 'Performing policy iteration dynamic programming require knowledge relative rather absolute measures utility actions Baird calls advantages actions states Nevertheless existing methods dynamic programming including Bairds compute form absolute utility function For smooth problems advantages satisfy two differential consistency conditions including requirement free curl show enforcing lead appropriate policy improvement solely terms advantages',\n",
              " 'We introduce parallel approach DTSelect selecting features used inductive learning algorithms predict protein secondary structure DTSelect able rapidly choose small nonredundant feature sets pools containing hundreds thousands potentially useful features It building decision tree using features pool classifies set training examples The features included tree provide compact description training data thus suitable use inputs inductive learning algorithms Empirical experiments protein secondarystructure task sets complex features chosen DTSelect used augment standard artificial neural network representation yield surprisingly little performance gain even though features selected large feature pools We discuss possible reasons result',\n",
              " 'This paper presents extension package developed author Faculty Sciences Technology New University Lisbon designed experimentation CoarseGrained Distributed Genetic Algorithms DGA The package implemented extension Basic Sugal system developed Andrew Hunter University Sunderland UK primarily intended used research Sequential Serial Genetic Algorithms SGA',\n",
              " 'We explore representation D objects several distinct D views stored object We demonstrate ability twolayer network thresholded summation units support representations Using unsupervised Hebbian relaxation network learned recognize ten objects different viewpoints The training process led emergence compact representations specific input views When tested novel views objects network exhibited substantial generalization capability In simulated psychophysical experiments networks behavior qualitatively similar human subjects',\n",
              " 'Internal models environment important role play adaptive systems general particular importance supervised learning paradigm In paper demonstrate certain classical problems associated notion teacher supervised learning solved judicious use learned internal models components adaptive system In particular show supervised learning algorithms utilized cases unknown dynamical system intervenes actions desired outcomes Our approach applies supervised learning algorithm capable learning multilayer networks This paper revised version MIT Center Cognitive Science Occasional Paper We wish thank Michael Mozer Andrew Barto Robert Jacobs Eric Loeb James McClelland helpful comments manuscript This project supported part BRSG S RR awarded Biomedical Research Support Grant Program Division Research Resources National Institutes Health grant ATR Auditory Visual Perception Research Laboratories grant Siemens Corporation grant Human Frontier Science Program grant NJ awarded Office Naval Research',\n",
              " 'Technical Report February updated April This paper appear Proceedings Eleventh International Conference Machine Learning Abstract This paper presents algorithm incremental induction decision trees able handle numeric symbolic variables In order handle numeric variables new tree revision operator called slewing introduced Finally nonincremental method given finding decision tree based direct metric candidate tree',\n",
              " 'fl This research primarily conducted author University Calif Santa Cruz support ONR grant NK Harvard University supported ONR grant NK DARPA grant AFOSR Current address NEC Research Institute Independence Way Princeton NJ Email address nicklresearchnjneccom Supported ONR grants NK NJ Part research done author sabbatical Aiken Computation Laboratory Harvard partial support ONR grants NK NK Address Department Computer Science University California Santa Cruz Email address manfredcsucscedu',\n",
              " 'Many recent approaches avoiding utility problem speedup learning rely sophisticated utility measures significant numbers training data accurately estimate utility control knowledge Empirical results presented elsewhere indicate simple selection strategy retaining control rules derived training problem explanation quickly defines efficient set control knowledge training problems This simple selection strategy provides lowcost alternative exampleintensive approaches improving speed problem solver',\n",
              " 'Partigame new algorithm learning feasible trajectories goal regions high dimensional continuous statespaces In high dimensions essential learning plan uniformly statespace Partigame maintains decisiontree partitioning statespace applies techniques gametheory computational geometry efficiently adaptively concentrate high resolution critical areas The current version algorithm designed find feasible paths trajectories goal regions high dimensional spaces Future versions designed find solution optimizes realvalued criterion Many simulated problems tested ranging twodimensional ninedimensional statespaces including mazes path planning nonlinear dynamics planar snake robots restricted spaces In cases good solution found less ten trials minutes',\n",
              " 'Predictive inference seen process determining predictive distribution discrete variable given data set training examples values problem domain variables We consider three approaches computing predictive distribution assume joint probability distribution variables belongs set distributions determined set parametric models In simplest case predictive distribution computed using model maximum posteriori MAP posterior probability In evidence approach predictive distribution obtained averaging individual models model family In third case define predictive distribution using Rissanens new definition stochastic complexity Our experiments performed family Naive Bayes models suggest using data available stochastic complexity approach produces accurate predictions logscore sense However amount available training data decreased evidence approach clearly outperforms two approaches The MAP predictive distribution clearly inferior logscore sense two sophisticated approaches score MAP approach may still cases produce best results',\n",
              " 'Given problem casebased reasoning CBR system search case memory use stored cases find solution possibly modifying retrieved cases adapt required input specifications In paper introduce neural network architecture efficient casebased reasoning We show rigorous Bayesian probability propagation algorithm implemented feedforward neural network adapted CBR In approach efficient indexing problem CBR naturally implemented parallel architecture heuristic matching replaced probability metric This allows CBR perform theoretically sound Bayesian reasoning We also show probability propagation actually offers solution adaptation problem natural way',\n",
              " 'We present new generalpurpose algorithm learning classes valued functions generalization prediction model prove general upper bound expected absolute error algorithm terms scalesensitive generalization Vapnik dimension proposed Alon BenDavid CesaBianchi Haussler We give lower bounds implying upper bounds improved constant factor general We apply result together techniques due Haussler Benedek Itai obtain new upper bounds packing numbers terms scalesensitive notion dimension Using different technique obtain new bounds packing numbers terms Kearns Schapires fatshattering function We show apply packing bounds obtain improved general bounds sample complexity agnostic learning For gt establish weaker sufficient stronger necessary conditions class valued functions agnostically learnable within uniform GlivenkoCantelli class',\n",
              " 'It widely considered ultimate connectionist objective incorporate neural networks intelligent systems These systems intended possess varied repertoire functions enabling adaptable interaction nonstatic environment The first step direction develop various neural network algorithms models second step combine networks modular structure might incorporated workable system In paper consider one aspect second point namely processing reliability hiding wetware details Pre sented architecture type neural expert module named Authority An Authority consists number Minos modules Each Minos modules Authority processing capabilities varies respect particular specialization aspects problem domain The Authority employs collection Minoses like panel experts The expert highest confidence believed answer confidence quotient transmitted levels system hierarchy',\n",
              " 'Partially observable Markov decision processes pomdps model decision problems agent tries maximize reward face limited andor noisy sensor feedback While study pomdps motivated need address realistic problems existing techniques finding optimal behavior appear scale well unable find satisfactory policies problems dozen states After brief review pomdps paper discusses several simple solution methods shows capable finding nearoptimal policies selection extremely small pomdps taken learning literature In contrast show none able solve slightly larger noisier problem based robot navigation We find combination two novel approaches performs well problems suggest methods scaling even larger complicated domains',\n",
              " 'We propose new method construction Markov chains given stationary distribution This method based construction auxiliary chain stationary distribution picking elements auxiliary chain suitable number times The proposed method many advantages rivals It easy implement provides simple analysis faster efficient currently available techniques also adapted course simulation We make theoretical numerical comparisons characteristics proposed algorithm MCMC techniques',\n",
              " 'The problem making optimal decisions uncertain conditions central Artificial Intelligence If state world known times world modeled Markov Decision Process MDP MDPs studied extensively many methods known determining optimal courses action policies The realistic case state information partially observable Partially Observable Markov Decision Processes POMDPs received much less attention The best exact algorithms problems inefficient space time We introduce Smooth Partially Observable Value Approximation SPOVA new approximation method quickly yield good approximations improve time This method combined reinforcement learning methods combination effective test cases',\n",
              " 'The Katsuno Mendelzon KM theory belief update proposed reasonable model revising beliefs changing world However semantics update relies information readily available We describe alternative semantical view update observations incorporated belief set explaining observation terms set plausible events might caused observation b predicting consequences explanations We also allow possibility conditional explanations We show picture naturally induces update operator conforming KM postulates certain assumptions However argue assumptions always reasonable restrict ability integrate update forms revision reasoning action fl Some parts report appeared preliminary form An EventBased Abductive Model Update Proc Tenth Canadian Conf AI Banff Alta',\n",
              " 'This paper specifies main features Brainlike Neuronal Connectionist models argues need usefulness appropriate successively larger brainlike structures examines parallelhierarchical Recognition Cone models perception perspective examples structures The anatomy physiology behavior development visual system briefly summarized motivate architecture brainstructured networks perceptual recognition Results presented simulations carefully predesigned Recognition Cone structures perceive objects eg houses digitized photographs A framework perceptual learning introduced including mechanisms generationdiscovery feedbackguided growth new links nodes subject brainlike constraints eg local receptive fields global convergencedivergence The information processing transforms discovered generation finetuned feedbackguided reweighting links Some preliminary results presented brainstructured networks learn recognize simple objects eg letters alphabet cups apples bananas feedbackguided generation reweighting These show large improvements networks either lack brainlike structure orand learn reweighting links alone',\n",
              " 'The ability restructure decision tree efficiently enables variety approaches decision tree induction would otherwise prohibitively expensive Two approaches described one incremental tree induction ITI nonincremental tree induction using measure tree quality instead test quality DMTI These approaches several variants offer new computational classifier characteristics lend particular applications',\n",
              " 'We consider logistic regression model Gaussian prior distribution parameters We show accurate variational techniques used obtain closed form posterior distribution parameters given data thereby yielding posterior predictive model The results readily extended binary belief networks For belief networks also derive closed form posteriors presence missing values Finally show dual regression problem gives latent variable density model variational formulation leads exactly solvable EM updates',\n",
              " 'Mean field methods provide computationally efficient approximations posterior probability distributions graphical models Simple mean field methods make completely factorized approximation posterior unlikely accurate posterior multimodal Indeed posterior multimodal one modes captured To improve mean field approximation cases employ mixture models posterior approximations mixture component factorized distribution We describe efficient methods optimizing parameters models',\n",
              " 'The success evolutionary methods standard control learning tasks created need new benchmarks The classic pole balancing problem longer difficult enough serve viable yardstick measuring learning efficiency systems In paper present difficult version classic problem cart pole move plane We demonstrate neuroevolution system Enforced SubPopulations ESP solve difficult problem without velocity information',\n",
              " 'This paper introduces explores representational biases efficient learning spatial temporal spatiotemporal patterns connectionist networks CN massively parallel networks simple computing elements It examines learning mechanisms constructively build network structures encode information environmental stimuli successively higher resolutions needed tasks eg perceptual recognition network perform Some simple examples presented illustrate basic structures processes used networks ensure parsimony learned representations guiding system focus efforts minimal adequate resolution Several extensions basic algorithm efficient learning using multiresolution representations spatial temporal spatiotemporal patterns discussed',\n",
              " 'Qlearning uses TDmethods accelerate Qlearning The update complexity previous online Q implementations based lookuptables bounded size stateaction space Our faster algorithms update complexity bounded number actions The method based observation Qvalue updates may postponed needed',\n",
              " 'Massively parallel networks relatively simple computing elements offer attractive versatile framework exploring variety learning structures processes intelligent systems This paper briefly summarizes popular learning structures processes used networks It outlines range potentially powerful alternatives patterndirected inductive learning systems It motivates develops class new learning algorithms massively parallel networks simple computing elements We call class learning processes generative offer set mechanisms constructive adaptive determination network architecture number processing elements connectivity among function experience Generative learning algorithms attempt overcome limitations approaches learning networks rely modification weights links within otherwise fixed network topology eg rather slow learning need apriori choice network architecture Several alternative designs well range control structures processes used regulate form content internal representations learned networks examined Empirical results study generative learning algorithms briefly summarized several extensions refinements algorithms directions future research outlined',\n",
              " 'The use artificial neural networks domain autonomous vehicle navigation produced promising results ALVINN Pomerleau shown neural system drive vehicle reliably safely many different types roads ranging paved paths interstate highways Even impressive results several areas within neural paradigm autonomous road following still need addressed These include transparent navigation roads different type simultaneous use different sensors generalization road types neural system never seen The system presented addresses issue modular neural architecture uses pretrained ALVINN networks connectionist superstructure robustly drive many dif ferent types roads',\n",
              " 'We introduce constructive incremental learning system regression problems models data means locally linear experts In contrast approaches experts trained independently compete data learning Only prediction query required experts cooperate blending individual predictions Each expert trained minimizing penalized local cross val dation error using second order methods In way expert able find local distance metric adjusting size shape rece p tive field predictions valid also detect relevant n put features adjusting bias importance individual input dimensions We derive asymptotic results method In variety simulations properties algorithm demonstrated respect interference learning speed prediction accuracy feature detection task oriented incremental learning',\n",
              " 'The model nonBayesian agent faces repeated game incomplete information Nature appropriate tool modeling general agentenvironment interactions In model environment state controlled Nature may change arbitrarily feedbackreward function initially unknown The agent Bayesian form prior probability neither state selection strategy Nature reward function A policy agent function assigns action every history observations actions Two basic feedback structures considered In one perfect monitoring case agent able observe previous environment state part feedback imperfect monitoring case available agent reward obtained Both settings refer partially observable processes current environment state unknown Our main result refers competitive ratio criterion perfect monitoring case We prove existence efficient stochastic policy ensures competitive ratio obtained almost stages arbitrarily high probability efficiency measured terms rate convergence It shown optimal policy exist imperfect monitoring case Moreover proved perfect monitoring case exist deterministic policy satisfies long run optimality criterion In addition discuss maxmin criterion prove deterministic efficient optimal strategy exist imperfect monitoring case criterion Finally show approach longrun optimality viewed qualitative distinguishes previous work area',\n",
              " 'We describe polynomialtime algorithm learning axisaligned rectangles Q respect product distributions multipleinstance examples PAC model Here example consists n elements Q together label indicating whether n points rectangle learned We assume unknown product distribution D Q instances independently drawn according D The accuracy hypothesis measured probability would incorrectly predict whether one n points drawn D rectangle learned Our algorithm achieves accuracy probability ffi',\n",
              " 'We present new machine learning method given set training examples induces definition target concept terms hierarchy intermediate concepts definitions This effectively decomposes problem smaller less complex problems The method inspired Boolean function decomposition approach design digital circuits To cope high time complexity finding optimal decomposition propose suboptimal heuristic algorithm The method implemented program HINT HIerarchy Induction Tool experimentally evaluated using set artificial realworld learning problems It shown method performs well terms classification accuracy discovery meaningful concept hierarchies',\n",
              " 'In context inductive learning Bayesian approach turned successful estimating probabilities events learning examples The mprobability estimate developed handle situations In paper present mdistribution estimate extension mprobability estimate besides estimation probabilities covers also estimation probability distributions We focus application construction regression trees The theoretical results incorporated system automatic induction regression trees The results applying upgraded system several domains presented compared previous results',\n",
              " 'Realworld learning tasks often involve highdimensional data sets complex patterns missing features In paper review problem learning incomplete data two statistical perspectivesthe likelihoodbased Bayesian The goal twofold place current neural network approaches missing data within statistical framework describe set algorithms derived likelihoodbased framework handle clustering classification function approximation incomplete data principled efficient manner These algorithms based mixture modeling make two distinct appeals ExpectationMaximization EM principle Dempster et al estimation mixture components coping missing data This report describes research done Center Biological Computational Learning Artificial Intelligence Laboratory Massachusetts Institute Technology Support Center provided part grant National Science Foundation contract ASC Support laboratorys artificial intelligence research provided part Advanced Research Projects Agency Department Defense The authors supported part grant ATR Auditory Visual Perception Research Laboratories grant Siemens Corporation grant IRI National Science Foundation grant NJ Office Naval Research Zoubin Ghahramani supported grant McDonnellPew Foundation Michael I Jordan NSF Presidential Young Investigator',\n",
              " 'Recently proven dynamics deterministic finitestate automata DFA n states input symbols implemented sparse secondorder recurrent neural network SORNN n state neurons Omn secondorder weights sigmoidal discriminant functions We investigate constructive algorithm extended faulttolerant neural DFA implementations faults analog implementation neurons weights affect desired network performance We show tolerance weight perturbation achieved easily tolerance weight andor neuron stuckatzero faults however requires duplication network resources This result impact construction neural DFAs dense internal representation DFA states',\n",
              " 'Technical Report No Department Statistics University Washington October Abhijit Dasgupta graduate student Department Biostatistics University Washington Box Seattle WA email address dasguptabiostatwashingtonedu Adrian E Raftery Professor Statistics Sociology Department Statistics University Washington Box Seattle WA email address rafterystatwashingtonedu This research supported Office Naval Research Grant NJ The authors grateful Peter Guttorp Girardeau Henderson Robert Muise helpful discussions',\n",
              " 'In multiarmed bandit problem gambler must decide arm K nonidentical slot machines play sequence trials maximize reward This classical problem received much attention simple model provides tradeoff exploration trying arm find best one exploitation playing arm believed give best payoff Past solutions bandit problem almost always relied assumptions statistics slot machines In work make statistical assumptions whatsoever nature process generating payoffs slot machines We give solution bandit problem adversary rather wellbehaved stochastic process complete control payoffs In sequence T plays prove expected perround payoff algorithm approaches best arm rate OT give improved rate convergence best arm fairly low payoff We also prove general matching lower bound best possible performance algorithm setting In addition consider setting player team experts advising arm play give strategy guarantee expected payoff close best expert Finally apply result problem learning play unknown repeated matrix game allpowerful adversary',\n",
              " 'We show alternative way representing Bayesian belief network sensitivities probability distributions This representation equivalent traditional representation conditional probabilities makes dependencies nodes apparent intuitively easy understand We also propose QR matrix representation sensitivities andor conditional probabilities efficient memory requirements computational speed traditional representation computerbased implementations probabilistic inference We use sensitivities show certain class binary networks computation time approximate probabilistic inference positive upper bound error result independent size network Finally alternative traditional algorithms use conditional probabilities describe exact algorithm probabilistic inference uses QRrepresentation sensitivities updates probability distributions nodes network according messages neigh bors',\n",
              " 'The requirement train large neural networks quickly prompted design new massively parallel supercomputer using custom VLSI This design features processing nodes communicating mesh network connected directly processor chip Studies show peak performance range billion arithmetic operations per second This paper presents case custom hardware combines neural networkspecific features general programmable machine architecture briefly describes design progress',\n",
              " 'In many realworld domains like text categorization supervised learning requires large number training examples In paper describe active learning method uses committee learners reduce number training examples required learning Our approach similar Query Committee framework disagreement among committee members predicted label input part example used signal need knowing actual value label Our experiments text categorization using committee Winnowbased learners demonstrate approach reduce number labeled training examples required used single Winnow learner orders magnitude This paper review accepted publication another conference journal Acknowledgements The availability Reuters corpus Reuters STAT Data Manipulation Analysis Programs Perlman greatly assisted research date',\n",
              " 'covering formalized used extensively In work divideandconquer technique formalized well compared covering technique logic programming framework Covering works repeatedly specializing overly general hypothesis iteration focusing finding clause high coverage positive examples Divideandconquer works specializing overly general hypothesis focusing discriminating positive negative examples Experimental results presented demonstrating cases accurate hypotheses found divideandconquer covering Moreover since covering considers alternatives repeatedly tends less efficient divideandconquer never considers alternative twice On hand covering searches larger hypothesis space may result compact hypotheses found technique divideandconquer Furthermore divideandconquer contrast covering applicable learn ing recursive definitions',\n",
              " 'This paper introduces Recurrence Surface Approximation inductive learning method based linear programming predicts recurrence times using censored training examples examples available training output may lower bound right answer This approach augmented feature selection method chooses appropriate feature set within context linear programming generalizer Computational results field breast cancer prognosis shown A straightforward translation prediction method artificial neural network model also proposed',\n",
              " 'Minimum Message Length MML invariant Bayesian point estimation technique also consistent efficient We provide brief overview MML inductive inference Wallace Boulton Wallace Freeman informationtheoretic Bayesian interpretation We outline MML used statistical parameter estimation MML mixture modelling program Snob Wallace Boulton Wallace Wallace Dowe uses message lengths various parameter estimates enable combine parameter estimation selection number components The message length within constant logarithm posterior probability theory So MML theory also regarded theory highest posterior probability Snob currently assumes variables uncorrelated permits multivariate data Gaussian discrete multistate Poisson von Mises circular distributions',\n",
              " 'One challenges models cognitive phenomena development efficient exible interfaces low level sensory information high level processes For visual processing researchers long argued attentional mechanism required perform many tasks required high level vision This thesis presents VISIT connectionist model covert visual attention used vehicle studying interface The model efficient exible biologically plausible The complexity network linear number pixels Effective parallel strategies used minimize number iterations required The resulting system able efficiently solve two tasks particularly difficult standard bottomup models vision computing spatial relations visual search Simulations show networks behavior matches much known psychophysical data human visual attention The general architecture model also closely matches known physiological data human attention system Various extensions VISIT discussed including methods learning component modules',\n",
              " 'A Lyapunov function excitatoryinhibitory networks constructed The construction assumes symmetric interactions within excitatory inhibitory populations neurons antisymmetric interactions populations The Lyapunov function yields sufficient conditions global asymptotic stability fixed points If conditions violated limit cycles may stable The relations Lyapunov function optimization theory classical mechanics revealed The dynamics neural network symmetric interactions provably converges fixed points general assumptions This mathematical result helped establish paradigm neural computation fixed point attractors But reality interactions neurons brain asymmetric Furthermore dynamical behaviors seen brain confined fixed point attractors also include oscillations complex nonperiodic behavior These types dynamics realized asymmetric networks may useful neural computation For reasons important understand global behavior asymmetric neural networks The interaction excitatory neuron inhibitory neuron clearly asymmetric Here consider class networks incorporates fundamental asymmetry brains microcircuitry Networks class distinct populations excitatory inhibitory neurons antisymmetric interactions minimax dissipative Hamiltonian forms network dynamics',\n",
              " 'In Bayesian inference Bayes factor defined ratio posterior odds versus prior odds posterior odds simply ratio normalizing constants two posterior densities In many practical problems two posteriors different dimensions For cases current Monte Carlo methods bridge sampling method Meng Wong path sampling method Gelman Meng ratio importance sampling method Chen Shao directly applied In article extend importance sampling bridge sampling ratio importance sampling problems different dimensions Then find global optimal importance sampling bridge sampling ratio importance sampling sense minimizing asymptotic relative meansquare errors estimators Implementation algorithms asymptotically achieve optimal simulation errors developed two illustrative examples also provided',\n",
              " 'As knowledge bases used AI systems increase size access relevant information dominant factor cost inference This especially true analogical casebased reasoning ability system perform inference dependent efficient flexible access large base exemplars cases judged likely relevant solving problem hand In chapter discuss novel algorithm efficient associative matching relational structures large semantic networks The structure matching algorithm uses massively parallel hardware search memory knowledge structures matching given probe structure The algorithm built top PARKA massively parallel knowledge representation system runs Connection Machine We currently exploring utility algorithm CaPER casebased planning system',\n",
              " 'We consider use online stopping rules reduce number training examples needed paclearn Rather collect large training sample proved sufficient eliminate bad hypotheses priori idea instead observe training examples oneatatime decide online whether stop return hypothesis continue training The primary benefit approach detect hypothesizer actually converged halt training standard fixedsamplesize bounds This paper presents series sequential learning procedures distributionfree paclearning mistakebounded pac conversion distributionspecific paclearning respectively We analyze worst case expected training sample size procedures show often smaller existing fixed sample size bounds still providing exact worst case pacguarantees We also provide lower bounds show reductions best involve constant possibly log factors However empirical studies show sequential learning procedures actually use many times fewer training examples practice',\n",
              " 'This paper presents novel approach determine structural similarity guidance adaptation casebased reasoning Cbr We advance structural similarity assessment provides single numeric value specific structure two cases common inclusive modification rules needed obtain structure two cases Our approach treats retrieval matching adaptation group dependent processes This guarantees retrieval matching similar adaptable cases Both together enlarge overall problem solving performance Cbr explainability case selection adaptation considerably Although approach theoretical nature restricted specific domain give example taken domain industrial building design Additionally sketch two prototypical implementations approach',\n",
              " 'We analyze blameassignment task context experiencebased design redesign physical devices We identify three types blameassignment tasks differ types information take input design achieve desired behavior device design results undesirable behavior specific structural element design misbehaves We describe modelbased approach solving blameassignment task This approach uses structurebehaviorfunction models capture designers comprehension way device works terms causal explanations structure results behaviors We also address issue indexing models memory We discuss three types blameassignment tasks require different types indices accessing models Finally describe KRITIK system implements evaluates modelbased approach blame assignment',\n",
              " 'We present framework taskdriven knowledge acquisition development design support systems Different types knowledge enter knowledge base design support system defined illustrated formal knowledge acquisition vantage point Special emphasis placed taskstructure used guide acquisition application knowledge Starting knowledge planning steps design augmenting problemsolving knowledge supports design formal integrated model knowledge design constructed Based notion knowledge acquisition incremental process give account possibilities problem solving depending knowledge disposal system Finally depict different kinds knowledge interact design support system This research supported German Ministry Research Technology BMFT within joint project FABEL contract IW Project partners FABEL German National Research Center Computer Science GMD Sankt Augustin BSR Consulting GmbH Munchen Technical University Dresden HTWK Leipzig University Freiburg University Karlsruhe',\n",
              " 'The activity sorting like objects classes without help omniscient supervisor known unsupervised classification In AI symbolic connectionist camps study classification The statistical classifiers Autoclass Snob search theory best explain distribution given data whereas neural network classifiers Kohonens networks ART use vector quantization principle classifying data Previously many studies compared supervised classification algorithms challenging problem comparing unsupervised classifiers largely ignored We performed empirical comparison ART Autoclass Snob We highlight strengths weaknesses various classifiers Overall statistical classifiers especially Snob perform better neural network counterpart ART',\n",
              " 'AI research casebased reasoning led development many laboratory casebased systems As move towards introducing systems work environments explaining processes casebased reasoning becoming increasingly important issue In paper describe notion metacase illustrating explaining justifying casebased reasoning A metacase contains trace processing problemsolving episode provides explanation problemsolving decisions partial justification solution The language representing problemsolving trace depends model problem solving We describe taskmethod knowledge TMK model problemsolving describe representation metacases TMK language We illustrate explanatory scheme examples Interactive Kritik computerbased de',\n",
              " 'Statistical decision theory provides principled way estimate amino acid frequencies conserved positions protein family The goal minimize risk function expected squarederror distance estimates true population frequencies The minimumrisk estimates obtained adding optimal number pseudocounts observed data Two formulas presented one pseudocounts based marginal amino acid frequencies one pseudocounts based observed data Experimental results show profiles constructed using minimalrisk estimates discriminating constructed using existing methods',\n",
              " 'The purpose paper propose refinement notion innateness If merely identify innateness bias obtain poor characterisation notion since learning device relies bias makes choose given hypothesis instead another We show intuition innateness better captured characteristic bias related isotropy Generalist models learning shown rely isotropic bias whereas bias specialised models include specific priori knowledge learned necessarily anisotropic The socalled generalist models however turn specialised way learn symmetrical forms preferentially strictly deficiencies learning ability Because learning beings always show two properties generalist models may sometimes ruled bad candidates cognitive modelling',\n",
              " 'Reliable visionbased control autonomous vehicle requires ability focus attention important features input scene Previous work autonomous lane following system ALVINN Pomerleau yielded good results uncluttered conditions This paper presents artificial neural network based learning approach handling difficult scenes confuse ALVINN system This work presents mechanism achieving taskspecific focus attention exploiting temporal coherence A saliency map based upon computed expectation contents inputs next time step indicates regions input retina important performing task The saliency map used accentuate features important task deemphasize',\n",
              " 'Production scheduling problem sequentially configuring factory meet forecasted demands critical problem throughout manufacturing industry The requirement maintaining product inventories face unpredictable demand stochastic factory output makes standard scheduling models jobshop inadequate Currently applied algorithms simulated annealing constraint propagation must employ adhoc methods frequent replanning cope uncertainty In paper describe Markov Decision Process MDP formulation production scheduling captures stochasticity production demands The solution MDP value function used generate optimal scheduling decisions online A simple example illustrates theoretical superiority approach replanningbased methods We describe industrial application two reinforcement learning methods generating approximate value function domain Our results demonstrate deterministic noisy scenarios value function approximation effective technique',\n",
              " 'In paper investigate new formal model machine learning concept boolean function learned may exhibit uncertain probabilistic behaviorthus input may sometimes classified positive example sometimes negative example Such probabilistic concepts pconcepts may arise situations weather prediction measured variables accuracy insufficient determine outcome certainty We adopt Valiant model learning demands learning algorithms efficient general sense perform well wide class pconcepts distribution domain In addition giving many efficient algorithms learning natural classes pconcepts study develop detail underlying theory learning pconcepts',\n",
              " 'This paper highlights phenomenon causes deductively learned knowledge harmful used problem solving The problem occurs deductive problem solvers encounter failure branch search tree The backtracking mechanism problem solvers force program traverse whole subtree thus visiting many nodes twice using deductively learned rule using rules generated learned rule first place We suggest approach called utilization filtering solve problem Learners use approach submit problem solver filter function together knowledge acquired The function decides problem whether use learned knowledge part use We tested idea context lemma learning system filter uses probability subgoal failing decide whether turn lemma usage Experiments show improvement performance factor This paper concerned particular type harmful redundancy occurs deductive problem solvers employ backtracking search procedure use deductively learned knowledge accelerate search The problem failure branches search tree backtracking mechanism problem solver forces exploration whole subtree Thus search procedure visit many states twice using deductively learned rule using search path produced rule first place',\n",
              " 'fl The authors thank Rich Yee Vijay Gullapalli Brian Pinette Jonathan Bachrach helping clarify relationships heuristic search control We thank Rich Sutton Chris Watkins Paul Werbos Ron Williams sharing fundamental insights subject numerous discussions thank Rich Sutton first making us aware Korfs research thoughtful comments manuscript We grateful Dimitri Bertsekas Steven Sullivan independently pointing error earlier version article Finally thank Harry Klopf whose insight persistence encouraged interest class learning problems This research supported grants AG Barto National Science Foundation ECS ECS Air Force Office Scientific Research Bolling AFB AFOSR',\n",
              " 'Technical Report OSUCISRC TR Abstract One classical topics neural networks winnertakeall WTA widely used unsupervised competitive learning cortical processing attentional control Because global connectivity WTA networks however encode spatial relations input thus support sensory perceptual processing spatial relations important We propose new architecture maintains spatial relations input features This selection network builds LEGION Locally Excitatory Globally Inhibitory Oscillator Networks dynamics slow inhibition In input scene many objects patterns network selects largest object This system easily adjusted select several largest objects alternate time We show twostage selection network gains efficiency combining selection parallel removal noisy regions The network applied select salient object real images As special case selection network without local excitation gives rise new form oscillatory WTA',\n",
              " 'Reinforcement learning RL become central paradigm solving learningcontrol problems robotics artificial intelligence RL researchers focussed almost exclusively problems controller maximize discounted sum payoffs However emphasized Schwartz many problems eg optimal behavior limit cycle natural computationally advantageous formulate tasks controllers objective maximize average payoff received per time step In paper I derive new averagepayoff RL algorithms stochastic approximation methods solving system equations associated policy evaluation optimal control questions averagepayoff RL tasks These algorithms analogous popular TD Qlearning algorithms already developed discountedpayoff case One algorithms derived significant variation Schwartzs Rlearning algorithm Preliminary empirical results presented validate new algorithms',\n",
              " 'We present algorithms exactly learning unknown environments described deterministic finite automata The learner performs walk target automaton step observes output state chooses labeled edge traverse next state The learner means reset access teacher answers equivalence queries gives learner counterexamples hypotheses We present two algorithms The first case outputs observed learner always correct second case outputs might corrupted random noise The running times algorithms polynomial cover time underlying graph target automaton',\n",
              " 'Exploring mapping unknown environment fundamental problem studied variety contexts Many works focused finding efficient solutions restricted versions problem In paper consider model makes limited assumptions environment solve mapping problem general setting We model environment unknown directed graph G consider problem robot exploring mapping G We assume vertices G labeled thus robot hope succeeding unless given means distinguishing vertices For reason provide robot pebble device place vertex use identify vertex later In paper show If robot knows upper bound number vertices learn graph efficiently one pebble If robot know upper bound number vertices n filog log n pebbles necessary sufficient In cases algorithms deterministic',\n",
              " 'In recent years increasing interest learning Bayesian networks data One effective methods learning networks based minimum description length MDL principle Previous work shown learning procedure asymptotically successful probability one converge target distribution given sufficient number samples However rate convergence hitherto unknown In work examine sample complexity MDL based learning procedures Bayesian networks We show number samples needed learn close approximation terms entropy distance confidence ffi O log ffi log log This means sample complexity loworder polynomial error threshold sublinear confidence bound We also discuss constants term depend complexity target distribution Finally address questions asymptotic minimality propose method using sample complexity results speed learning process',\n",
              " 'Almost work Averagereward Reinforcement Learning ARL far focused tablebased methods scale domains large state spaces In paper propose two extensions modelbased ARL method called Hlearning address scaleup problem We extend Hlearning learn action models reward functions form Bayesian networks approximate value function using local linear regression We test algorithms several scheduling tasks simulated Automatic Guided Vehicle AGV show effective significantly reducing space requirement Hlearning making converge faster To best knowledge results first apply ing function approximation ARL',\n",
              " 'Understanding highdimensional real world data usually requires learning structure data space The structure may contain highdimensional clusters related complex ways Methods merge clustering selforganizing maps designed aid visualization interpretation data However methods often fail capture critical structural properties input Although selforganizing maps capture highdimensional topology represent cluster boundaries discontinuities Merge clustering extracts clusters capture local global topology This paper proposes algorithm combines topologypreserving characteristics selforganizing maps flexible adaptive structure learns cluster bound aries data',\n",
              " 'Although building sophisticated learning agents operate complex environments require learning perform multiple tasks applications reinforcement learning focussed single tasks In paper I consider class sequential decision tasks SDTs called composite sequential decision tasks formed temporally concatenating number elemental sequential decision tasks Elemental SDTs decomposed simpler SDTs I consider learning agent learn solve set elemental composite SDTs I assume structure composite tasks unknown learning agent The straightforward application reinforcement learning multiple tasks requires learning tasks separately waste computational resources memory time I present new learning algorithm modular architecture learns decomposition composite SDTs achieves transfer learning sharing solutions elemental SDTs across multiple composite SDTs The solution composite SDT constructed computationally inexpensive modifications solutions constituent elemental SDTs I provide proof one aspect learning algorithm',\n",
              " 'Existing approaches learning control robot arm rely supervised methods correct behavior explicitly given It difficult learn avoid obstacles using methods however examples obstacle avoidance behavior hard generate This paper presents alternative approach evolves neural network controllers genetic algorithms No inputoutput examples necessary since neuroevolution learns single performance measurement entire task grasping object The approach tested simulation OSCAR robot arm receives visual sensory input Neural networks evolved effectively avoid obstacles various locations reach random target locations',\n",
              " 'It widely accepted use compact representations lookup tables crucial scaling reinforcement learning RL algorithms realworld problems Unfortunately almost theory reinforcement learning assumes lookup table representations In paper address pressing issue combining function approximation RL present function approximator based simple extension state aggregation commonly used form compact representation namely soft state aggregation theory convergence RL arbitrary fixed soft state aggregation novel intuitive understanding effect state aggregation online RL new heuristic adaptive state aggregation algorithm finds improved compact representations exploiting nondiscrete nature soft state aggregation Preliminary empirical results also presented',\n",
              " 'This article introduces class incremental learning procedures specialized predictionthat using past experience incompletely known system predict future behavior Whereas conventional predictionlearning methods assign credit means difference predicted actual outcomes new methods assign credit means difference temporally successive predictions Although temporaldifference methods used Samuels checker player Hollands bucket brigade authors Adaptive Heuristic Critic remained poorly understood Here prove convergence optimality special cases relate supervisedlearning methods For realworld prediction problems temporaldifference methods require less memory less peak computation conventional methods produce accurate predictions We argue problems supervised learning currently applied really prediction problems sort temporaldifference methods applied advantage',\n",
              " 'This paper extends previous work Dyna class architectures intelligent systems based approximating dynamic programming methods Dyna architectures integrate trialanderror reinforcement learning executiontime planning single process operating alternately world learned model world In paper I present show results two Dyna architectures The DynaPI architecture based dynamic programmings policy iteration method related existing AI ideas evaluation functions universal plans reactive systems Using navigation task results shown simple DynaPI system simultaneously learns trial error learns world model plans optimal routes using evolving world model The DynaQ architecture based Watkinss Qlearning new kind reinforcement learning DynaQ uses less familiar set data structures DynaPI arguably simpler implement use We show DynaQ architectures easy adapt use changing environments',\n",
              " 'On large problems reinforcement learning systems must use parameterized function approximators neural networks order generalize similar situations actions In cases strong theoretical results accuracy convergence computational results mixed In particular Boyan Moore reported last years meeting series negative results attempting apply dynamic programming together function approximation simple control problems continuous state spaces In paper present positive results control tasks attempted one significantly larger The important differences used sparsecoarsecoded function approximators CMACs whereas used mostly global function approximators learned online whereas learned oine Boyan Moore others suggested problems encountered could solved using actual outcomes rollouts classical Monte Carlo methods TD algorithm However experiments always resulted substantially poorer performance We conclude reinforcement learning work robustly conjunction function approximators little justification present avoiding case general',\n",
              " 'We consider requirements online learninglearning must done incrementally realtime results learning available soon new example acquired Despite abundance methods learning examples used effectively online learning eg components reinforcement learning systems Most including radial basis functions CMACs Kohonens selforganizing maps developed paper share structure All expand original input representation higher dimensional representation unsupervised way map representation final answer using relatively simple supervised learner perceptron LMS rule Such structures learn rapidly reliably thought either scale poorly require extensive domain knowledge To contrary researchers Rosenblatt Gallant Smith Kanerva Prager Fallside argued expanded representation chosen largely random good results The main contribution paper develop test hypothesis We show simple randomrepresentation methods perform well nearestneighbor methods suited online learning significantly better backpropagation We find size random representation increase dimensionality problem unreasonably required size reduced substantially using unsupervisedlearning techniques Our results suggest randomness useful role play online supervised learning constructive induction',\n",
              " 'We consider problem dynamically apportioning resources among set options worstcase online framework The model study interpreted broad abstract extension wellstudied online prediction model general decisiontheoretic setting We show multiplicative weightupdate rule Littlestone Warmuth adapted model yielding bounds slightly weaker cases applicable considerably general class learning problems We show resulting learning algorithm applied variety problems including gambling multipleoutcome prediction repeated games prediction points R n',\n",
              " 'A new online learning algorithm minimizes statistical dependency among outputs derived blind separation mixed signals The dependency measured average mutual information MI outputs The source signals mixing matrix unknown except number sources The GramCharlier expansion instead Edgeworth expansion used evaluating MI The natural gradient approach used minimize MI A novel activation function proposed online learning algorithm equivariant property easily implemented neural network like model The validity new learning algorithm verified computer simulations',\n",
              " 'Overfitting wellknown problem fields symbolic connectionist machine learning It describes deterioration generalisation performance trained model In paper investigate ability novel artificial neural network bpsom avoid overfitting bpsom hybrid neural network combines multilayered feedforward network mfn Kohonens selforganising maps soms During training supervised backpropagation learning unsupervised som learning cooperate finding adequate hiddenlayer representations We show bpsom outperforms standard backpropagation also backpropagation weight decay dealing problem overfitting In addition show bpsom succeeds preserving generalisation performance hiddenunit pruning methods fail',\n",
              " 'We describe model iterated belief revision extends AGM theory revision account effect revision conditional beliefs agent In particular model ensures agent makes changes possible conditional component belief set Adopting Ramsey test minimal conditional revision provides acceptance conditions arbitrary rightnested conditionals We show problem determining acceptance nested conditional reduced acceptance tests unnested conditionals Thus iterated revision accomplished virtual manner using uniterated revision',\n",
              " 'Reinforcement learning techniques address problem learning select actions unknown dynamic environments It widely acknowledged use complex domains reinforcement learning techniques must combined generalizing function approximation methods artificial neural networks Little however understood theoretical properties combinations many researchers encountered failures practice In paper identify prime source failuresnamely systematic overestimation utility values Using Watkins QLearning example give theoretical account phenomenon deriving conditions one may expected cause learning fail Employing popular function approximators present experimental results support theoretical findings',\n",
              " 'We derive new selforganising learning algorithm maximises information transferred network nonlinear units The algorithm assume knowledge input distributions defined zeronoise limit Under conditions information maximisation extra properties found linear case Linsker The nonlinearities transfer function able pick higherorder moments input distributions perform something akin true redundancy reduction units output representation This enables network separate statistically independent components inputs higherorder generalisation Principal Components Analysis We apply network source separation cocktail party problem successfully separating unknown mixtures ten speakers We also show variant network architecture able perform blind deconvolution cancellation unknown echoes reverberation speech signal Finally derive dependencies information transfer time delays We suggest information maximisation provides unifying framework problems blind signal processing fl Please send comments tonysalkedu This paper appear Neural Computation The reference version Technical Report INC February Institute Neural Computation UCSD San Diego CA',\n",
              " 'This paper multidisciplinary review empirical statistical learning graphical model perspective Wellknown examples graphical models include Bayesian networks directed graphs representing Markov chain undirected networks representing Markov field These graphical models extended model data analysis empirical learning using notation plates Graphical operations simplifying manipulating problem provided including decomposition differentiation manipulation probability models exponential family Two standard algorithm schemas learning reviewed graphical framework Gibbs sampling expectation maximization algorithm Using operations schemas popular algorithms synthesized graphical specification This includes versions linear regression techniques feedforward networks learning Gaussian discrete Bayesian networks data The paper concludes sketching implications data analysis summarizing popular algorithms fall within framework presented',\n",
              " 'The utility problem speedup learning describes common behavior machine learning methods eventual degradation performance due increasing amounts learned knowledge The shape learning curve cost using learning method vs number training examples several domains suggests parameterized model relating performance amount learned knowledge mechanism limit amount learned knowledge optimal performance Many recent approaches avoiding utility problem speedup learning rely sophisticated utility measures significant numbers training data accurately estimate utility control knowledge Empirical results presented elsewhere indicate simple selection strategy retaining control rules derived training problem explanation quickly defines efficient set control knowledge training problems This simple selection strategy provides lowcost alternative exampleintensive approaches improving speed problem solver Experimentation illustrates existence minimum representing least cost learning curve reached training examples Stress placed controlling amount learned knowledge opposed knowledge An attempt also made relate domain characteristics shape learning curve',\n",
              " 'We compare kernel estimators single multilayered perceptrons radialbasis functions problems classification handwritten digits speech phonemes By taking two different applications employing many techniques report twodimensional study whereby domainindependent assessment learning methods possible We consider feedforward network one hidden layer As examples local methods use kernel estimators like knearest neighbor knn Parzen windows generalized knn Grow Learn Condensed Nearest Neighbor We also considered fuzzy knn due similarity As distributed networks use linear perceptron pairwise separating linear perceptron multilayer perceptrons sigmoidal hidden units We also tested radialbasis function network combination local distributed networks Four criteria taken comparison Correct classification test set network size learning time operational complexity We found perceptrons architecture suitable generalize better local memorybased kernel estimators require longer training precise computation Local networks simple learn quickly acceptably use memory',\n",
              " 'In current CBR systems case adaptation usually performed rulebased methods use taskspecific rules handcoded system developer The ability define rules depends knowledge task domain may available priori presenting serious impediment endowing CBR systems needed adaptation knowledge This paper describes ongoing research method address problem acquiring adaptation knowledge experience The method uses reasoning scratch based introspective reasoning requirements successful adaptation build library adaptation cases stored future reuse We describe tenets approach types knowledge requires We sketch initial computer implementation lessons learned open questions study',\n",
              " 'This position paper sketches framework modeling introspective reasoning discusses relevance framework modeling introspective reasoning memory search It argues effective flexible memory processing rich memories built five types explicitly represented selfknowledge knowledge information needs relationships different types information expectations actual behavior information search process desires ideal behavior representations expectations desires relate actual performance This approach modeling memory search illustration general principles modeling introspective reasoning step towards addressing problem reasoner human machinecan acquire knowledge properties knowledge base',\n",
              " 'Machine learning techniques perceived great potential means acquisition knowledge nevertheless use complex engineering domains still rare Most machine learning techniques studied context knowledge acquisition well defined tasks classification Learning tasks handled relatively simple algorithms Complex domains present difficulties approached combining strengths several complementing learning techniques overcoming weaknesses providing alternative learning strategies This study presents two perspectives macro micro viewing issue multistrategy learning The macro perspective deals decomposition overall complex learning task relatively welldefined learning tasks micro perspective deals designing multistrategy learning techniques supporting acquisition knowledge task The two perspectives discussed context',\n",
              " 'In order learn effectively reasoner must possess knowledge world able improve knowledge also must introspectively reason performs given task particular pieces knowledge needs improve performance current task Introspection requires declarative representations metaknowledge reasoning performed system performance task systems knowledge organization knowledge This chapter presents taxonomy possible reasoning failures occur performance task declarative representations failures associations failures particular learning strategies The theory based MetaXPs explanation structures help system identify failure types formulate learning goals choose appropriate learning strategies order avoid similar mistakes future The theory implemented computer model introspective reasoner performs multistrategy learning story understanding task',\n",
              " 'We introduce learning algorithm unsupervised neural networks based ideas statistical mechanics The algorithm derived mean field approximation large layered sigmoid belief networks We show approximately infer statistics networks without resort sampling This done solving mean field equations relate statistics unit Markov blanket Using statistics target values weights network adapted local delta rule We evaluate strengths weaknesses networks problems statistical pattern recognition',\n",
              " 'The paper describes selflearning control system mobile robot Based sensor information control system provide steering signal way collisions avoided Since case examples available system learns basis external reinforcement signal negative case collision zero otherwise We describe adaptive algorithm used discrete coding state space adaptive algorithm learning correct mapping input state vector output steering signal',\n",
              " 'fl Partially supported Advanced Research Projects Agency AFOSR Partially supported Air Force Office Scientific Research AFOSR FJ Advanced Research Projects Agency ONR NJ Office Naval Research ONR NJ z Partially funded Air Force Office Scientific Research AFOSR FJ Office Naval Research ONR NJ ONR N',\n",
              " 'The problem approximating smooth L p functions spaces spanned integer translates radially symmetric function well understood In case points translation ffi scattered throughout R approximation problem well understood stationary setting In work treat nonstationary setting assumption ffi small perturbation Z Our results similar many respects known results case ffi Z apply specifically examples Gauss kernel Generalized Multiquadric',\n",
              " 'In paper initiate investigation generalizations Probably Approximately Correct PAC learning model attempt significantly weaken target function assumptions The ultimate goal direction informally termed agnostic learning make virtually assumptions target function The name derives fact designers learning algorithms give belief Nature represented target function simple succinct explanation We give number positive negative results provide initial outline possibilities agnostic learning Our results include hardness results obvious generalization PAC model agnostic setting efficient general agnostic learning method based dynamic programming relationships loss functions agnostic learning algorithm learning problem involves hidden variables',\n",
              " 'In paper describe design implementation derivation replay framework dersnlpebl Derivational snlpebl based within partial order planner dersnlpebl replays previous plan derivations first repeating earlier decisions context new problem situation extending replayed path obtain complete solution new problem When replayed path extended new solution explanationbased learning ebl techniques employed identify features new problem prevent extension These features added censors retrieval stored case To keep retrieval costs low dersnlpebl normally stores plan derivations individual goals replays one derivations solving multigoal problems Cases covering multiple goals stored subplans individual goals successfully merged The aim constructing case library predict goal interactions store multigoal case set negatively interacting goals We provide empirical results demonstrating effectiveness dersnlpebl improving planning performance randomlygenerated problems drawn complex domain',\n",
              " 'Previous algorithms supervised sequence learning based dynamic recurrent networks This paper describes alternative class gradientbased systems consisting two feedforward nets learn deal temporal sequences using fast weights The first net learns produce context dependent weight changes second net whose weights may vary quickly The method offers potential STM storage efficiency A single weight instead fullfledged unit may sufficient storing temporal information Various learning methods derived Two experiments unknown time delays illustrate approach One experiment shows system used adaptive temporary variable binding',\n",
              " 'Evolutionary tree reconstruction important step many biological research problems yet extremely difficult variety computational statistical scientific reasons In particular reconstruction large trees containing significant amounts divergence especially challenging We present paper new tree reconstruction method call DiskCovering Method used recover accurate estimations evolutionary tree otherwise intractable datasets DCM obtains decomposition input dataset small overlapping sets closely related taxa reconstructs trees subsets using base phylogenetic method choice combines subtrees one tree entire set taxa Because subproblems analyzed DCM smaller computationally expensive methods maximum likelihood estimation used without incurring much cost At time taxa within subset closely related even simple methods neighborjoining much likely highly accurate The result DCMboosted methods typically faster accurate compared naive use method In paper describe basic ideas techniques DCM demonstrate advantages DCM experimentally simulating sequence evolution variety trees',\n",
              " 'Automating construction semantic grammars difficult interesting problem machine learning This paper shows semanticgrammar acquisition problem viewed learning searchcontrol heuristics logic program Appropriate control rules learned using new firstorder induction algorithm automatically invents useful syntactic semantic categories Empirical results show learned parsers generalize well novel sentences outperform previous approaches based connectionist techniques',\n",
              " 'Conventional speculative architectures use branch prediction evaluate likely execution path program execution However certain branches difficult predict One solution problem evaluate paths following conditional branch Predicated execution used implement form multipath execution Predicated architectures fetch issue instructions associated predicates These predicates indicate instruction commit result Predicating branch reduces number branches executed eliminating chance branch misprediction cost executing additional instructions In paper propose restricted form multipath execution called Dynamic Predication architectures little support predicated instructions instruction set Dynamic predication dynamically predicates instruction sequences form branch hammock concurrently executing paths branch A branch hammock short forward branch spans instructions form ifthen ifthenelse construct We mark constructs executable When decode stage detects sequence passes predicated instruction sequence dynamically scheduled execution core Our results show dynamic predication accrue speedups',\n",
              " 'In barn owl selforganization auditory map space external nucleus inferior colliculus ICx strongly influenced vision nature interaction unknown In paper biologically plausible minimalistic model ICx selforganization proposed ICx receives learn signal based owls visual attention When visual attention focused spatial location auditory input learn signal turned map allowed adapt A twodimensional Kohonen map used model ICx simulations performed evaluate learn signal would affect auditory map When primary area visual attention shifted different spatial locations auditory map shifted corresponding location The shift complete done early development partial done later Similar results observed barn owl visual field modified prisms Therefore simulations suggest learn signal based visual attention possible explanation auditory plasticity',\n",
              " 'The place fields hippocampal cells old animals sometimes change animal removed returned environment Barnes et al The ensemble correlation two sequential visits environment shows strong bimodality old animals near indicative remapping greater indicative similar representation experiences strong unimodality young animals greater indicative similar representation experiences One explanation multimap hypothesis multiple maps encoded hippocampus old animals may sometimes returning wrong map A theory proposed Samsonovich McNaughton suggests Barnes et al experiment implies maps prewired CA region hippocampus Here offer alternative explanation orthogonalization properties dentate gyrus DG region hippocampus interact errors selflocalization reset path integrator reentry environment produce bimodality',\n",
              " 'MIT Media Laboratory Perceptual Computing Section Technical Report No Appeared th IEEE Intl Conference Pattern Recognition ICPR Vienna Austria Abstract We present foveated gesture recognition system guides active camera foveate salient features based reinforcement learning paradigm Using vision routines previously implemented interactive environment determine spatial location salient body parts user guide active camera obtain images gestures expressions A hiddenstate reinforcement learning paradigm based Partially Observable Markov Decision Process POMDP used implement visual attention The attention module selects targets foveate based goal successful recognition uses new multiplemodel Qlearning formulation Given set target distractor gestures system learn foveate maximally discriminate particular gesture',\n",
              " 'Various extensions Genetic Algorithm GA attempt find optima search space containing several optima Many emulate natural speciation For coevolutionary learning succeed range management control problems learning game strategies methods must find optima However suitable comparison studies rare We compare two similar GA speciation methods fitness sharing implicit sharing Using realistic letter classification problem find advantages different circumstances Implicit sharing covers optima comprehensively population large enough species form optimum With population large enough fitness sharing find optima larger basins attraction ignore peaks narrow bases implicit sharing easily distracted This indicates speciated GA trying find many nearglobal optima possible implicit sharing works well population large enough This requires prior knowledge many peaks exist',\n",
              " 'Modern knowledge systems design typically employ multiple problemsolving methods turn use different kinds knowledge The construction heterogeneous knowledge system support practical design thus raises two fundamental questions accumulate huge volumes design information support heterogeneous design processing Fortunately partial answers questions exist separately Legacy databases already contain huge amounts generalpurpose design information In addition modern knowledge systems typically characterize kinds knowledge needed specific problemsolving methods quite precisely This leads us hypothesize methodspecific datatoknowledge compilation potential mechanism integrating heterogeneous knowledge systems legacy databases design In paper first outline general computational architecture called HIPED integration Then focus specific issue convert data accessed legacy database form appropriate problemsolving method used heterogeneous knowledge system We describe experiment legacy knowledge system called Interactive Kritik integrated ORACLE database using IDI communication tool The limited experiment indicates computational feasibility methodspecific datatoknowledge compilation also raises additional research issues',\n",
              " 'This paper investigates advantages disadvantages mixture experts ME model introduced connectionist community JJNH applied time series analysis WM two time series dynamics well understood The first series computergenerated series consisting mixture noisefree process quadratic map noisy process composition noisy linear autoregressive hyperbolic tangent There three main results ME model produces significantly better results single networks discovers regimes correctly also allows us characterize subprocesses variances due correct matching noise level model data avoids overfitting The second series laser series used Santa Fe competition ME model also obtains excellent outofsample predictions allows analysis shows overfitting',\n",
              " 'In natural visual experience different views object tend appear close temporal proximity animal manipulates object navigates around We investigated ability attractor network acquire view invariant visual representations associating first neighbors pattern sequence The pattern sequence contains successive views faces ten individuals change pose Under network dynamics developed Griniasty Tsodyks Amit multiple views given subject fall basin attraction We use independent component ICA representation faces input patterns Bell Sejnowski The ICA representation advantages principal component representation PCA viewpointinvariant recognition without attractor network suggesting ICA better representation PCA object recognition',\n",
              " 'This paper examines effects relaxed synchronization numerical parallel efficiency parallel genetic algorithms GAs We describe coarsegrain geographically structured parallel genetic algorithm Our experiments provide preliminary evidence asynchronous versions algorithms lower run time synchronous GAs Our analysis shows improvement due decreased synchronization costs high numerical efficiency eg fewer function evaluations asynchronous GAs This analysis includes critique utility traditional parallel performance measures parallel GAs',\n",
              " 'Key ideas statistical learning theory support vector machines generalized decision trees A support vector machine used decision tree The optimal decision tree characterized primal dual space formulation constructing tree proposed The result method generating logically simple decision trees multivariate linear nonlinear decisions The preliminary results indicate method produces simple trees generalize well respect decision tree algorithms single support vector machines',\n",
              " 'We previously shown regularization principles lead approximation schemes equivalent networks one layer hidden units called Regularization Networks In particular standard smoothness functionals lead subclass regularization networks well known Radial Basis Functions approximation schemes This paper shows regularization networks encompass much broader range approximation schemes including many popular general additive models neural networks In particular introduce new classes smoothness functionals lead different classes basis functions Additive splines well tensor product splines obtained appropriate classes smoothness functionals Furthermore generalization extends Radial Basis Functions RBF Hyper Basis Functions HBF also leads additive models ridge approximation models containing special cases Breimans hinge functions forms Projection Pursuit Regression several types neural networks We propose use term Generalized Regularization Networks broad class approximation schemes follow extension regularization In probabilistic interpretation regularization different classes basis functions correspond different classes prior probabilities approximating function spaces therefore different types smoothness assumptions In summary different multilayer networks one hidden layer collectively call Generalized Regularization Networks correspond different classes priors associated smoothness functionals classical regularization principle Three broad classes Radial Basis Functions generalized Hyper Basis Functions b tensor product splines c additive splines generalized schemes type ridge approximation hinge functions several perceptronlike neural networks onehidden layer This paper appear Neural Computation vol pages An earlier version',\n",
              " 'Learning inputoutput mapping set examples type many neural networks constructed perform regarded synthesizing approximation multidimensional function solving problem hypersurface reconstruction From point view form learning closely related classical approximation techniques generalized splines regularization theory This paper considers problems exact representation detail approximation linear nonlinear mappings terms simpler functions fewer variables Kolmogorovs theorem concerning representation functions several variables terms functions one variable turns almost irrelevant context networks learning We develop theoretical framework approximation based regularization techniques leads class threelayer networks call Generalized Radial Basis Functions GRBF since mathematically related wellknown Radial Basis Functions mainly used strict interpolation tasks GRBF networks equivalent generalized splines also closely related pattern recognition methods Parzen windows potential functions several neural network algorithms Kanervas associative memory backpropagation Kohonens topology preserving map They also interesting interpretation terms prototypes synthesized optimally combined learning stage The paper introduces several extensions applications technique discusses intriguing analogies neurobiological data c fl Massachusetts Institute Technology This paper describes research done within Center Biological Information Processing Department Brain Cognitive Sciences Artificial Intelligence Laboratory This research sponsored grant Office Naval Research ONR Cognitive Neural Sciences Division Artificial Intelligence Center Hughes Aircraft Corporation Alfred P Sloan Foundation National Science Foundation Support A I Laboratorys artificial intelligence research provided Advanced Research Projects Agency Department Defense Army contract DACAC part ONR contract NK',\n",
              " 'This article describes reasoner improve understanding incompletely understood domain application already knows novel problems domain Casebased reasoning process using past experiences stored reasoners memory understand novel situations solve novel problems However process assumes past experiences well understood provide good lessons used future situations This assumption usually false one learning novel domain since situations encountered previously domain might understood completely Furthermore reasoner may even case adequately deals new situation may able access case using existing indices We present theory incremental learning based revision previously existing case knowledge response experiences situations The theory implemented casebased story understanding program learn new case situations case already exists b learn index case memory c incrementally refine understanding case using reason new situations thus evolving better understanding domain experience This research complements work casebased reasoning providing mechanisms case library automatically built use casebased reasoning program',\n",
              " 'We present statistical model genes DNA A Generalized Hidden Markov Model GHMM provides framework describing grammar legal parse DNA sequence Stormo Haussler Probabilities assigned transitions states GHMM generation nucleotide base given particular state Machine learning techniques applied optimize probabilities using standardized training set Given new candidate sequence best parse deduced model using dynamic programming algorithm identify path model maximum probability The GHMM flexible modular new sensors additional states inserted easily In addition provides simple solutions integrating cardinality constraints reading frame constraints indels homology searching The description results implementation genefinding model called Genie presented The exon sensor codon frequency model conditioned windowed nucleotide frequency preceding codon Two neural networks used Brunak Engelbrecht Knudsen splice site prediction We show simple model performs quite well For crossvalidated standard test set genes ftpwwwhgclblgovpubgenesets human DNA genefinding system identified proteincoding bases correctly specificity exons exactly identified specificity Genie shown perform favorably compared several genefinding systems',\n",
              " 'We examine questions optimality domination repeated stage games one players may draw strategies perhaps different computationally bounded sets We also consider optimality domination bounded convergence rates infinite payoff We develop notion grace period handle problem vengeful strategies',\n",
              " 'We study problem efficiently learning play game optimally unknown adversary chosen computationally bounded class We contribute line research playing games finite automata expand scope research considering new classes adversaries We introduce natural notions games recent history adversaries whose current action determined simple boolean formula recent history play games statistical adversaries whose current action determined simple function statistics entire history play In cases give efficient algorithms learning play pennymatching difficult game called contract We also give powerful positive result date learning play finite automata efficient algorithm learning play game finite automata probabilistic actions low cover time',\n",
              " 'MORGAN integrated system finding genes vertebrate DNA sequences MORGAN uses variety techniques accomplish task distinctive decision tree classifier The decision tree system combined new methods identifying start codons donor sites acceptor sites brought together framesensitive dynamic programming algorithm finds optimal segmentation DNA sequence coding noncoding regions exons introns The optimal segmentation dependent separate scoring function takes subsequence assigns score reflecting probability sequence exon The scoring functions MORGAN sets decision trees combined give probability estimate Experimental results database vertebrate DNA sequences show MORGAN excellent performance many different measures On separate test set achieves overall accuracy correlation coefficient sensitivity specificity coding bases In addition MORGAN identifies coding exons exactly ie beginning end coding regions predicted correctly This paper describes MORGAN system including decision tree routines algorithms site recognition performance benchmark database vertebrate DNA',\n",
              " 'In paper report use backpropagation based neural networks implement phase computational intelligence process PYTHIA expert system supporting numerical simulation applications modelled partial differential equations PDEs PYTHIA exemplar based reasoning system provides advice method parameters use simulation specified PDE based application When advice requested characteristics given model matched characteristics previously seen classes models The performance various solution methods previously seen similar classes models used basis predicting method use Thus major step reasoning process PYTHIA involves analysis categorization models classes models based characteristics In study demonstrate use neural networks identify class predefined models whose characteristics match ones specified PDE based application',\n",
              " 'A method described reduces hypotheses space efficient easily interpretable reduction criteria called reduction A learning algorithm described based reduction analyzed using probability approximate correct learning results The results obtained reducing rule set equivalent set kDNF formulas The goal learning algorithm induce compact rule set describing basic dependencies within set data The reduction based criterion exible gives semantic interpretation rules fulfill criteria Comparison syntactical hypotheses reduction show reduction improves search smaller probability missclassification',\n",
              " 'We identify three principle factors affecting performance learning networks localized units unit noise sample density structure target function We analyze effect unit receptive field parameters factors use analysis propose new learning algorithm dynamically alters receptive field properties learning',\n",
              " 'SemiMarkov Decision Problems continuous time generalizations discrete time Markov Decision Problems A number reinforcement learning algorithms developed recently solution Markov Decision Problems based ideas asynchronous dynamic programming stochastic approximation Among TD Qlearning Realtime Dynamic Programming After reviewing semiMarkov Decision Problems Bellmans optimality equation context propose algorithms similar named adapted solution semiMarkov Decision Problems We demonstrate algorithms applying problem determining optimal control simple queueing system We conclude discussion circumstances algorithms may usefully ap plied',\n",
              " 'There recently widespread interest use multiple models classification regression statistics neural networks communities The Hierarchical Mixture Experts HME successful number regression problems yielding significantly faster training use Expectation Maximisation algorithm In paper extend HME classification results reported three common classification benchmark tests ExclusiveOr Ninput Parity Two Spirals',\n",
              " 'One important factor determining computa tional complexity evaluating probabilistic network cardinality state spaces nodes By varying granularity state spaces one trade accuracy result computational efficiency We present time procedure approximate evaluation probabilistic networks based idea On application simple networks proce dure exhibits smooth improvement approxi mation quality computation time increases This suggests statespace abstraction one useful control parameter designing real time probabilistic reasoners',\n",
              " 'Existing complexity measures contemporary learning theory conveniently applied specific learning problems eg training sets Moreover typically nongeneric ie necessitate making assumptions way learner operate The lack satisfactory generic complexity measure learning problems poses difficulties researchers various areas present paper puts forward idea may help alleviate It shows supervised learning problems fall two generic complexity classes one associated computational tractability By determining class particular problem belongs thus effectively evaluate degree generic difficulty',\n",
              " 'The paper investigates statistical effects may need exploited supervised learning It notes effects classified according conditionality order proposes learning algorithms typically form bias towards particular classes effect It presents results empirical study statistical bias backpropagation The study involved applying algorithm wide range learning problems using variety different internal architectures The results study revealed backpropagation specific bias general direction statistical rather relational effects The paper shows existence bias effectively constitutes weakness algorithms ability discount noise',\n",
              " 'segmentation Preliminary results Abstract Scatterpartitioning Radial Basis Function RBF networks increase number degrees freedom complexity inputoutput mapping estimated basis supervised training data set Due superior expressive power scatterpartitioning Gaussian RBF GRBF model termed Supervised Growing Neural Gas SGNG selected literature SGNG employs onestage errordriven learning strategy capable generating removing hidden units synaptic connections A slightly modified SGNG version tested function estimator training surface fitted image ie D signal whose size finite The relationship generation learning system disjointed maps hidden units presence image pictorially homogeneous subsets segments investigated Unfortunately examined SGNG version performs poorly function estimator image segmenter This may due intrinsic inadequacy onestage errordriven learning strategy adjust structural parameters output weights simultaneously consistently In framework RBF networks studies investigate combination twostage errordriven learning strategies synapse generation removal criteria Internal report paper entitled Image segmentation scatterpartitioning RBF networks A feasibility study presented conference Applications Science Neural Networks Fuzzy Systems Evolutionary Computation part SPIEs International Symposium Optical Science Engineering Instrumentation July San Diego CA',\n",
              " 'A distinct advantage symbolic learning algorithms artificial neural networks typically concept representations form easily understood humans One approach understanding representations formed neural networks extract symbolic rules trained networks In paper describe investigate approach extracting rules networks uses NofM extraction algorithm network training method soft weightsharing Previously NofM algorithm successfully applied knowledgebased neural networks Our experiments demonstrate extracted rules generalize better rules learned using C system In addition accurate extracted rules also reasonably comprehensible',\n",
              " 'Our experience showed us exibility expressing parallel algorithm simulating neural networks desirable even possible obtain efficient solution single training algorithm We believe advantages clear easy understand program predominates disadvantages approaches allowing specific machine neural network algorithm We currently investigate neural network models worth parallelized resulting parallel algorithms composed common basic building blocks logarithmic tree efficient communication structure connections connections D Ackley G Hinton T Sejnowski A Learning Algorithm Boltzmann Machines Cognitive Science pp B M Forrest et al Implementing Neural Network Models Parallel Computers The computer Journal vol W Giloi Latency Hiding Message Passing Architectures International Parallel Processing Symposium April Cancun Mexico IEEE Computer Society Press T Nordstrm B Svensson Using And Designing Massively Parallel Computers Artificial Neural Networks Journal Of Parallel And Distributed Computing vol pp A Kramer A Vincentelli Efficient parallel learning algorithms neural networks Advances Neural Information Processing Systems I D Touretzky ed pp T Kohonen SelfOrganization Associative Memory SpringerVerlag Berlin D A Pomerleau G L Gusciora D L Touretzky H T Kung Neural Network Simulation Warp Speed How We Got Million Connections Per Second IEEE Intern Conf Neural Networks July A Rbel Dynamic selection training patterns neural networks A new method control generalization Technical Report Technical University Berlin D E Rumelhart D E Hinton R J Williams Learning internal representations error propagation Rumelhart McClelland eds Parallel Distributed Processing Explorations Microstructure Cognition vol I pp Bradford BooksMIT Press Cambridge MA W Schiffmann M Joost R Werner Comparison optimized backpropagation algorithms Proc European Symposium Artificial Neural Networks ESANN Brussels pp J Schmidhuber Accelerated Learning BackPropagation Nets Connectionism perspective Elsevier Science Publishers BV NorthHolland pp M Taylor P Lisboa eds Techniques Applications Neural Networks Ellis Horwood M Witbrock M Zagha An implementation backpropagation learning GF large SIMD parallel computer Parallel Computing vol pp X Zhang M Mckenna J P Mesirov D L Waltz The backpropagation algorithm grid hypercube architectures Parallel Computing vol pp',\n",
              " 'In order learn effectively system must possess knowledge world able improve knowledge also must introspectively reason performs given task particular pieces knowledge needs improve performance current task Introspection requires declaratflive representation reasoning performed system performance task This paper presents taxonomy possible reasoning failures occur task declarative representations associations particular learning strategies We propose theory MetaXPs explanation structures help system identify failure types choose appropriate learning strategies order avoid similar mistakes future A program called MetaAQUA embodies theory processes examples domain drug smuggling',\n",
              " 'Although artificial neural networks applied variety realworld scenarios remarkable success often criticized exhibiting low degree human comprehensibility Techniques compile compact sets symbolic rules artificial neural networks offer promising perspective overcome obvious deficiency neural network representations This paper presents approach extraction ifthen rules artificial neural networks Its key mechanism validity interval analysis generic tool extracting symbolic knowledge propagating rulelike knowledge Backpropagationstyle neural networks Empirical studies robot arm domain illustrate appropriateness proposed method extracting rules networks realvalued distributed representations',\n",
              " 'In paper examine method feature subset selection based Information Theory Initially framework defining theoretically optimal computationally intractable method feature subset selection presented We show goal eliminate feature gives us little additional information beyond subsumed remaining features In particular case irrelevant redundant features We give efficient algorithm feature selection computes approximation optimal feature selection criterion The conditions approximate algorithm successful examined Empirical results given number data sets showing algorithm effectively han dles datasets large numbers features',\n",
              " 'In paper address problem casebased learning presence irrelevant features We review previous work attribute selection present new algorithm Oblivion carries greedy pruning oblivious decision trees effectively store set abstract cases memory We hypothesize approach efficiently identify relevant features even interact parity concepts We report experimental results artificial domains support hypothesis experiments natural domains show improvement cases others In closing discuss implications experiments consider additional work irrelevant features outline directions future research',\n",
              " 'Learning plays vital role development situated agents In paper explore use reinforcement learning shape robot perform predefined target behavior We connect simulated real robots A LECSYS parallel implementation learning classifier system extended genetic algorithm After classifying different kinds Animatlike behaviors explore effects learning different types agents architecture monolithic flat hierarchical training strategies In particular hierarchical architecture requires agent learn coordinate basic learned responses We show best results achieved agents architecture training strategy match structure behavior pattern learned We report results number experiments carried simulated real environments show results simulations carry smoothly real robots While experiments deal simple reactive behavior one demonstrate use simple general memory mechanism As whole experimental activity demonstrates classifier systems genetic algorithms practically employed develop autonomous agents',\n",
              " 'Although probabilistic inference general Bayesian belief network NPhard problem inference computation time reduced practical cases exploiting domain knowledge making appropriate approximations knowledge representation In paper introduce property similarity states new method approximate knowledge representation based property We define two states node similar likelihood ratio probabilities depend instantiations nodes network We show similarity states exposes redundancies joint probability distribution exploited reduce computational complexity probabilistic inference networks multiple similar states For example show BNO networka two layer networks often used diagnostic problemscan reduced close network multiple similar states Probabilistic inference new network done polynomial time respect size network results queries practical importance close results obtained exponential time original network The error introduced reduction converges zero faster exponentially respect degree polynomial describing resulting computational complexity',\n",
              " 'Multilayer architectures used Bayesian belief networks Helmholtz machines provide powerful framework representing learning higher order statistical relations among inputs Because exact probability calculations models often intractable much interest finding approximate algorithms We present algorithm efficiently discovers higher order structure using EM Gibbs sampling The model interpreted stochastic recurrent network ambiguity lowerlevel states resolved feedback higher levels We demonstrate performance algorithm bench mark problems',\n",
              " 'In paper study extension distributionfree model learning introduced Valiant also known probably approximately correct PAC model allows presence malicious errors examples given learning algorithm Such errors generated adversary unbounded computational power access entire history learning algorithms computation Thus study worstcase model errors Our results include general methods bounding rate error tolerable learning algorithm efficient algorithms tolerating nontrivial rates malicious errors equivalences problems learning errors standard combinatorial optimization problems',\n",
              " 'We investigate problem computing posterior probability model class given data sample prior distribution possible parameter settings By model class mean group models share parametric form In general posterior may hard compute highdimensional parameter spaces usually case realworld applications In literature several methods computing posterior approximately proposed quality approximations may depend heavily size available data sample In work interested testing well approximative methods perform realworld problem domains In order conduct study chosen model family finite mixture distributions With certain assumptions able derive model class posterior analytically model family We report series model class selection experiments realworld data sets true posterior approximations compared The empirical results support hypothesis approximative techniques provide good estimates true posterior especially sample size grows large',\n",
              " 'Email FirstnameLastnamecsHelsinkiFI Report C University Helsinki Department Computer Science Abstract In paper explore use finite mixture models building decision support systems capable sound probabilistic inference Finite mixture models many appealing properties computationally efficient prediction reasoning phase universal sense approximate problem domain distribution handle multimodality well We present formulation model construction problem Bayesian framework finite mixture models describe Bayesian inference performed given model The model construction problem seen missing data estimation describe realization ExpectationMaximization EM algorithm finding good models To prove feasibility approach report crossvalidated empirical results several publicly available classification problem datasets compare results corresponding results obtained alternative techniques neural networks decision trees The comparison based best results reported literature datasets question It appears using theoretically sound Bayesian framework suggested reported results outperformed relatively small effort',\n",
              " 'One application models reasoning behavior allow reasoner introspectively detect repair failures reasoning process We address issues transferability models versus specificity knowledge kinds knowledge needed selfmodeling knowledge structured evaluation introspective reasoning systems We present ROBBIE system implements model planning processes improve planner response reasoning failures We show ROBBIEs hierarchical model balances model generality access implementationspecific details discuss qualitative quantitative measures used evaluating introspective component',\n",
              " 'We consider multicriteria sequential decision making problems criteria ordered according importance Structural properties problems touched reinforcement learning algorithms learn asymptotically optimal decisions derived Computer experiments confirm theoretical results provide insight learning processes',\n",
              " 'Acyclic digraphs ADGs widely used describe dependences among variables multivariate distributions In particular likelihood functions ADG models admit convenient recursive factorizations often allow explicit maximum likelihood estimates well suited building Bayesian networks expert systems There may however many ADGs determine dependence Markov model Thus family ADGs given set vertices naturally partitioned Markovequivalence classes class associated unique statistical model Statistical procedures model selection model averaging fail take account equivalence classes may incur substantial computational inefficiencies Recent results shown Markovequivalence class uniquely determined single chain graph essential graph Markovequivalent simultaneously ADGs equivalence class Here propose two stochastic Bayesian model averaging selection algorithms essential graphs apply analysis three discretevariable data sets',\n",
              " 'Given set samples unknown probability distribution study problem constructing good approximative Bayesian network model probability distribution question This task viewed search problem goal find maximal probability network model given data In work make attempt learn arbitrarily complex multiconnected Bayesian network structures since resulting models unsuitable practical purposes due exponential amount time required reasoning task Instead restrict special class simple treestructured Bayesian networks called Bayesian prototype trees polynomial time algorithm Bayesian reasoning exists We show probability given Bayesian prototype tree model evaluated given data evaluation criterion used stochastic simulated annealing algorithm searching model space The simulated annealing algorithm provably finds maximal probability model provided sufficient amount time used',\n",
              " 'We use simple illustrative example expose main ideas Evidential Probability Specifically show use acceptance rule naturally leads use intervals represent probabilities change opinion due experience facilitated probabilities concerning compound experiments events computed given proper knowledge underlying distributions',\n",
              " 'This paper presents UTree reinforcement learning algorithm uses selective attention shortterm memory simultaneously address intertwined problems large perceptual state spaces hidden state By combining advantages work instancebased memorybased learning work robust statistical tests separating noise task structure method learns quickly creates taskrelevant state distinctions handles noise well UTree uses treestructured representation related work Prediction Suffix Trees Ron et al Partigame Moore Galgorithm Chapman Kaelbling Variable Resolution Dynamic Programming Moore It builds Utile Suffix Memory McCallum c used shortterm memory selective perception The algorithm demonstrated solving highway driving task agent weaves around slower faster traffic The agent uses active perception simulated eye movements The environment hidden state time pressure stochasticity world states percepts From environment sensory system agent uses utile distinction test build tree represents depththree memory necessary internal statesfar fewer states would resulted fixedsized historywindow ap proach',\n",
              " 'Feature selection problem choosing subset relevant features In general exhaustive search bring optimal subset With monotonic measure exhaustive search avoided without sacrificing optimality Unfortunately error distancebased measures monotonic A new measure employed work monotonic fast compute The search relevant features according measure guaranteed complete exhaustive Experiments conducted verification',\n",
              " 'This paper describes novel method dialogue agent learn choose optimal dialogue strategy While widely agreed dialogue strategies formulated terms communicative intentions little work automatically optimizing agents choices multiple ways realize communicative intention Our method based combination learning algorithms empirical evaluation techniques The learning component method based algorithms reinforcement learning dynamic programming Qlearning The empirical component uses PARADISE evaluation framework Walker et al identify important performance factors provide performance function needed learning algorithm We illustrate method dialogue agent named ELVIS EmaiL Voice Interactive System supports access email phone We show ELVIS learn choose among alternate strategies agent initiative reading messages summarizing email folders',\n",
              " 'This paper outlines problems may occur Reduced Error Pruning Inductive Logic Programming notably efficiency Thereafter new method Incremental Reduced Error Pruning proposed attempts address problems Experiments show many noisy domains method much efficient alternative algorithms along slight gain accuracy However experiments show well use algorithm recommended domains specific concept description',\n",
              " 'EEG analysis played key role modeling brains cortical dynamics relatively little effort devoted developing EEG limited means communication If several mental states reliably distinguished recognizing patterns EEG paralyzed person could communicate device like wheelchair composing sequences mental states EEG pattern recognition difficult problem hinges success finding representations EEG signals patterns distinguished In article report study comparing three EEG representations unprocessed signals reduceddimensional representation using KarhunenLoeve transform frequencybased representation Classification performed twolayer neural network implemented CNAPS server processor SIMD architecture Adaptive Solutions Inc Execution time comparisons show hundredfold speed Sun Sparc The best classification accuracy untrained samples using frequencybased representation',\n",
              " 'This paper surveys field reinforcement learning computerscience perspective It written accessible researchers familiar machine learning Both historical basis field broad selection current work summarized Reinforcement learning problem faced agent learns behavior trialanderror interactions dynamic environment The work described resemblance work psychology differs considerably details use word reinforcement The paper discusses central issues reinforcement learning including trading exploration exploitation establishing foundations field via Markov decision theory learning delayed reinforcement constructing empirical models accelerate learning making use generalization hierarchy coping hidden state It concludes survey implemented systems assessment practical utility current methods reinforcement learning',\n",
              " 'We add internal memory XCS classifier system We test XCS internal memory named XCSM nonMarkovian environments two four aliasing states Experimental results show XCSM easily converge optimal solutions simple environments moreover XCSMs performance stable respect size internal memory involved learning However results present evidence complex nonMarkovian environments XCSM may fail evolve optimal solution Our results suggest happens exploration strategies currently employed XCS adequate guarantee convergence optimal policy XCSM complex nonMarkovian environments',\n",
              " 'Simple modification standard hill climbing optimization algorithm taking account learning features discussed Basic concept approach socalled probability vector single entries determine probabilities appearance entries nbit vectors This vector used random generation nbit vectors form neighborhood specified given probability vector Within neighborhood best solutions smallest functional values minimized function recorded The feature learning introduced probability vector updated formal analogue Hebbian learning rule wellknown theory artificial neural networks The process repeated probability vector entries close either zero one The resulting probability vector unambiguously determines nbit vector may interpreted optimal solution given optimization task Resemblance genetic algorithms discussed Effectiveness proposed method illustrated example looking global minima highly multimodal function',\n",
              " 'This paper describes efficient methods exact approximate implementation MINFEATURES bias prefers consistent hypotheses definable features possible This bias useful learning domains many irrelevant features present training data We first introduce FOCUS new algorithm exactly implements MINFEATURES bias This algorithm empirically shown substantially faster FOCUS algorithm previously given Almuallim Dietterich We introduce MutualInformationGreedy SimpleGreedy WeightedGreedy algorithms apply efficient heuristics approximating MINFEATURES bias These algorithms employ greedy heuristics trade optimality computational efficiency Experimental studies show learning performance ID greatly improved algorithms used preprocess training data eliminating irrelevant features IDs consideration In particular WeightedGreedy algorithm provides excellent efficient approximation MIN',\n",
              " 'A statistical approach decision tree modeling described In approach decision tree modeled parametrically process output generated input sequence decisions The resulting model yields likelihood measure goodness fit allowing ML MAP estimation techniques utilized An efficient algorithm presented estimate parameters tree The model selection problem presented several alternative proposals considered A hidden Markov version tree described data sequences temporal dependencies',\n",
              " 'A binary matrix Our task infer given z A given assumptions statistical properties n This problem arises decoding noisy communication z transmitted using errorcorrecting code based parity checks original signal inference sequence linear feedback shift register LFSR noisy observation sequence P zjA I assume decoders aim find probable For large N exhaustive search N possible sequences feasible One way attack combinatorial problem create related continuous optimization problem discrete variables replaced real variables Here I derive continuous representation terms free energy approximation awkward posterior distribution',\n",
              " 'An intelligent system capable adapting constantly changing environment It therefore ought capable learning perceptual interactions surroundings This requires certain amount plasticity structure Any attempt model perceptual capabilities living system matter construct synthetic system comparable abilities must therefore account plasticity variety developmental learning mechanisms This paper examines results neuroanatomical morphological well behavioral studies development visual perception integrates computational framework suggests several interesting experiments computational models yield insights development visual perception In order understand development information processing structures brain one needs knowledge changes undergoes birth maturity context normal environment However knowledge development aberrant settings also extremely useful reveals extent development function environmental experience opposed genetically determined prewiring Accordingly consider development visual system normal restricted rearing conditions The role experience early development sensory systems general visual system particular widely studied variety experiments involving carefully controlled manipulation environment presented animal Extensive reviews results found Mitchell Movshon Hirsch Boothe Singer Some examples manipulation visual experience total pattern deprivation eg dark rearing selective deprivation certain class patterns eg vertical lines monocular deprivation animals binocular vision etc Extensive studies involving behavioral deficits resulting total visual pattern deprivation indicate deficits arise primarily result impairment visual information processing brain The results experiments suggest specific developmental learning mechanisms may operating various stages development different levels system We discuss hhhhhhhhhhhhhhh This working draft All comments especially constructive criticism suggestions improvement appreciated I indebted Prof James Dannemiller introducing literature infant development Prof Leonard Uhr helpful comments initial draft paper numerous researchers whose experimental work provided basis model outlined paper This research partially supported grants National Science Foundation University Wisconsin Graduate School',\n",
              " 'This paper studies problem ergodicity transition probability matrices Markovian models hidden Markov models HMMs makes difficult task learning represent longterm context sequential data This phenomenon hurts forward propagation longterm context information well learning hidden state representation represent longterm context depends propagating credit information backwards time Using results Markov chain theory show problem diffusion context credit reduced transition probabilities approach ie transition probability matrices sparse model essentially deterministic The results found paper apply learning approaches based continuous optimization gradient descent BaumWelch algorithm',\n",
              " 'Modern industry today needs flexible adaptive faulttolerant methods information processing Several applications shown neural networks fulfill requirements In paper application areas neural networks successfully used presented Then kind check list described mentioned different steps applying neural networks The paper finished discussion neural networks projects done research group Interactive Planning Research Center Computer Science FZI',\n",
              " 'Gas oil pipelines need inspected corrosion defects regular intervals For application Pipetronix GmbH PTX Karlsruhe developed special ultrasonic based probe Based recorded wall thicknesses called pipe pig Research center computer science FZI developed cooperation PTX automatic inspection system called NeuroPipe NeuroPipe task detect defects like metal loss The kernel inspection tool neural classifier trained using manually collected defect examples The following paper focus aspects successfull use learning methods industrial application',\n",
              " 'We construct mixture locally linear generative models collection pixelbased images digits use recognition Different models given digit used capture different styles writing new images classified evaluating loglikelihoods model We use EMbased algorithm Mstep computationally straightforward principal components analysis PCA Incorporating tangentplane information expected local deformations requires adding tangent vectors sample covariance matrices PCA demonstrably improves performance',\n",
              " 'In International Journal Neural Systems p URL paper ftpftpcscoloradoedupubTimeSeriesMyPapersexpertspsZ httpwwwcscoloradoeduandreasTimeSeriesMyPapersexpertspsZ University Colorado Computer Science Technical Report CUCS In analysis prediction realworld systems two key problems nonstationarityoften form switching regimes overfitting particularly serious noisy processes This article addresses problems using gated experts consisting nonlinear gating network several also nonlinear competing experts Each expert learns predict conditional mean expert adapts width match noise level regime The gating network learns predict probability expert given input This article focuses case gating network bases decision information inputs This contrasted hidden Markov models decision based previous states ie output gating network previous time step well averaging several predictors In contrast gated experts softpartition input space This article discusses underlying statistical assumptions derives weight update rules compares performance gated experts standard methods three time series computergenerated series obtained randomly switching two nonlinear processes time series Santa Fe Time Series Competition light intensity laser chaotic state daily electricity demand France realworld multivariate problem structure several time scales The main results gating network correctly discovers different regimes process widths associated expert important segmentation task used characterize subprocesses less overfitting compared single networks homogeneous multilayer perceptrons since experts learn match variances local noise levels This viewed matching local complexity model local complexity data',\n",
              " 'This paper describes approach modelling drug activity using machine learning tools Some experiments modelling quantitative structureactivity relationship QSAR using standard Hansch method machine learning system Golem already reported literature The paper describes results applying two machine learning systems Magnus Assistant Retis data The results achieved machine learning systems better results Hansch method therefore machine learning tools considered promising solving kind problems The given results also illustrate variations performance different machine learning systems applied drug design problem',\n",
              " 'In paper describe one aspect research project called HIPED addressed problem performing design engineering devices accessing heterogeneous databases The front end HIPED system consisted interactive KRITIK multimodal reasoning system combined case based model based reasoning solve design problem This paper focuses backend processing five types queries received front end evaluated mapping appropriately using facts schemas underlying databases rules establish correspondance among data databases terms relationships equivalence overlap set containment The uniqueness approach stems fact mapping process forgiving query received front end evaluated respect large number possibilities These possibilities encoded form rules consider various ways tokens given query may match relation names attrribute names values underlying tables The approach implemented using CORAL deductive database system rule processing engine',\n",
              " 'Temporal difference methods solve temporal credit assignment problem reinforcement learning An important subproblem general reinforcement learning learning achieve dynamic goals Although existing temporal difference methods Q learning applied problem take advantage special structure This paper presents DGlearning algorithm learns efficiently achieve dynamically changing goals exhibits good knowledge transfer goals In addition paper shows traditional relaxation techniques applied problem Finally experimental results given demonstrate superiority DG learning Q learning moderately large synthetic nondeterministic domain',\n",
              " 'In paper prove intractability learning several classes Boolean functions distributionfree model also called Probably Approximately Correct PAC model learning examples These results representation independent hold regardless syntactic form learner chooses represent hypotheses Our methods reduce problems cracking number wellknown publickey cryptosys tems learning problems We prove polynomialtime learning algorithm Boolean formulae deterministic finite automata constantdepth threshold circuits would dramatic consequences cryptography number theory particular algorithm could used break RSA cryptosystem factor Blum integers composite numbers equivalent modulo detect quadratic residues The results hold even learning algorithm required obtain slight advantage prediction random guessing The techniques used demonstrate interesting duality learning cryptography We also apply results obtain strong intractability results approximating gener alization graph coloring fl This research conducted author Harvard University supported AT T Bell Laboratories scholarship Supported grants ONRNK NSFDCR NSFCCR DAALK DARPA AFOSR SERC',\n",
              " 'We present new method determining consensus sequence DNA fragment assemblies The new method TraceEvidence directly incorporates aligned ABI trace information consensus calculations via previously described representation TraceData Classifications The new method extracts sums evidence indicated representation determine consensus calls Using TraceEvidence method results automatically produced consensus sequences accurate less ambiguous produced standard majority voting methods Additionally improvements achieved less coverage required standard methods using TraceEvidence coverage three error rates low coverage ten sequences',\n",
              " 'To learned morphology natural language capacity recognize produce words consisting novel combinations familiar morphemes Most recent work acquisition morphology takes perspective production receptive morphology comes first child This paper presents connectionist model acquisition capacity recognize morphologically complex words The model takes sequences phonetic segments inputs maps onto output units representing meanings lexical grammatical morphemes It consists simple recurrent network separate hiddenlayer modules tasks recognizing root grammatical morphemes input word Experiments artificial language stimuli demonstrate model generalizes novel words morphological rules one major types found natural languages version network unassigned hiddenlayer modules learn assign output recognition tasks efficient manner I also argue rules involving reduplication copying portions root network requires separate recurrent subnetworks sequences larger units syllables The network learn develop syllable representations support recognition reduplication also provide basis learning produce well recognize morphologically complex words The model makes many detailed predictions learning difficulty particular morphological rules',\n",
              " 'This paper presents algorithm combines traditional EBL techniques recent developments inductive logic programming learn effective clause selection rules Prolog programs When control rules incorporated original program significant speedup may achieved The algorithm shown improvement competing EBL approaches several domains Additionally algorithm capable automatically transforming intractable algorithms ones run polynomial time',\n",
              " 'Neurons ventral stream primate visual system exhibit responses images objects invariant respect natural transformations translation size view Anatomical neurophysiological evidence suggests achieved series hierarchical processing areas In attempt elucidate manner representations established constructed model cortical visual processing seeks parallel many features system specifically multistage hierarchy topologically constrained convergent connectivity Each stage constructed competitive network utilising modified Hebblike learning rule called trace rule incorporates previous well current neuronal activity The trace rule enables neurons learn whatever invariant short time periods eg representation objects objects transform real world The trace rule enables neurons learn statistical invariances objects transformations associating together representations occur close together time We show using trace rule training algorithm model indeed learn produce transformation invariant responses natural stimuli faces',\n",
              " 'In paper propose recurrent neural networks feedback input units handling two types data analysis problems On one hand scheme used static data input variables missing On hand also used sequential data input variables missing available different frequencies Unlike case probabilistic models eg Gaussian missing variables network attempt model distribution missing variables given observed variables Instead discriminant approach fills missing variables sole purpose minimizing learning criterion eg minimize output error',\n",
              " 'Many neural networks derived optimization dynamics suitable objective functions We show networks designed repeated transformations one objective another fixpoints We exhibit collection algebraic transformations reduce network cost increase set objective functions neurally implementable The transformations include simplification products expressions functions one two expressions sparse matrix products may interpreted Legendre transformations also minimum maximum set expressions These transformations introduce new interneurons force network seek saddle point rather minimum Other transformations allow control network dynamics reconciling Lagrangian formalism need fixpoints We apply transformations simplify number structured neural networks beginning standard reduction winnertakeall network ON connections ON Also susceptible inexact graphmatching random dot matching convolutions coordinate transformations sorting Simulations show fixpointpreserving transformations may applied repeatedly elaborately example networks still robustly converge',\n",
              " 'The use casebased reasoning process model design involves subtasks recalling previously known designs memory adapting design cases subcases fit current design context The development process model particular design domain proceeds parallel development representation cases case memory organisation design knowledge needed addition specific designs The selection particular representational paradigm types information details use particular problemsolving domain depend intended use information represented project information available well nature domain In paper describe development implementation four casebased design systems CASECAD CADSYN WIN DEMEX Each system described terms content organisation source case memory implementation case recall case adaptation A comparison systems considers relative advantages disadvantages implementations',\n",
              " 'We describe biologically plausible model dynamic recognition learning visual cortex based statistical theory Kalman filtering optimal control theory The model utilizes hierarchical network whose successive levels implement Kalman filters operating successively larger spatial temporal scales Each hierarchical level network predicts current visual recognition state lower level adapts recognition state using residual error prediction actual lowerlevel state Simultaneously network also learns internal model spatiotemporal dynamics input stream adapting synaptic weights hierarchical level order minimize prediction errors The Kalman filter model respects key neuroanatomical data reciprocity connections visual cortical areas assigns specific computational roles interlaminar connections known exist neurons visual cortex Previous work elucidated usefulness model explaining neurophysiological phenomena endstopping related extraclassical receptive field effects In paper addition providing detailed exposition model present variety experimental results demonstrating ability model perform robust spatiotemporal segmentation recognition objects image sequences presence varying amounts occlusion background clutter noise',\n",
              " 'Individual lifetime learning guide evolving population areas high fitness genotype space evolutionary phenomenon known Baldwin effect Baldwin Hinton Nowlan It accepted wisdom guiding speeds rate evolution By highlighting another interaction learning evolution termed Hiding effect argued depends measure evolutionary speed one adopts The Hiding effect shows learning reduce selection pressure individuals hiding genetic differences There thus tradeoff Baldwin effect Hiding effect determine learnings influence evolution two factors contribute tradeoff cost learning landscape epis tasis investigated experimentally',\n",
              " 'This paper focuses optimization hyperparameters function approximators We describe kind racing algorithm continuous optimization problems spends less time evaluating poor parameter settings time honing estimates promising regions parameter space The algorithm able automatically optimize parameters function approximator less computation time We demonstrate algorithm problem finding good parameters memory based learner show tradeoffs involved choosing right amount computation spend evaluation',\n",
              " 'Based analysis experiments using realworld datasets find greediness forward feature selection algorithms severely corrupt accuracy function approximation using selected input features improves efficiency significantly Hence propose three greedier algorithms order enhance efficiency feature selection processing We provide empirical results linear regression locally weighted regression knearestneighbor models We also propose use algorithms develop offline Chinese Japanese handwriting recognition system auto matically configured local models',\n",
              " 'This paper considers aspect mixture modelling Significantly overlapping distributions require data parameters accurately estimated well separated distributions For example two Gaussian distributions considered significantly overlap means within three standard deviations If insufficient data available single component distribution estimated although data originates two component distributions We consider much data required distinguish two component distributions one distribution mixture modelling using minimum message length MML criterion First perform experiments show MML criterion performs well relative Bayesian criteria Second make two improvements existing MML estimates improve performance overlapping distributions',\n",
              " 'In paper present performance prediction model indicating performance range MIMD parallel processor systems neural network simulations The model expresses total execution time simulation function execution times small number kernel functions measured one processor one physical communication link The functions depend type neural network geometry decomposition connection structure MIMD machine Using model execution time speedup scalability efficiency large MIMD systems predicted The model validated quantitatively applying two popular neural networks backpropagation Kohonen selforganizing feature map decomposed GCel transputer system Measurements taken network simulations decomposed via dataset network decomposition techniques Agreement model measurements within Estimates given performances expected new T transputer systems The presented method also used application areas image processing',\n",
              " 'With goal reducing computational costs without sacrificing accuracy describe two algorithms find sets prototypes nearest neighbor classification Here term prototypes refers reference instances used nearest neighbor computation instances respect similarity assessed order assign class new data item Both algorithms rely stochastic techniques search space sets prototypes simple implement The first Monte Carlo sampling algorithm second applies random mutation hill climbing On four datasets show three four prototypes sufficed give predictive accuracy equal superior basic nearest neighbor algorithm whose runtime storage costs approximately times greater We briefly investigate random mutation hill climbing may applied select features prototypes simultaneously Finally explain performance sampling algorithm datasets terms statistical measure extent clustering displayed target classes',\n",
              " 'We present new selforganizing neural network model two variants The first variant performs unsupervised learning used data visualization clustering vector quantization The main advantage existing approaches eg Kohonen feature map ability model automatically find suitable network structure size This achieved controlled growth process also includes occasional removal units The second variant model supervised learning method results combination abovementioned selforganizing network radial basis function RBF approach In model possible contrast earlier approaches toperform positioning RBF units supervised training weights parallel Therefore current classification error used determine insert new RBF units This leads small networks generalize well Results twospirals benchmark vowel classification problem presented better results previously published fl submitted publication',\n",
              " 'I present first results COLUMBUS autonomous mobile robot COLUMBUS operates initially unknown structured environments Its task explore model environment efficiently avoiding collisions obstacles COLUMBUS uses instancebased learning technique modeling environment Realworld experiences generalized via two artificial neural networks encode characteristics robots sensors well characteristics typical environments robot assumed face Once trained networks allow knowledge transfer across different environments robot face lifetime COLUMBUS models represent expected reward confidence expectations Exploration achieved navigating low confidence regions An efficient dynamic programming method employed background find minimalcost paths executed robot maximize exploration COLUMBUS operates realtime It operating successfully office building environment periods hours',\n",
              " 'We analyze performance Genetic Algorithm GA call Culling variety algorithms problem refer Additive Search Problem ASP ASP closely related several previously well studied problems game Mastermind additive fitness functions We show problem learning Ising perceptron reducible noisy version ASP Culling efficient ASP highly noise tolerant best known approach regimes Noisy ASP first problem aware Genetic Type Algorithm bests known competitors Standard GAs contrast perform much poorly ASP hillclimbing approaches even though Schema theorem holds ASP We generalize ASP kASP study whether GAs achieve implicit parallelism problem many schemata GAs fail achieve implicit parallelism describe algorithm call Explicitly Parallel Search succeeds We also compute optimal culling point selective breeding turns independent fitness function population distribution We also analyze Mean Field Theoretic algorithm performing similarly Culling many problems These results provide insight GAs beat competing methods',\n",
              " 'Many extensions proposed help instancebased learning algorithms perform better wide variety realworld applications However trivial decide parameters options use applying instancebased learning algorithm particular problem Traditionally crossvalidation used choose parameters k k nearest neighbor classifier This paper points cross validation often provide enough information allow finetuning classifier confidence levels used break ties common crossvalidation used It proposes Fuzzy Instance Based Learning FIBL algorithm uses distanceweighted voting parameters set via combination crossvalidation confidence levels In experiments datasets FIBL higher average generalization accuracy using majority voting using crossvalidation alone determine parameters',\n",
              " 'This paper describes formulation reinforcement learning enables learning noisy dynamic environemnts complex concurrent multirobot learning domain The methodology involves minimizing learning space use behaviors conditions dealing credit assignment problem shaped reinforcement form heterogeneous reinforcement functions progress estimators We experimentally validate ap proach group four mobile robots learning foraging task',\n",
              " 'Most existing decision tree systems use greedy approach induce trees locally optimal splits induced every node tree Although greedy approach suboptimal believed produce reasonably good trees In current work attempt verify belief We quantify goodness greedy tree induction empirically using popular decision tree algorithms C CART We induce decision trees thousands synthetic data sets compare corresponding optimal trees turn found using novel map coloring idea We measure effect greedy induction variables underlying concept complexity training set size noise dimensionality Our experiments show among things expected classification cost greedily induced tree consistently close optimal tree',\n",
              " 'Report SYCON ABSTRACT Previous results input state stabilizability shown hold even systems linear controls provided general type feedback allowed Applications certain stabilization problems coprime factorizations well comparisons results input state stability also briefly discussed',\n",
              " 'Attractor networks map continuous input space discrete output space useful pattern completion cleaning noisy missing features input However designing net given set attractors notoriously tricky training procedures CPU intensive often produce spurious attractors illconditioned attractor basins These difficulties occur connection network participates encoding multiple attractors We describe alternative formulation attractor networks encoding knowledge local distributed Although localist attractor nets similar dynamics distributed counterparts much easier work interpret We propose statistical formulation localist attractor net dynamics yields convergence proof mathematical interpretation model parameters We present simulation experiments explore behavior localist attractor nets showing produce gang effectthe presence attractor enhances attractor basins neighboring attractorsand spurious attractors occur points symmetry state space',\n",
              " 'According Wolperts nofreelunch NFL theorems generalisation absence domain knowledge necessarily zerosum enterprise Good generalisation performance one situation always offset bad performance another Wolpert notes theorems demonstrate effective generalisation logical impossibility merely learners bias assumption set key importance',\n",
              " 'Learning limited modification parameters limited scope capability modify system structure also needed get wider range learnable In case artificial neural networks learning iterative adjustment synaptic weights succeed network designer predefines appropriate network structure ie number hidden layers units size shape receptive projective fields This paper advocates view network structure usually done determined trialanderror computed learning algorithm Incremental learning algorithms modify network structure addition andor removal units andor links A survey current connectionist literature given line thought Grow Learn GAL new algorithm learns association oneshot due incremental using local representation During socalled sleep phase units previously stored longer necessary due recent modifications removed minimize network complexity The incrementally constructed network later finetuned offline improve performance Another method proposed greatly increases recognition accuracy train number networks vote responses The algorithm variants tested recognition handwritten numerals seem promising especially terms learning speed This makes algorithm attractive online learning tasks eg robotics The biological plausibility incremental learning also discussed briefly Earlier part work realized Laboratoire de Microinformatique Ecole Polytechnique Federale de Lausanne supported Fonds National Suisse de la Recherche Scientifique Later part realized supported International Computer Science Institute A number people helped guiding stimulating discussions questions Subutai Ahmad Peter Clarke Jerry Feldman Christian Jutten Pierre Marchal Jean Daniel Nicoud Steve Omohondro Leon Personnaz',\n",
              " 'This paper introduces probability model mixture trees account sparse dynamically changing dependence relationships We present family efficient algorithms use EM Minimum Spanning Tree algorithm find ML MAP mixture trees variety priors including Dirichlet MDL priors',\n",
              " 'Two fundamental problems analyzing DNA sequences locating regions DNA sequence encode proteins determining reading frame region We investigate using artificial neural networks ANNs find coding regions determine reading frames detect frameshift errors E coli DNA sequences We describe adaptation approach used Uberbacher Mural identify coding regions human DNA compare performance ANNs several conventional methods predicting reading frames Our experiments demonstrate ANNs outperform conventional approaches',\n",
              " 'The paper describes selflearning control system mobile robot Based sensor information control system provide steering signal way collisions avoided Since case examples available system learns basis external reinforcement signal negative case collision zero otherwise Rules Temporal Difference learning used find correct mapping discrete sensor input space steering signal We describe algorithm learning correct mapping input state vector output steering signal algorithm used discrete coding input state space',\n",
              " 'Work currently underway devise learning methods better able transfer knowledge one task another The process knowledge transfer usually viewed logically separate inductive procedures ordinary learning However paper argues seperatist view leads number conceptual difficulties It offers task analysis situates transfer process inside generalised inductive protocol It argues transfer viewed subprocess within induction independent procedure transporting knowledge learning trials',\n",
              " 'This chapter describes three studies address question neural network learning improved via incorporation information extracted networks This general problem call network transfer encompasses many types relationships source target networks Our focus utilization weights source networks solve subproblem target network task goal speeding learning target task We demonstrate approach described improve learning speed ten times learning starting random weights',\n",
              " 'ALVINN Autonomous Land Vehicle Neural Net Backpropagation trained neural network capable autonomously steering vehicle road highway environments Although ALVINN fairly robust one problems time takes train As vehicle capable online learning driver drive car minutes network capable autonomous operation One reason use Backprop In report describe original ALVINN system look three alternative training methods Quickprop Cascade Correlation Cascade We run series trials using Quickprop Cascade Correlation Cascade compare BackProp baseline Finally hidden unit analysis performed determine network learning Applying Advanced Learning Algorithms ALVINN',\n",
              " 'The work discussed paper motivated need building decision support systems realworld problem domains Our goal use systems tool supporting Bayes optimal decision making action maximizing expected utility respect predicted probabilities possible outcomes selected For reason models used need probabilistic nature output model probability distribution set numbers For model family chosen set simple discrete finite mixture models advantage computationally efficient In work describe Bayesian approach constructing finite mixture models sample data Our approach based twophase unsupervised learning process used exploratory analysis model construction In first phase selection model class ie number parameters performed calculating CheesemanStutz approximation model class evidence In second phase MAP parameters selected class estimated EM algorithm In framework overfitting problem common many traditional learning approaches avoided learning process automatically regulates complexity model This paper focuses model class selection phase approach validated presenting empirical results natural synthetic data',\n",
              " 'We introduce analyze new algorithm linear classification combines Rosenblatts perceptron algorithm Helmbold Warmuths leaveoneout method Like Vapniks maximalmargin classifier algorithm takes advantage data linearly separable large margins Compared Vapniks algorithm however much simpler implement much efficient terms computation time We also show algorithm efficiently used high dimensional spaces using kernel functions We performed experiments using algorithm variants classifying images handwritten digits The performance algorithm close good performance maximalmargin classifiers problem',\n",
              " 'A version paper appear ACM Transactions Computer Systems August Permission make digital copies part work personal classroom use grantedwithout fee provided copies made distributed profit commercial advantage copies bear notice full citation first page Copyrights components work owned others ACM must honored Abstracting credit permitted To copy otherwise republish post servers redistribute lists requires prior specific permission andor fee Abstract To achieve high performance contemporary computer systems rely two forms parallelism instructionlevel parallelism ILP threadlevel parallelism TLP Wideissue superscalar processors exploit ILP executing multiple instructions single program single cycle Multiprocessors MP exploit TLP executing different threads parallel different processors Unfortunately parallelprocessing styles statically partition processor resources thus preventing adapting dynamicallychanging levels ILP TLP program With insufficient TLP processors MP idle insufficient ILP multipleissue hardware superscalar wasted This paper explores parallel processing alternative architecture simultaneous multithreading SMT allows multiple threads compete share processors resources every cycle The compelling reason running parallel applications SMT processor ability use threadlevel parallelism instructionlevel parallelism interchangeably By permitting multiple threads share processors functional units simultaneously processor use ILP TLP accommodate variations parallelism When program single thread SMT processors resources dedicated thread TLP exists parallelism compensate lack',\n",
              " 'In paper present framework building probabilistic automata parameterized contextdependent probabilities Gibbs distributions used model state transitions output generation parameter estimation carried using EM algorithm Mstep uses generalized iterative scaling procedure We discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology',\n",
              " 'In regression context boosting bagging techniques build committee regressors may superior single regressor We use regression trees fundamental building blocks bagging committee machines boosting committee machines Performance analyzed three nonlinear functions Boston housing database In cases boosting least equivalent cases better bagging terms prediction error',\n",
              " 'Current expert systems properly handle imprecise incomplete information On hand neural networks perform pattern recognition operations even noisy environments Against background implemented neural expert system shell NEULA whose computational mechanism processes imprecisely incompletely given information means approximate probabilistic reasoning',\n",
              " 'Coevolution give rise Red Queen effect interacting populations alter others fitness landscapes The Red Queen effect significantly complicates measurement coevolutionary progress introducing fitness ambiguities improvements performance coevolved individuals appear decline stasis usual measures evolutionary progress Unfortunately appropriate measures fitness given Red Queen effect developed artificial life theoretical biology population dynamics evolutionary genetics We propose set appropriate performance measures based genetic behavioral data illustrate use simulation coevolution genetically specified continuoustime noisy recurrent neural networks generate pursuit evasion behaviors autonomous agents',\n",
              " 'Inferences measurement error models sensitive modeling assumptions Specifically model incorrect estimates inconsistent To reduce sensitivity modeling assumptions yet still retain efficiency parametric inference propose use flexible parametric models accommodate departures standard parametric models We use mixtures normals purpose We study two cases detail linear errorsinvariables model changepoint Berkson model fl Raymond J Carroll Professor Statistics Nutrition Toxicology Department Statistics Texas AM University College Station TX Kathryn Roeder Associate Professor Larry Wasserman Professor Department Statistics CarnegieMellon University Pittsburgh PA Carrolls research supported grant National Cancer Institute CA Roeders research supported NSF grant DMS Wassermans research supported NIH grant ROCA NSF grants DMS DMS',\n",
              " 'In paper investigate phenomenon multiparent reproduction ie study recombination mechanisms arbitrary n gt number parents participate creating children In particular discuss scanning crossover generalizes standard uniform crossover diagonal crossover generalizes point crossover study effects different number parents GA behavior We conduct experiments tough function optimization problems observe multiparent operators performance GAs enhanced significantly We also give theoretical foundation showing operators work distributions',\n",
              " 'TECHNICAL REPORT No Department Statistics GN University Washington Seattle Washington USA Susan L Rosenkranz Pew Health Policy Postdoctoral Fellow Institute Health Policy Studies Box University California San Francisco San Francisco CA Adrian E Raftery Professor Statistics Sociology Department Statistics GN University Washington Seattle WA Rosenkranzs research supported National Research Service Award TCA National Cancer Institute The authors grateful Paula Diehr Kevin Cain helpful discussions',\n",
              " 'Draft A Brief Introduction Neural Networks Richard D De Veaux Lyle H Ungar Williams College University Pennsylvania Abstract Artificial neural networks used increasing frequency high dimensional problems regression classification This article provides tutorial overview neural networks focusing back propagation networks method approximating nonlinear multivariable functions We explain statisticians vantage point neural networks might attractive compare modern regression techniques KEYWORDS nonparametric regression function approximation backpropagation Introduction Networks mimic way brain works computer programs actually LEARN patterns forecasting without know statistics These many claims attractions artificial neural networks Neural networks henceforth drop term artificial unless need distinguish biological neural networks seem everywhere days least advertising able statistics without fuss bother anything except buy piece software Neural networks successfully used many different applications including robotics chemical process control speech recognition optical character recognition credit card fraud detection interpretation chemical spectra vision autonomous navigation vehicles Pointers literature given end article In article attempt explain one particular type neural network feedforward networks sigmoidal activation functions backpropagation networks actually works trained compares well known statistical techniques As example someone would want use neural network consider problem recognizing hand written ZIP codes letters This classification problem',\n",
              " 'To apply algorithm classification assign class separate set codebook Gaussians Each set trained patterns single class After trained codebook Gaussians set provides estimate probability function one class Parzen window estimation take estimate pattern distribution average Gaussians set Classification pattern may done calculating probability class respective sample point assigning pattern class highest probability Hence whole codebook plays role classification patterns This case regular classification schemes using codebooks We tested classification scheme several classification tasks including two spiral problem We compared algorithm various classification algorithms came second best algorithm applications Parzen window estimation However computing time memory Parzen window estimation excessive compared algorithm hence practical situations algorithm preferred We developed fast algorithm combines attractive properties Parzen window estimation vector quantization The scale parameter tuned adaptively therefore set ad hoc manner It allows classification strategy codebook vectors taken account This yields better results standard vector quantization techniques An interesting topic research use radially nonsymmetric Gaussians',\n",
              " 'Predictions lifetimes dynamically allocated objects used improve time space efficiency dynamic memory management computer programs Barrett Zorn used simple lifetime predictor demonstrated improvement variety computer programs In paper use decision trees lifetime prediction programs show significantly better prediction Our method also advantage training use large number features let decision tree automatically choose relevant subset',\n",
              " 'Evolutionary systems used variety applications turbine design scheduling problems The basic algorithms similar applications representation always problem specific Unfortunately search time evolutionary systems much depends efficient codings using problem specific domain knowledge reduce size search space This paper describes approach user specifies general basic coding used larger variety problems The system learns efficient problem specific coding To evolutionary system variable length coding used While system optimizes example problem meta process identifies successful combinations genes population combines higher level evolved genes The extraction repeated iteratively allowing genes evolve high level complexity encode high number original basic genes This results continuous restructuring search space allowing potentially successful solutions found much shorter search time The evolved coding used solve related problems While excluding potentially desirable solutions evolved coding makes knowledge example problem available new problem',\n",
              " 'The coverage learning algorithm number concepts learned algorithm samples given size This paper asks whether good learning algorithms designed maximizing coverage The paper extends previous upper bound coverage Boolean concept learning algorithm describes two algorithmsMultiBalls LargeBallwhose coverage approaches upper bound Experimental measurement coverage ID FRINGE algorithms shows coverage far bound Further analysis LargeBall shows although learns many concepts seem interesting concepts Hence coverage maximization alone appear yield practicallyuseful learning algorithms The paper concludes definition coverage within bias suggests way coverage maximization could applied strengthen weak preference biases',\n",
              " 'Markov decision processes MDPs recently applied problem modeling decisiontheoretic planning While traditional methods solving MDPs often practical small states spaces effectiveness large AI planning problems questionable We present algorithm called structured policy iteration SPI constructs optimal policies without explicit enumeration state space The algorithm retains fundamental computational steps commonly used modified policy iteration algorithm exploits variable propositional independencies reflected temporal Bayesian network representation MDPs The principles behind SPI applied structured representation stochastic actions policies value functions algorithm used conjunction cent approximation methods',\n",
              " 'This paper reviews features new class multilayer connectionist architectures known ASOCS Adaptive SelfOrganizing Concurrent Systems ASOCS similar decisionmaking neural network models attempts learn adaptive set arbitrary vector mappings However differs dramatically mechanisms ASOCS based networks adaptive digital elements selfmodify using local information Function specification entered incrementally use rules rather complete inputoutput vectors processing network able extract critical features large environment give output parallel fashion Learning also uses parallelism selforganization new rule completely learned time linear depth network The model guarantees learning arbitrary mapping boolean inputoutput vectors The model also stable learning erase previously learned mappings except explicitly contradicted',\n",
              " 'Maximum working likelihood MWL inference presence missing data quite challenging intractability associated marginal likelihood This problem exacerbated number parameters involved large We propose using Markov chain Monte Carlo MCMC first obtain MWL estimator working Fisher information matrix second using Monte Carlo quadrature obtain remaining components correct asymptotic MWL variance Evaluation marginal likelihood needed We demonstrate consistency asymptotic normality number independent identically distributed data clusters large likelihood may incorrectly specified An analysis longitudinal ordinal data given example KEY WORDS Convergence posterior distributions Maximum likelihood Metropolis',\n",
              " 'Natural images contain characteristic statistical regularities set apart purely random images Understanding regularities enable natural images coded efficiently In paper describe forms structure contained natural images show related response properties neurons early stages visual system Many important forms structure require higherorder ie linear pairwise statistics characterize makes models based linear Hebbian learning principal components analysis inappropriate finding efficient codes natural images We suggest good objective efficient coding natural scenes maximize sparseness representation show network learns sparse codes natural scenes succeeds developing localized oriented bandpass receptive fields similar primate striate cortex',\n",
              " 'In paper develop empirical methodology studying behavior evolutionary algorithms based problem generators We describe three generators used study effects epistasis performance EAs Finally illustrate use ideas preliminary exploration effects epistasis simple GAs',\n",
              " 'Traditionally genetic algorithms relied upon point crossover operators Many recent empirical studies however shown benefits higher numbers crossover points Some intriguing recent work focused uniform crossover involves average L crossover points strings length L Theoretical results suggest view hyperplane sampling disruption uniform crossover redeeming features However growing body experimental evidence suggests otherwise In paper attempt reconcile opposing views uniform crossover present framework understanding virtues',\n",
              " 'Conditional logics introduced Lewis Stalnaker utilized artificial intelligence capture broad range phenomena In paper examine complexity several variants discussed literature We show general deciding satisfiability PSPACEcomplete formulas arbitrary conditional nesting NPcomplete formulas bounded nesting conditionals However provide several exceptions rule Of particular note results showing assuming uniformity ie worlds agree worlds possible decision problem becomes EXPTIMEcomplete even formulas bounded nesting b assuming absoluteness ie worlds agree conditional statements decision problem NPcomplete mulas arbitrary nesting',\n",
              " 'An incremental higherorder nonrecurrent network combines two properties found useful learning sequential tasks higherorder connections incremental introduction new units The network adds higher orders needed adding new units dynamically modify connection weights Since new units modify weights next timestep information previous step temporal tasks learned without use feedback thereby greatly simplifying training Furthermore theoretically unlimited number units added reach arbitrarily distant past Experiments Reber grammar demonstrated speedups two orders magnitude recurrent networks',\n",
              " 'I propose novel general principle unsupervised learning distributed nonredundant internal representations input patterns The principle based two opposing forces For representational unit adaptive predictor tries predict unit remaining units In turn unit tries react environment minimizes predictability This encourages unit filter abstract concepts environmental input concepts statistically independent upon units focus I discuss various simple yet potentially powerful implementations principle aim finding binary factorial codes Barlow et al ie codes probability occurrence particular input simply product probabilities corresponding code symbols Such codes potentially relevant segmentation tasks speeding supervised learning novelty detection Methods finding factorial codes automatically implement Occams razor finding codes using minimal number units Unlike previous methods novel principle potential removing linear also nonlinear output redundancy Illustrative experiments show algorithms based principle predictability minimization practically feasible The final part paper describes entirely local algorithm potential learning unique representations extended input sequences',\n",
              " 'In paper study learning PAC model Valiant example oracle used learning may faulty one two ways either misclassifying example distorting distribution examples We first consider models examples misclassified Kearns recently showed efficient learning new model using statistical queries sufficient condition PAC learning classification noise We show efficient learning statistical queries sufficient learning PAC model malicious error rate proportional required statistical query accuracy One application result new lower bound tolerable malicious error learning monomials k literals This first bound independent number irrelevant attributes n We also use statistical query model give sufficient conditions using distribution specific algorithms distributions outside prescribed domains A corollary result expands class distributions weakly learn monotone Boolean formulae We also consider new models learning examples chosen according distribution learner tested We examine three variations distribution noise give necessary sufficient conditions polynomial time learning noise We show containments separations various models faulty oracles Finally examine hypothesis boosting algorithms context learning distribution noise show Schapires result regarding strength weak learnability sense tight requiring weak learner nearly distribution free',\n",
              " 'This paper describes first stage study evolution learning abilities We use simple maze exploration problem designed R Sutton task individual encode inherent learning parameters genome The learning architecture use one step Qlearning using lookup table inherent parameters initial Qvalues learning rate discount rate rewards exploration rate Under fitness measure proportioning number times achieves goal later half life learners evolve genetic algorithm The results computer simulation indicated learning ability emerge environment changes every generation inherent map optimal path acquired environment doesnt change These results suggest emergence learning ability needs environmental change faster alternate generation',\n",
              " 'We examine problem performing exact dynamicprogramming updates partially observable Markov decision processes pomdps computational complexity viewpoint Dynamicprogramming updates crucial operation wide range pomdp solution methods find intractable perform updates piecewiselinear convex value functions general pomdps We offer new algorithm called witness algorithm compute updated value functions efficiently restricted class pomdps number linear facets great We compare witness algorithm existing algorithms analytically empirically find fastest algorithm wide range pomdp sizes',\n",
              " 'This paper examines limits instruction level parallelism found programs particular SPEC benchmark suite Apart using recent version SPEC benchmark suite differs earlier studies removing nonessential true dependencies occur result compiler employing stack subroutine linkage This subtle limitation parallelism readily evident appears true dependency stack pointer Other methods used employ stack remove dependency In paper show removal exposes far parallelism seen previously We refer type parallelism parallelism distance requires impossibly large instruction windows detection We conclude two observations single instruction window characteristic superscalar machines inadequate detecting parallelism distance order take advantage parallelism compiler must involved separate threads must explicitly programmed',\n",
              " 'In paper present framework building probabilistic automata parameterized contextdependent probabilities Gibbs distributions used model state transitions output generation parameter estimation carried using EM algorithm Mstep uses generalized iterative scaling procedure We discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology',\n",
              " 'Models unsupervised correlationbased Hebbian synaptic plasticity typically unstable either synapses grow reaches maximum allowed strength synapses decay zero strength A common method avoiding outcomes use constraint conserves limits total synaptic strength cell We study dynamical effects constraints Two methods enforcing constraint distinguished multiplicative subtractive For otherwise linear learning rules multiplicative enforcement constraint results dynamics converge principal eigenvector operator determining unconstrained synaptic development Subtractive enforcement contrast typically leads final state almost synaptic strengths reach either maximum minimum allowed value This final state often dominated weight configurations principal eigenvector unconstrained operator Multiplicative enforcement yields graded receptive field mutually correlated inputs represented whereas subtractive enforcement yields receptive field sharpened subset maximallycorrelated inputs If two equivalent input populations eg two eyes innervate common target multiplicative enforcement prevents segregation ocular dominance segregation two populations weakly correlated whereas subtractive enforcement allows segregation circumstances These results may used understand constraints output cells input cells A variety rules implement constrained dynamics discussed',\n",
              " 'This project supported part grant McDonnellPew Foundation grant ATR Human Information Processing Research Laboratories grant Siemens Corporation grant NJ Office Naval Research The project also supported NSF grant ASC support Center Biological Computational Learning MIT including funds provided DARPA HPCC program Michael I Jordan NSF Presidential Young Investigator',\n",
              " 'Learning made efficient actively select particularly salient data points Within Bayesian learning framework objective functions discussed measure expected informativeness candidate measurements Three alternative specifications want gain information lead three different criteria data selection All criteria depend assumption hypothesis space correct may prove main weakness',\n",
              " 'Knowledge clusters relations important understanding highdimensional input data unknown distribution Ordinary feature maps fully connected fixed grid topology properly reflect structure clusters input spacethere cluster boundaries map Incremental feature map algorithms nodes connections added deleted map according input distribution overcome problem However far algorithms limited maps drawn D case dimensional input space In approach proposed paper nodes added incrementally regular dimensional grid drawable times irrespective dimensionality input space The process results map explicitly represents cluster structure highdimensional input',\n",
              " 'Lattice conditional independence LCI models multivariate normal data recently introduced analysis nonmonotone missing data patterns nonnested dependent linear regression models seemingly unrelated regressions It shown class LCI models coincides subclass class graphical Markov models determined acyclic digraphs ADGs namely subclass transitive ADG models An explicit graph theoretic characterization ADGs Markov equivalent transitive ADG obtained This characterization allows one determine whether specific ADG D Markov equivalent transitive ADG hence LCI model polynomial time without exhaustive search exponentially large equivalence class D These results require existence positivity joint densities',\n",
              " 'In paper describe method improving geneticalgorithmbased optimization using casebased learning The idea utilize sequence points explored search guide exploration The proposed method particularly suitable continuous spaces expensive evaluation functions arise engineering design Empirical results two engineering design domains across different representations demonstrate proposed method significantly improve efficiency reliability GA optimizer Moreover results suggest modification makes genetic algorithm less sensitive poor choices tuning parameters muta tion rate',\n",
              " 'Genetic algorithms GAs extensively used means performing global optimization simple yet reliable manner However realistic engineering design optimization domains simple classical implementation GA based binary encoding bit mutation crossover often inefficient unable reach global optimum In paper describe GA continuous designspace optimization uses new GA operators strategies tailored structure properties engineering design domains Empirical results domains supersonic transport aircraft supersonic missile inlets demonstrate newly formulated GA significantly better classical GA efficiency reliability',\n",
              " 'D E Rumelhart G E Hinton R J Williams Learning Internal Representations Error Propagation D E Rumelhart J L McClelland eds Parallel Distributed Processing Explorations Microstructure Cognition Vol MIT Press',\n",
              " 'Evolutionary trees frequently used underlying model design algorithms optimization criteria software packages multiple sequence alignment MSA In paper reexamine suitability trees universal model MSA light broad range biological questions MSAs used address A tree model consists tree topology model accepted mutations along branches After surveying major applications MSA examples molecular biology literature used illustrate situations tree model fails This occurs relationship residues column described tree example structural functional applications MSA It also occurs situations lateral gene transfer entire gene modeled unique tree In cases nonparsimonous data convergent evolution may difficult find consistent mutational model We hope survey promote dialogue biologists computer scientists leading biologically realistic research MSA',\n",
              " 'Selective suppression transmission feedback synapses learning proposed mechanism combining associative feedback selforganization feedforward synapses Experimental data demonstrates cholinergic suppression synaptic transmission layer I feedback synapses lack suppression layer IV feedforward synapses A network feature uses local rules learn mappings linearly separable During learning sensory stimuli desired response simultaneously presented input Feedforward connections form selforganized representations input suppressed feedback connections learn transpose feedforward connectivity During recall suppression removed sensory input activates selforganized representation activity generates learned response',\n",
              " 'Technical Report No Department Statistics University Toronto Abstract One way sample distribution sample uniformly region plot density function A Markov chain converges uniform distribution constructed alternating uniform sampling vertical direction uniform sampling horizontal slice defined current vertical position Variations slice sampling methods easily implemented univariate distributions used sample multivariate distribution updating variable turn This approach often easier implement Gibbs sampling may efficient easilyconstructed versions Metropolis algorithm Slice sampling therefore attractive routine Markov chain Monte Carlo applications use software automatically generates Markov chain sampler model specification One also easily devise overrelaxed versions slice sampling sometimes greatly improve sampling efficiency suppressing random walk behaviour Random walks also avoided slice sampling schemes simultaneously update variables',\n",
              " 'Markov decision problems MDPs provide foundations number problems interest AI researchers studying automated planning reinforcement learning In paper summarize results regarding complexity solving MDPs running time MDP solution algorithms We argue although MDPs solved efficiently theory study needed reveal practical algorithms solving large problems quickly To encourage future research sketch alternative methods analysis rely struc ture MDPs',\n",
              " 'Learning reinforcements promising approach creating intelligent agents However reinforcement learning usually requires large number training episodes We present evaluate design addresses shortcoming allowing connectionist Qlearner accept advice given time natural manner external observer In approach advicegiver watches learner occasionally makes suggestions expressed instructions simple imperative programming language Based techniques knowledgebased neural networks insert programs directly agents utility function Subsequent reinforcement learning integrates refines advice We present empirical evidence investigates several aspects approach show given good advice learner achieve statistically significant gains expected reward A second experiment shows advice improves expected reward regardless stage training given another study demonstrates subsequent advice result gains reward Finally present experimental results indicate method powerful naive technique making use advice',\n",
              " 'This paper presents mathematical foundations Dirichlet mixtures used improve database search results homologous sequences variable number sequences protein family domain known We present method condensing information protein database mixture Dirichlet densities These mixtures designed combined observed amino acid frequencies form estimates expected amino acid probabilities position profile hidden Markov model statistical model These estimates give statistical model greater generalization capacity remotely related family members reliably recognized model Dirichlet mixtures shown outperform substitution matrices methods computing expected amino acid distributions database search resulting fewer false positives false negatives families tested This paper corrects previously published formula estimating expected probabilities contains complete derivations Dirichlet mixture formulas methods optimizing mixtures match particular databases suggestions efficient implementation',\n",
              " 'Derivational analogy technique reusing problem solving experience improve problem solving performance This research addresses issue common problem solvers use derivational analogy overcoming mismatches past experiences new problems impede reuse First research describes variety mismatches arise proposes new approach derivational analogy uses appropriate adaptation strategies Second compares approach seven others common domain This empirical study shows derivational analogy almost always efficient problem solving scratch amount contributes depends ability overcome mismatches',\n",
              " 'Pollack demonstrated secondorder recurrent neural networks act dynamical recognizers formal languages trained positive negative examples observed phase transitions learning IFSlike fractal state sets Followon work focused mainly extraction minimization finite state automaton FSA trained network However networks capable inducing languages regular therefore equivalent FSA Indeed may simpler small network fit training data inducing nonregular language But networks language regular In paper using low dimensional network capable learning Tomita data sets present empirical method testing whether language induced network regular We also provide detailed machine analysis trained networks regular nonregular languages',\n",
              " 'COINS Technical Report January Abstract This article presents algorithm inducing multiclass decision trees multivariate tests internal decision nodes Each test constructed training linear machine eliminating variables controlled manner Empirical results demonstrate algorithm builds small accurate trees across variety tasks',\n",
              " 'Funes P Pollack J Computer Evolution Buildable Objects Fourth European Conference Artificial Life P Husbands I Harvey eds MIT Press pp knowledge program would result familiar structures provided algorithm model physical reality purely utilitarian fitness function thus supplying measures feasibility functionality In way evolutionary process runs environment unnecessarily constrained We added however requirement computability reject overly complex structures took long simulations evaluate The results encouraging The evolved structures surprisingly alien look based common knowledge build brick toys instead computer found ways evolutionary search process We able assemble final designs manually confirm accomplish objectives introduced fitness functions After background related problems describe physical simulation model twodimensional Lego structures representation encoding applying evolution We demonstrate feasibility work photos actual objects result particular optimizations Finally discuss future work draw conclusions In order evolve morphology behavior autonomous mechanical devices manufactured one must simulator operates several constraints resultant controller adaptive enough cover gap simulated real world eral space mechanisms Conservative simulation never perfect preserve margin safety Efficient quicker test simulation physical production test Buildable results convertible simula tion real object Computer Evolution Buildable Objects Abstract The idea coevolution bodies brains becoming popular little work done evolution physical structure lack general framework Evolution creatures simulation constrained reality gap implies resultant objects usually buildable The work present takes step problem body evolution applying evolutionary techniques design structures assembled parts Evolution takes place simulator designed computes forces stresses predicts failure dimensional Lego structures The final printout program schematic assembly built physically We demonstrate functionality several different evolved entities',\n",
              " 'In paper concerned problem acquiring knowledge integration Our aim construct integrated knowledge base several separate sources The need merge knowledge bases arise example knowledge bases acquired independently interactions several domain experts As opinions different domain experts may differ knowledge bases constructed way normally differ A similar problem also arise whenever separate knowledge bases generated learning algorithms The objective integration construct one system exploits knowledge available good performance The aim paper discuss methodology knowledge integration describe implemented system INTEG present concrete results demonstrate advantages method',\n",
              " 'Many arthropods particularly insects exhibit sophisticated visually guided behaviours Yet cases behaviours guided input hundreds thousands pixels ie ommatidia compound eye Inspired observation several years exploring possibilities visually guided robots lowbandwidth vision Rather design robot controllers hand use artificial evolution form extended genetic algorithm automatically generate architectures artificial neural networks generate effective sensorymotor coordination controlling mobile robots Analytic techniques drawn neuroethology dynamical systems theory allow us understand evolved robot controllers function predict behaviour environments used evolutionary process Initial experiments performed simulation techniques successfully transferred work variety real physical robot platforms This chapter reviews past work concentrating analysis evolved controllers gives overview current research We conclude discussion application evolutionary techniques problems biological vision',\n",
              " 'The major implementational problem reversible jump MCMC commonly natural way choose jump proposals since Euclidean structure guide choice In paper consider mechanism guiding proposal choice analysis acceptance probabilities jumps Essentially method involves approximation acceptance probability around certain canonical jumps We illustrate procedure using example reversible jump MCMC application involving Bayesian analysis graphical gaussian models',\n",
              " 'Instancebased learning methods explicitly remember data receive They usually training phase prediction time perform computation Then take query search database similar datapoints build online local model local average local regression predict output value In paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large We present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning Earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure Our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously This permits us query database exibility conventional linear search greatly reduced computational cost',\n",
              " 'In standard online model learning algorithm tries minimize total number mistakes made series trials On trial learner sees instance either accepts rejects instance told appropriate response We define natural variant model apple tasting learner gets feedback instance accepted We use two transformations relate apple tasting model enhanced standard model false acceptances counted separately false rejections We present strategy trading false acceptances false rejections standard model From one perspective strategy exactly optimal including constants We apply results obtain good general purpose apple tasting algorithm well nearly optimal apple tasting algorithms variety standard classes conjunctions disjunctions n boolean variables We also present analyze simpler transformation useful instances drawn random rather selected adversary',\n",
              " 'In paper describe algorithm exploits error distribution generated learning algorithm order break domain approximated piecewise learnable partitions Traditionally error distribution neglected favor lump error measure RMS By however lose lot important information The error distribution tells us algorithm badly exists ridge errors also tells us partition space one part space interfere learning another The algorithm builds variable arity kd tree whose leaves contain partitions Using tree new points predicted using correct partition traversing tree We instantiate algorithm using memory based learners crossvalidation',\n",
              " 'PREENS Parallel Research Execution Environment Neural Systems distributed neurosimulator targeted networks workstations transputer systems As current applications neural networks often contain large amounts data neural networks involved tasks vision large high requirements memory computational resources imposed target execution platforms PREENS executed distributed environment ie tools neural network simulation programs running machine connectable via TCPIP Using approach larger tasks data examined using efficient coarse grained parallelism Furthermore design PREENS allows neural networks running high performance MIMD machine transputer system In paper different features design concepts PREENS discussed These also used applications like image processing',\n",
              " 'It well known standard learning classifier systems applied many different domains exhibit number problems payoff oscillation difficult regulate interplay reward system background genetic algorithm GA rule chains instability default hierarchies instability ALECSYS parallel version standard learning classifier system CS suffers problems In paper propose innovative solutions problems We introduce following original features Mutespec new genetic operator used specialize potentially useful classifiers Energy quantity introduced measure global convergence order apply genetic algorithm system close steady state Dynamical adjustment classifiers set cardinality order speed performance phase algorithm We present simulation results experiments run simulated twodimensional world simple agent learns follow light source',\n",
              " 'Supervised neural networks generalize well much less information weights output vectors training cases So learning important keep weights simple penalizing amount information contain The amount information weight controlled adding Gaussian noise noise level adapted learning optimize tradeoff expected squared error network amount information weights We describe method computing derivatives expected squared error amount information noisy weights network contains layer nonlinear hidden units Provided output units linear exact derivatives computed efficiently without timeconsuming Monte Carlo simulations The idea minimizing amount information required communicate weights neural network leads number interesting schemes encoding weights',\n",
              " 'There many applications desirable order rather classify instances Here consider problem learning order given feedback form preference judgments ie statements effect one instance ranked ahead another We outline twostage approach one first learns conventional means preference function form PREFu v indicates whether advisable rank u v New instances ordered maximize agreements learned preference function We show problem finding ordering agrees best preference function NPcomplete even restrictive assumptions Nevertheless describe simple greedy algorithm guaranteed find good approximation We discuss online learning algorithm based Hedge algorithm finding good linear combination ranking experts We use ordering algorithm combined online learning algorithm find combination search experts domainspecific query expansion strategy WWW search engine present experimental results demonstrate merits approach',\n",
              " 'Technical Report CSRP March Abstract Evolutionary algorithms powerful techniques optimisation whose operation principles inspired natural selection genetics In paper discuss relation evolutionary techniques numerical classical search methods show methods instances single general search strategy call evolutionary computation cookbook By combining features classical evolutionary methods different ways new instances general strategy generated ie new evolutionary classical algorithms designed One algorithm GA fl described',\n",
              " 'We present neural net architecture discover hierarchical recursive structure symbol strings To detect structure multiple levels architecture capability reducing symbols substrings single symbols makes use external stack memory In terms formal languages architecture learn parse strings LR contextfree grammar Given training sets positive negative exemplars architecture trained recognize many different grammars The architecture one layer modifiable weights allowing Many cognitive domains involve complex sequences contain hierarchical recursive structure eg music natural language parsing event perception To illustrate spider ate hairy fly noun phrase containing embedded noun phrase hairy fly Understanding multilevel structures requires forming reduced descriptions Hinton string symbols states hairy fly reduced single symbolic entity noun phrase We present neural net architecture learns encode structure symbol strings via reduction transformations The difficult problem extracting multilevel structure complex extended sequences studied Mozer Ring Rohwer Schmidhuber among others While previous efforts made straightforward interpretation behavior',\n",
              " 'Selforganizing feature maps usually implemented abstracting lowlevel neural parallel distributed processes An external supervisor finds unit whose weight vector closest Euclidian distance input vector determines neighborhood weight adaptation The weights changed proportional Euclidian distance In biologically plausible implementation similarity measured scalar product neighborhood selected lateral inhibition weights changed redistributing synaptic resources The resulting selforganizing process quite similar abstract case However process somewhat hampered boundary effects parameters need carefully evolved It also necessary add redundant dimension input vectors',\n",
              " 'The application decision making learning algorithms multiagent systems presents many interestingresearch challenges opportunities Among ability agents learn act observing imitating agents We describe algorithm IQalgorithm integrates imitation Qlearning Roughly Qlearner uses observations made expert agent bias exploration promising directions This algorithm goes beyond previous work direction relaxing oftmade assumptions learner observer expert observed agent share objectives abilities Our preliminary experiments demonstrate significant transfer agents using IQmodel many cases reductions training time',\n",
              " 'Faces represent complex multidimensional meaningful visual stimuli developing computational model face recognition difficult We present hybrid neural network solution compares favorably methods The system combines local image sampling selforganizing map neural network convolutional neural network The selforganizing map provides quantization image samples topological space inputs nearby original space also nearby output space thereby providing dimensionality reduction invariance minor changes image sample convolutional neural network provides partial invariance translation rotation scale deformation The convolutional network extracts successively larger features hierarchical set layers We present results using KarhunenLoeve transform place selforganizing map multilayer perceptron place convolutional network The KarhunenLoeve transform performs almost well error versus The multilayer perceptron performs poorly error versus The method capable rapid classification requires fast approximate normalization preprocessing consistently exhibits better classification performance eigenfaces approach database considered number images per person training database varied With images per person proposed method eigenfaces result error respectively The recognizer provides measure confidence output classification error approaches zero rejecting examples We use database images individuals contains quite high degree variability expression pose facial details We analyze computational complexity discuss new classes could added trained recognizer',\n",
              " 'We present new algorithm solving Markov decision problems extends modified policy iteration algorithm Puterman Shin two important ways The new algorithm asynchronous allows values states updated arbitrary order need consider actions state updating policy The new algorithm converges general initial conditions required modified policy iteration Specifically set initial policyvalue function pairs algorithm guarantees convergence strict superset set modified policy iteration converges This generalization obtained making simple easily implementable change policy evaluation operator used updating value function Both asynchronous nature algorithm convergence general conditions expand range problems algorithm applied',\n",
              " 'Recently Markov chain Monte Carlo MCMC sampling methods become widely used determining properties posterior distribution Alternative Gibbs sampler elaborate HitandRun sampler generalization blackbox sampling scheme generate timereversible Markov chain posterior distribution The proof convergence applications Bayesian computation constrained parameter spaces provided comparisons MCMC samplers made In addition propose importance weighted marginal density estimation IWMDE method An IWMDE obtained averaging many dependent observations ratio full joint posterior densities multiplied weighting conditional density w The asymptotic properties IWMDE guidelines choosing weighting conditional density w also considered The generalized version IWMDE estimating marginal posterior densities full joint posterior density contains analytically intractable normalizing constants developed Furthermore develop Monte Carlo methods based KullbackLeibler divergences comparing marginal posterior density estimators This article summary authors PhD thesis presented Savage Award session',\n",
              " 'In paper characterize complexity noisetolerant learning PAC model Specifically show general lower bound logffi number examples required PAC learning presence classification noise Combined result Simon effectively show sample complexity PAC learning presence classification noise VCF Furthermore demonstrate optimality general lower bound providing noisetolerant learning algorithm class symmetric Boolean functions uses sample size within constant factor bound Finally note general lower bound compares favorably various general upper bounds PAC learning presence classification noise',\n",
              " 'It recently realized parasite virulence harm caused parasites hosts adaptive trait Selection particular level virulence happen either level betweenhost tradeoffs result shortsighted withinhost competition This paper describes simulations study effect modifier genes changes mutation rate suppressing shortsighted development virulence investigates interaction simplified model im mune clearance',\n",
              " 'Much work qualitative physics involves constructing models physical systems using functional descriptions flow monotonically increases pressure Semiquantitative methods improve model precision adding numerical envelopes monotonic functions Ad hoc methods normally used determine envelopes This paper describes systematic method computing bounding envelope multivariate monotonic function given stream data The derived envelope computed determining simultaneous confidence band special neural network guaranteed produce monotonic functions By composing envelopes complex systems simulated using semiquantitative methods',\n",
              " 'In paper describe application MemoryBased Learning problem Prepositional Phrase attachment disambiguation We compare MemoryBased Learning stores examples memory generalizes using intelligent similarity metrics number recently proposed statistical methods well suited large numbers features We evaluate methods common benchmark dataset show method compares favorably previous methods wellsuited incorporating various unconventional representations word patterns value difference metrics Lexical Space',\n",
              " 'Hierarchically structured mixture models studied context data analysis inference neural synaptic transmission characteristics mammalian central nervous systems Mixture structures arise due uncertainties stochastic mechanisms governing responses electrochemical stimulation individual neurotransmitter release sites nerve junctions Models attempt capture scientific features sensitivity individual synaptic transmission sites electrochemical stimuli extent electrochemical responses stimulated This done via suitably structured classes prior distributions parameters describing features Such priors may structured permit assessment currently topical scientific hypotheses fundamental neural function Posterior analysis implemented via stochastic simulation Several data analyses described illustrate approach resulting neurophysiological insights recently generated experimental contexts Further developments open questions neurophysiological statistical noted Research partially supported NSF grants DMS DMS DMS This work represents part collaborative project Dr Dennis A Turner Duke University Medical Center Durham VA Data provided Dr Turner Dr Howard V Wheal Southampton University A slightly revised version paper published Journal American Statistical Association vol pp modified title Hierarchical Mixture Models Neurological Transmission Analysis The author recipient Mitchell Prize Bayesian analysis substantive concrete problem based work reported paper',\n",
              " 'The need software modules performing natural language processing NLP tasks growing These modules perform efficiently accurately time rapid development often mandatory Recent work indicated machine learning techniques general memorybased learning MBL particular offer tools meet ends We present examples modules trained MBL three NLP tasks texttospeech conversion ii partofspeech tagging iii phrase chunking We demonstrate three modules display high generalization accuracy argue MBL applicable similarly well large class NLP tasks',\n",
              " 'We present membership query ie interpolation algorithm exactly identifying class readonce formulas basis boolean threshold functions Using generic transformation Angluin Hellerstein Karpinski gives algorithm using membership equivalence queries exactly identifying class readonce formulas basis boolean threshold functions negation We also present series generic transformations used convert algorithm one learning model algorithm different model',\n",
              " 'We study time series model viewed decision tree Markov temporal structure The model intractable exact calculations thus utilize variational approximations We consider three different distributions approximation one Markov calculations performed exactly layers decision tree decoupled one decision tree calculations performed exactly time steps Markov chain decoupled one Viterbilike assumption made pick single likely state sequence We present simulation results artificial data Bach chorales Accepted oral presentation NIPS',\n",
              " 'Stochastic simulation algorithms likelihood weighting often give fast accurate approximations posterior probabilities probabilistic networks methods choice large networks Unfortunately special characteristics dynamic probabilistic networks DPNs used represent stochastic temporal processes mean standard simulation algorithms perform poorly In essence simulation trials diverge reality process observed time In paper present simulation algorithms use evidence observed time step push set trials back towards reality The first algorithm evidence reversal ER restructures time slice DPN evidence nodes slice become ancestors state variables The second algorithm called survival fittest sampling SOF repopulates set trials time step using stochastic reproduction rate weighted likelihood evidence according trial We compare performance algorithm likelihood weighting original network also investigate benefits combining ER SOF methods The ERSOF combination appears maintain bounded error independent number time steps simulation',\n",
              " 'Simulated Annealing Search technique single trial solution modified random An energy defined represents good solution The goal find best solution minimising energy Changes lead lower energy always accepted increase probabilistically accepted The probability given expEk B T Where E change energy k B constant T Temperature Initially temperature high corresponding liquid molten state large changes possible progressively reduced using cooling schedule allowing smaller changes system solidifies low energy solution',\n",
              " 'Systems learn examples often create disjunctive concept definition The disjuncts concept definition cover training examples referred small disjuncts The problem small disjuncts error prone large disjuncts may necessary achieve high level predictive accuracy Holte Acker Porter This paper extends previous work done problem small disjuncts investigating reasons small disjuncts error prone large disjuncts evaluating impact small disjuncts inductive learning This paper shows attribute noise missing attributes class noise training set size cause small disjuncts error prone large disjuncts This paper also evaluates impact factors learning small disjuncts ie error rate It shows two artificial domains low levels attribute noise applied training set ability learn correct noisefree concept evaluated small disjuncts primarily responsible making learning difficult',\n",
              " 'A number efficient learning algorithms achieve exact identification unknown function class using membership equivalence queries Using standard transformation algorithms easily converted online learning algorithms use membership queries Under transformation number equivalence queries made query algorithm directly corresponds number mistakes made online algorithm In paper consider several natural classes known learnable setting investigate minimum number equivalence queries accompanying counterexamples equivalently minimum number mistakes online model made learning algorithm makes polynomial number membership queries uses polynomial computation time We able reduce number equivalence queries used previous algorithms often prove matching lower bounds As example consider class DNF formulas n variables k Olog n terms Previously algorithm Blum Rudich BR provided best known upper bound Ok log n minimum number equivalence queries needed exact identification We greatly improve upper bound showing exactly k counterexamples needed learner knows k priori exactly k counterexamples needed learner know k priori This exactly matches known lower bounds BC For many results obtain complete characterization tradeoff number membership equivalence queries needed exact identification The classes consider monotone DNF formulas Horn sentences Olog nterm DNF formulas readk satj DNF formulas readonce formulas various bases deterministic finite automata',\n",
              " 'We present learning algorithm rulebased concept representations called rippledown rule sets Rippledown rule sets allow us deal exceptions rule separately introducing exception rules exception rules exception rule etc constant depth These local exception rules contrast decision lists exception rules must placed global ordering rules The localization exceptions makes possible represent concepts decision list representation On hand decision lists constant number alternations rules different classes represented constant depth rippledown rule sets polynomial increase size Our algorithm Occam algorithm constant depth rippledown rule sets hence PAC learning algorithm It based repeatedly applying greedy approximation method weighted set cover problem find good exception rule sets',\n",
              " 'We present algorithm learning sets rules organized k levels Each level contain arbitrary number rules c l l class associated level c concept given class basic concepts The rules higher levels precedence rules lower levels used represent exceptions As basic concepts use Boolean attributes infinite attribute space model certain concepts defined terms substrings Given sample examples algorithm runs polynomial time produces consistent concept representation size Olog k n k n size smallest consistent representation k levels rules This implies algorithm learns PAC model The algorithm repeatedly applies greedy heuristics weighted set cover The weights obtained approximate solutions previous set cover problems',\n",
              " 'In paper investigate representational methodological issues attractor network model mapping orthography semantics based Plaut We find contrary psycholinguistic studies response time concrete words represented bits output pattern slower abstract words This model also predicts response times words dense semantic neighborhood faster words semantically similar neighbors language This conceptually consistent neighborhood effect seen mapping orthography phonology Seidenberg McClelland Plaut et al patterns many neighbors faster pathways since regularity random mapping used clear cause effect different previous experiments We also report rather distressing finding Reaction time model measured time takes network settle presented new input When criterion used determine network settled changed include testing hidden units results reported change direction effect abstract words slower words dense semantic neighborhoods Since independent reasons exclude hidden units stopping criterion done common practice believe phenomenon interest mostly neural network practitioners However provide insight interaction hidden output units settling',\n",
              " 'Casebased reasoning CBR used form caching solved problems speedup later problem solving Using cached cases brings additional costs due retrieval time case adaptation time also storage space Simply storing cases result situation retrieving trying adapt old cases take time average caching This means caching must applied selectively build case memory actually useful This form utility problem The approach taken construct cost model system used predict effect changes system In paper describe utility problem associated caching cases construction cost model We present experimental results demonstrate model used predict effect certain changes case memory',\n",
              " 'A Genetic Algorithmic GA approach vector quantizer design combines conventional Generalized Lloyd Algorithm GLA presented We refer hybrid Genetic Generalized Lloyd Algorithm GGLA It works briefly follows A finite number codebooks called chromosomes selected Each codebook undergoes iterative cycles reproduction We perform experiments various alternative design choices using GaussianMarkov processes speech image source data signaltonoise ratio SNR performance measure In cases GGLA showed performance improvements respect GLA We also compare results ZadorGersho formula',\n",
              " 'In casebased planning CBP previously generated plans stored cases memory reused solve similar planning problems future CBP save considerable time planning scratch generative planning thus offering potential heuristic mechanism handling intractable problems One drawback CBP systems need highly structured memory requires significant domain engineering complex memory indexing schemes enable efficient case retrieval In contrast CBP system CaPER based massively parallel framebased AI language extremely fast retrieval complex cases large unindexed memory The ability fast frequent retrievals many advantages indexing unnecessary large casebases used memory probed numerous alternate ways allowing specific retrieval stored plans better fit target problem less adaptation fl Preliminary version article appearing IEEE Expert February pp This paper extended version',\n",
              " 'We present efficient method assigning number processors tasks associated cells rectangular uniform grid Load balancing equipartition constraints observed approximately minimizing total perimeter partition corresponds amount interprocessor communication This method based upon decomposition grid stripes optimal height We prove mild assumptions problem size grows large parameters error bound associated feasible solution approaches zero We also present computational results high level parallel Genetic Algorithm utilizes method make comparisons methods On network workstations algorithm solves within minutes instances problem would require one billion binary variables Quadratic Assignment formulation',\n",
              " 'Finding Bayesian balance exploration exploitation adaptive optimal control general intractable This paper shows compute suboptimal estimates based certainty equivalence approximation arising form dual control This systematizes extends existing uses exploration bonuses reinforcement learning Sutton The approach two components statistical model uncertainty world way turning exploratory behaviour',\n",
              " 'This paper deals nonlinear leastsquares problems involving fitting data parameterized analytic functions For generic regression data general result establishes countability stronger assumptions finiteness set functions giving rise critical points quadratic loss function In special case usually called singlehidden layer neural networks built upon standard sigmoidal activation tanhx equivalently e x rough upper bound cardinality provided well',\n",
              " 'This research funded part NSF Grant No IRI part ONR Grant No NJ We thank John Clement use protocol transcript James Greeno contribution developing constructive modeling interpretation Ryan Tweney helpful comments Todd W Griffith Nancy J Nersessian Ashok Goel Abstract We hypothesize generic models central conceptual change science This hypothesis origins two theoretical sources The first source constructive modeling derives philosophical theory synthesizes analyses historical conceptual changes science investigations reasoning representation cognitive psychology The theory constructive modeling posits generic mental models productive conceptual change The second source adaptive modeling derives computational theory creative design Both theories posit situation independent domain abstractions ie generic models Using constructive modeling interpretation reasoning exhibited protocols collected John Clement problem solving session involving conceptual change employ resources theory adaptive modeling develop new computational model ToRQUE Here describe piece analysis protocol illustrate synthesis two theories used develop system articulating testing ToRQUE The results research show generic modeling plays central role conceptual change They also demonstrate interdisciplinary synthesis provide significant insights scientific reasoning',\n",
              " 'This paper discusses design neural networks solve specific problems adaptive control In particular investigates influence typical problems arising realworld control tasks well techniques solution exist framework neurocontrol Based investigation systematic design method developed The method exemplified development adaptive force controller robot manipulator',\n",
              " 'We present informationtheoretic derivation learning algorithm clusters unlabelled data linear discriminants In contrast methods try preserve information input patterns maximize information gained observing output robust binary discriminators implemented sigmoid nodes We derive local weight adaptation rule via gradient ascent objective demonstrate dynamics simple data sets relate approach previous work suggest directions may extended',\n",
              " 'This paper presents ASOCS Adaptive SelfOrganizing Concurrent System model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control An ASOCS adaptive network composed many simple computing elements operating asynchronously parallel This paper focuses Adaptive Algorithm AA details architecture learning algorithm AA significant memory knowledge maintenance advantages previous ASOCS models An ASOCS operate either data processing mode learning mode During learning mode ASOCS given new rule expressed boolean conjunction The AA learning algorithm incorporates new rule distributed fashion short bounded time During data processing mode ASOCS acts parallel hardware circuit',\n",
              " 'This paper presents method analyzing coupled time series using Markov models domain state space immense To make parameter estimation tractable large state space represented Cartesian product smaller state spaces paradigm known factorial Markov models The transition matrix model represented mixture transition matrices underlying dynamical processes This formulation know mixed memory Markov models Using framework analyze daily exchange rates five currencies British pound Canadian dollar Deutsch mark Japanese yen Swiss franc measured US dollar',\n",
              " 'This work explores use machine learning methods extracting knowledge simulations complex systems In particular use genetic algorithms learn rulebased strategies used autonomous robots The evaluation given strategy may require several executions simulation produce meaningful estimate quality strategy As consequence evaluation single individual genetic algorithm requires fairly substantial amount computation Such system suggests sort largegrained parallelism available network workstations We describe implementation parallel genetic algorithm present case studies resulting speedup two robot learning tasks',\n",
              " 'Most Artificial Neural Networks ANNs fixed topology learning often suffer number shortcomings result Variations ANNs use dynamic topologies shown ability overcome many problems This paper introduces LocationIndependent Transformations LITs general strategy implementing distributed feedforward networks use dynamic topologies dynamic ANNs efficiently parallel hardware A LIT creates set locationindependent nodes node computes part network output independent nodes using local information This type transformation allows efficient support adding deleting nodes dynamically learning In particular paper presents LIT dynamic Backpropagation networks single hidden layer The complexity learning execution algorithms Onplogm single pattern nis number inputs p number outputs number hidden nodes original network Keywords Neural Networks Backpropagation Implementation Design Dynamic Topologies Reconfigurable Architectures',\n",
              " 'Hard combinatorial problems sequencing scheduling led recently research genetic algorithms Canonical coding symmetric TSP modified coding njob mmachine flowshop problem configurates solution space different way We show well known genetic operators act intelligently coding scheme They implecitely prefer subset solutions contain probably best solutions respect objective We conjecture every new problem needs determination necessary condition genetic algorithm work e proof experiment We implemented asynchronous parallel genetic algorithm UNIXbased computer network Computational results new heuristic discussed',\n",
              " 'This paper presents VLSI implementation Priority Adaptive SelfOrganizing Concurrent System PASOCS learning model built using multichip module MCM substrate Many current hardware implementations neural network learning models direct implementations classical neural network structuresa large number simple computing nodes connected dense number weighted links PASOCS one class ASOCS Adaptive SelfOrganizing Concurrent System connectionist models whose overall goal classical neural networks models whose functional mechanisms differ significantly This model potential application areas pattern recognition robotics logical inference dynamic control',\n",
              " 'The application adaptive optimization strategies scheduling manufacturing systems recently become research topic broad interest Population based approaches scheduling predominantly treat static data models whereas realworld scheduling tends dynamic problem This paper briefly outlines application genetic algorithm dynamic job shop problem arising production scheduling First sketch genetic algorithm handle release times jobs In second step preceding simulation method used improve performance algorithm Finally job shop regarded nondeterministic optimization problem arising occurrence job releases Temporal Decomposition leads scheduling control interweaves simulation time genetic search',\n",
              " 'Neural network pruning methods level individual network parameters eg connection weights improve generalization shown empirical study However open problem pruning methods known today OBD OBS autoprune epsiprune selection number parameters removed pruning step pruning strength This work presents pruning method lprune automatically adapts pruning strength evolution weights loss generalization training The method requires algorithm parameter adjustment user Results statistical significance tests comparing autoprune lprune static networks early stopping given based extensive experimentation different problems The results indicate training pruning often significantly better rarely significantly worse training early stopping without pruning Furthermore lprune often superior autoprune superior OBD diagnosis tasks unless severe pruning early training process required',\n",
              " 'Casebased problemsolving systems rely similarity assessment select stored cases whose solutions easily adaptable fit current problems However widelyused similarity assessment strategies evaluation semantic similarity poor predictors adaptability As result systems may select cases difficult impossible adapt even easily adaptable cases available memory This paper presents new similarity assessment approach couples similarity judgments directly case library containing systems adaptation knowledge It examines approach context casebased planning system learns new plans new adaptations Empirical tests alternative similarity assessment strategies show approach enables better case selection increases benefits accrued learned adaptations',\n",
              " 'The casebased reasoning process depends multiple overlapping knowledge sources provides opportunity learning Exploiting opportunities requires determining learning mechanisms use individual knowledge source also different learning mechanisms interact combined utility This paper presents case study examining relative contributions costs involved learning processes three different knowledge sourcescases case adaptation knowledge similarity informationin casebased planner It demonstrates importance interactions different learning processes identifies promising method integrating multiple learning methods improve casebased reasoning',\n",
              " 'Casebased reasoning depends multiple knowledge sources beyond case library including knowledge case adaptation criteria similarity assessment Because hand coding knowledge accounts large part knowledge acquisition burden developing CBR systems appealing acquire learning CBR promising learning method apply This observation suggests developing casebased CBR systems CBR systems whose components use CBR However despite early interest casebased approaches CBR method received comparatively little attention Open questions include casebased components CBR system designed amount knowledge acquisition effort require effectiveness This paper investigates questions case study issues addressed methods used results achieved casebased planning system uses CBR guide case adaptation similarity assessment The paper discusses design considerations presents empirical results support usefulness casebased CBR point potential problems tradeoffs directly demonstrate overlapping roles different CBR knowledge sources The paper closes general lessons casebased CBR areas future research',\n",
              " 'A linear support vector machine formulation used generate fast finitelyterminating linearprogramming algorithm discriminating two massive sets ndimensional space number points orders magnitude larger n The algorithm creates succession sufficiently small linear programs separate chunks data time The key idea small number support vectors corresponding linear programming constraints positive dual variables carried successive small linear programs containing chunk data We prove procedure monotonic terminates finite number steps exact solution leads globally optimal separating plane entire dataset Numerical results fully dense publicly available datasets numbering million points dimensional space confirm theoretical results demonstrate ability handle large problems',\n",
              " 'In Sejnowski Rosenberg developed famous NETtalk system English texttospeech This chapter describes machine learning approach texttospeech builds upon extends initial NETtalk work Among many extensions NETtalk system following different learning algorithm wider input window errorcorrecting output coding righttoleft scan word pronounced results decision influencing subsequent decisions addition several useful input features These changes yielded system performs much better original NETtalk system After training words system achieves correct pronunciation individual phonemes correct pronunciation whole words pronunciation must exactly match dictionary pronunciation correct Based judgements three human participants blind assessment study system estimated serious error rate whole words compared error rate DECTalk rulebase',\n",
              " 'The problem minimizing number misclassified points plane attempting separate two point sets intersecting convex hulls ndimensional real space formulated linear program equilibrium constraints LPEC This general LPEC converted exact penalty problem quadratic objective linear constraints A FrankWolfetype algorithm proposed penalty problem terminates stationary point global solution Novel aspects approach include A linear complementarity formulation step function counts misclassifications ii Exact penalty formulation without boundedness nondegeneracy constraint qualification assumptions iii An exact solution extraction sequence minimizers penalty function finite value penalty parameter general LPEC explicitly exact solution LPEC uncoupled constraints iv A parametric quadratic programming formulation LPEC associated misclassification minimization problem',\n",
              " 'Planning analogical reasoning learning method consists storage retrieval replay planning episodes Planning performance improves accumulation reuse library planning cases Retrieval driven domaindependent similarity metrics based planning goals scenarios In complex situations multiple goals retrieval may find multiple past planning cases jointly similar new planning situation This paper presents issues implications involved replay multiple planning cases opposed single one Multiple case plan replay involves adaptation merging annotated derivations planning cases Several merge strategies replay introduced process various forms eagerness differences past new situations annotated justifications planning cases In particular introduce effective merging strategy considers plan step choices especially appropriate interleaving planning plan execution We illustrate discuss effectiveness merging strategies specific domains',\n",
              " 'Mixedinitiative planning envisions framework automated human planners interact jointly construct plans satisfy specific objectives In paper report work engineering robust mixedinitiative planning system Human planners rely strongly past planning experience generate new plans ForMAT casebased system supports human planning accumulation userbuilt plans querydriven browsing past plans several plan functionality analysis primitives ProdigyAnalogy automated AI planner combines generative casebased planning Stored plans annotated plan rationale reuse involves adaptation driven rationale Our system MICBP integrates ForMAT ProdigyAnalogy realtime messagepassing mixedinitiative planning system The main technical approach consists allowing user specify link objectives enable system capture reuse plan rationale We present MICBP concrete application domain military force deployment planning This synergistic system increases planning efficiency human planners automated suggestion similar past plans plausible plan modifications',\n",
              " 'The primary goal inductive learning generalize well induce function accurately produces correct output future inputs Hansen Salamon showed certain assumptions combining predictions several separately trained neural networks improve generalization One key assumptions individual networks independent errors produce In standard way performing backpropagation assumption may violated standard procedure initialize network weights region weight space near origin This means backpropagations gradientdescent search may reach small subset possible local minima In paper present approach initializing neural networks uses competitive learning intelligently create networks originally located far origin weight space thereby potentially increasing set reachable local minima We report experiments two realworld datasets combinations networks initialized method generalize better combina tions networks initialized traditional way',\n",
              " 'We present two algorithms inducing structural equation models data Assuming latent variables models causal interpretation parameters may estimated linear multiple regression Our algorithms comparable PC IC rely conditional independence We present algorithms empirical comparisons PC IC',\n",
              " 'We investigate neural network based approximation methods These methods depend locality basis functions After discussing local global basis functions propose multiresolution hierarchical method The various resolutions stored various levels tree At root tree global approximation kept leafs store learning samples Intermediate nodes store intermediate representations In order find optimal partitioning input space selforganising maps SOMs used The proposed method implementational problems reminiscent encountered manyparticle simulations We investigate parallel implementation method using parallel hierarchical meth ods manyparticle simulations starting point',\n",
              " 'Orthogonal incremental learning OIL new approach incremental training feedforward network single hidden layer OIL based idea describe output weights hidden nodes set orthogonal basis functions Hidden nodes treated orthogonal representation network output weights domain We proved separate training hidden nodes conflict previously optimized nodes described special relationship orthogonal backpropagation OBP rule An advantage OIL existing algorithms extremely fast learning This approach also easily extended buildup incrementally arbitrary function linear composition adjustable functions necessarily orthogonal OIL tested twospirals Net Talk benchmark problems',\n",
              " 'Todays potential users machine learning technology faced nontrivial problem choosing large everincreasing number available tools one appropriate particular task To assist often noninitiated users desirable model selection process automated Using experience base level learning researchers proposed metalearning possible solution Historically predictive accuracy de facto criterion work metalearning focusing discovery rules match applications models based accuracy Although predictive accuracy clearly important criterion also case number criteria could often ought considered learning model selection This paper presents number criteria discusses impact metalevel approaches model selection',\n",
              " 'One approach invariant object recognition employs recurrent neural network associative memory In standard depiction networks state space memories objects stored attractive fixed points dynamics I argue modification picture object continuous family instantiations represented continuous attractor This idea illustrated network learns complete patterns To perform task filling missing information network develops continuous attractor models manifold patterns drawn From statistical viewpoint pattern completion task allows formulation unsupervised A classic approach invariant object recognition use recurrent neural network associative memory In spite intuitive appeal biological plausibility approach largely abandoned practical applications This paper introduces two new concepts could help resurrect object representation continuous attractors learning attractors pattern completion In models associative memory memories stored attractive fixed points discrete locations state space Discrete attractors may appropriate patterns continuous variability like images threedimensional object different viewpoints When instantiations object lie continuous pattern manifold appropriate represent objects attractive manifolds fixed points continuous attractors To make idea practical important find methods learning attractors examples A naive method train network retain examples shortterm memory This method deficient prevent network storing spurious fixed points unrelated examples A superior method train network restore examples corrupted learns complete patterns filling missing information learning terms regression rather density estimation',\n",
              " 'In paper consider problem independent constraint handling mechanism Stepwise Adaptation Weights SAW show working graph coloring problems SAWing technically belongs penalty function based approaches amounts modifying penalty function search We show twofold benefit First proves rather insensitive technical parameters thereby providing general problem independent way handle constrained problems Second leads superior EA performance In extensive series comparative experiments show SAWing EA outperforms powerful graph coloring heuristic algorithm DSatur hardest graph instances linear scaleup behaviour',\n",
              " 'Recently several neural algorithms introduced Independent Component Analysis Here approach problem point view single neuron First simple Hebbianlike learning rules introduced estimating one independent components sphered data Some learning rules used estimate independent component negative kurtosis others estimate component positive kurtosis Next twounit system introduced estimate independent component kurtosis The results generalized estimate independent components nonsphered raw mixtures To separate several independent components system several neurons linear negative feedback used The convergence learning rules rigorously proven without unnecessary hypotheses distributions independent components',\n",
              " 'In paper define task place learning describe one approach problem The framework represents distinct places using evidence grids probabilistic description occupancy Place recognition relies casebased classification augmented registration process correct translations The learning mechanism also similar casebased systems involving simple storage inferred evidence grids Experimental studies physical simulated robots suggest approach improves place recognition experience handle significant sensor noise scales well increasing numbers places Previous researchers studied evidence grids place learning combined two powerful concepts used experimental methods machine learning evaluate methods abilities',\n",
              " 'In constructive induction CI learners problem representation modified normal part learning process This useful initial representation inadequate inappropriate In paper I argue distinction constructive nonconstructive methods unclear I propose theoretical model allows clean distinction made b process CI properly motivated I also show although constructive induction used almost exclusively context supervised learning reason form part unsupervised regime',\n",
              " 'When designing deductive database designer decide predicate relation whether defined extensionally intensionally definition look like An intelligent system presented assist designer task It starts example database predicates defined extensionally It tries compact database transforming extensionally defined predicates intensionally defined ones The intelligent system employs techniques area inductive logic programming',\n",
              " 'When work information multiple sources formalism employs handle uncertainty may uniform In order able combine knowledge bases different formats need first establish common basis characterizing evaluating different formalisms provide semantics combined mechanism A common framework provide infrastructure building integrated system essential understand behavior We present unifying framework based ordered partition possible worlds called partition sequences corresponds intuitive notion biasing towards certain possible scenarios uncertain actual situation We show existing formalisms namely default logic autoepistemic logic probabilistic conditioning thresholding generalized conditioning possibility theory incorporated general framework',\n",
              " 'This paper investigates technique creating sparsely connected feedforward neural networks may capable producing networks large input output layers The architecture appears particularly suited tasks involve sparse training data able take advantage sparseness reduce training time Some initial results presented based tests bit compression problem',\n",
              " 'In paper propose method calculate posterior probability nondecomposable graphical Gaussian model Our proposal based new device sample Wishart distributions conditional graphical constraints As result methodology allows Bayesian model selection within whole class graphical Gaussian models including nondecomposable ones',\n",
              " 'For absorbing Markov chain reinforcement transition Bertsekas gives simple example function learned TD depends Bertsekas showed approximation optimal respect leastsquares error value function approximation obtained TD method poor respect metric With respect error values TD approximates function better TD However respect error differences values TD approximates function better TD TD better TD respect former metric rather latter In addition direct TD weights errors unequally residual gradient methods Baird Harmon Baird Klopf weight errors equally For case control simple Markov decision process presented direct TD residual gradient TD learn optimal policy TD learns suboptimal policy These results suggest example differences state values significant state values TD preferable TD',\n",
              " 'Lazy learning methods provide useful representations training algorithms learning complex phenomena autonomous adaptive control complex systems This paper surveys ways locally weighted learning type lazy learning applied us control tasks We explain various forms control tasks take affects choice learning paradigm The discussion section explores interesting impact explicitly remembering previous experiences problem learning control',\n",
              " 'Genetic programming GP variant genetic algorithms data structures handled trees This makes GP especially useful evolving functional relationships computer programs represented trees Symbolic regression determination function dependence gx approximates set data points x In paper feasibility symbolic regression GP demonstrated two examples taken different domains Furthermore several suggested methods literature compared intended improve GP performance readability solutions taking account introns redundancy occurs trees keeping size trees small The experiments show GP elegant useful tool derive complex functional dependencies numerical data',\n",
              " 'We report study mixture modeling problems arising assessment chemical structureactivity relationships drug design discovery Pharmaceutical research laboratories developing test compounds screening synthesize many related candidate compounds linking together collections basic molecular building blocks known monomers These compounds tested biological activity feeding screening analysis drug design The tests also provide data relating compound activity chemical properties aspects structure associated monomers focus studying relationships aid future monomer selection The level chemical activity compounds based geometry chemical binding test compounds target binding sites receptor compounds screening tests unable identify binding configurations Hence potentially critical covariate information missing natural latent variable Resulting statistical models mixed respect missing information complicating data analysis inference This paper reports study twomonomer twobinding site framework associated data We build structured mixture models mix linear regression models predicting chemical effectiveness respect sitebinding selection mechanisms We discuss aspects modeling analysis including problems pitfalls describe results analyses simulated real data set In modeling real data led critical model extensions introduce hierarchical random effects components adequately capture heterogeneities site binding mechanisms resulting levels effectiveness compounds bound Comments current potential future directions conclude report',\n",
              " 'We give analysis generalization error cross validation terms two natural measures difficulty problem consideration approximation rate accuracy target function ideally approximated function number hypothesis parameters estimation rate deviation training generalization errors function number hypothesis parameters The approximation rate captures complexity target function respect hypothesis model estimation rate captures extent hypothesis model suffers overfitting Using two measures give rigorous general bound error cross validation The bound clearly shows tradeoffs involved making fl fraction data saved testing large small By optimizing bound respect fl argue combination formal analysis plotting controlled experimentation following qualitative properties cross validation behavior quite robust significant changes underlying model selection problem',\n",
              " 'Model selection eg considered problem choosing hypothesis language provides optimal balance low empirical error high structural complexity In Abstract discuss intuition new efficient approach model selection Our approach inherently Bayesian eg instead using priors target functions hypotheses talk priors error values leads us new mathematical characterization expected true error In setting classification learning learner given sample drawn according unknown distribution labeled instances returns empirical minimizer hypothesis least empirical error certain unknown true error If process carried repeatedly true error empirical minimizer vary run run empirical minimizer depends randomly drawn sample This induces distribution true errors empirical minimizers possible samples drawn according unknown distribution If distribution would known one could easily derive expected true error empirical minimizer model integrating distribution This would immediately lead optimal model selection algorithm Enumerate models calculate expected error model integrating error distribution select model least expected error PAC theory VC framework provide worstcase bounds chance drawing sample true error minimizer exceeds worstcase meaning hold distribution instances concept given class By contrast focus determine distribution fixed given learning problem specified assumptions Unlike worstcase bound depends size VCdimension hypothesis space actual error distribution depends hypothesis space unknown distribution labeled instances However prove certain assumption independence hypotheses distribution true errors hence expected true error expressed function distribution empirical errors uniformly drawn hypotheses thought prior error values The latter distribution always onedimensional estimated fixedsized initial portion training data fixedsized set randomly drawn hypotheses This estimate distribution leads us estimate expected true error empirical minimizer model turn leads highly efficient model selection algorithm We study behavior approach several controlled experiments Our results show accuracy error estimate least comparable accuracy estimate obtained fold crossvalidation provided prior error values estimated using least examples But CV requires ten invocations learner per model time algorithm requires assess model constant size model We also study robustness algorithm violations independence assumptions We observe bias predictions hypotheses space size four less When hypothesis space size dependencies diluted violations assumptions negligible incur significant error The full paper available httpkicstuberlindeschefferpaperseedreportps',\n",
              " 'In area inductive learning generalization main operation usual definition induction based logical implication Recently rising interest clausal representation knowledge machine learning Almost inductive learning systems perform generalization clauses use relation subsumption instead implication The main reason wellknown simple technique compute least general generalizations subsumption implication However generalization subsumption inappropriate learning recursive clauses crucial problem since recursion basic program structure logic programs We note implication clauses undecidable therefore introduce stronger form implication called Timplication decidable clauses We show every finite set clauses exists least general generalization Timplication We describe technique reduce generalizations implication clause generalizations subsumption call expansion original clause Moreover show every nontautological clause exists Tcomplete expansion means every generalization Timplication clause reduced generalization subsumption expansion',\n",
              " 'This paper argues Bayesian probability theory general method machine learning From two wellfounded axioms theory capable accomplishing learning tasks incremental nonincremental supervised unsupervised It learn different types data regardless whether noisy perfect independent facts behaviors unknown machine These capabilities partially demonstrated paper uniform application theory two typical types machine learning incremental concept learning unsupervised data classification The generality theory suggests process learning may many different types currently held method oldest may best',\n",
              " 'This paper focuses bias variance decomposition analysis local learning algorithm nearest neighbor classifier extended error correcting output codes This extended algorithm often considerably reduces ie classification error comparison nearest neighbor Ricci Aha The analysis presented reveals performance improvement obtained drastically reducing bias cost increasing variance We also show even classification problems classes extending codeword length beyond limit assures column separation yields error reduction This error reduction variance due voting mechanism used errorcorrecting output codes also bias',\n",
              " 'We integrated distributed search genetic programming GP based systems collective memory form collective adaptation search method Such system significantly improves search problem complexity increased Since pure GP approach scale well problem complexity natural question two components actually contributing search process We investigate collective memory search utilizes random search engine find significantly outperforms GP based search engine We examine solution space show problem complexity search space grow collective adaptive system perform better collective memory search employing random search engine',\n",
              " 'The document presents approach judging relevance retrieved information based novel approach similarity assessment Contrary systems define relevance measures context similarity query time This necessary since without context similarity one guarantee similar items also relevant',\n",
              " 'This paper presents selfimproving reactive control system autonomous robotic navigation The navigation module uses schemabased reactive control system perform navigation task The learning module combines casebased reasoning reinforcement learning continuously tune navigation system experience The casebased reasoning component perceives characterizes systems environment retrieves appropriate case uses recommendations case tune parameters reactive control system The reinforcement learning component refines content cases based current experience Together learning components perform online adaptation resulting improved performance reactive control system tunes environment well online learning resulting improved library cases capture environmental regularities necessary perform online adaptation The system extensively evaluated simulation studies using several performance metrics system configurations',\n",
              " 'A model onsite learning presented The system learns querying hard patterns classifying easy ones This model related querybased filtering methods takes account addition labelling filtering data cost A simple policies introduced analyzed simple problem D high low game In addition QuerybyCommittee algorithm Seung et al suggested good approximator model space realworld domains Results using algorithm synthesized problem realworld OCR task using backpropagation network nearest neighbor classifier show onsite learner perform well classifier trained offsite achieving significant cost reduction',\n",
              " 'The standard method obtaining response treebased genetic programming take value returned root node In nontree representations alternate methods explored One alternative treat specific location indexed memory response value program terminates The purpose paper explore applicability technique treestructured programs explore intron effects studies bring light This papers experimental results support finding memorybased program response technique improvement problems In addition papers experimental results support finding contrary past research speculation addition even facilitation introns seriously degrade search performance genetic programming',\n",
              " 'We discuss implications Holtes recentlypublished article demonstrated commonly used data simple classification rules almost accurate decision trees produced Quinlans C We consider particular significance Holtes results future topdown induction decision trees To extent Holte questioned sense research multilevel decision tree learning We go detail parts Holtes study We try put results perspective We argue absolute terms small difference accuracy R C witnessed Holte still significant We claim C possesses additional accuracyrelated advantages R In addition discuss representativeness databases used Holte We compare empirically optimal accuracies multilevel onelevel decision trees observe significant differences We point several deficien cies limitedcomplexity classifiers',\n",
              " 'We describe approach graphemetophoneme conversion languageindependent dataoriented Given set examples spelling words associated phonetic representation language graphemetophoneme conversion system automatically produced language takes input spelling words produces output phonetic transcription according rules implicit training data We describe design system compare performance knowledgebased alternative dataoriented approaches',\n",
              " 'No finite sample sufficient determine density therefore entropy signal directly Some assumption either functional form density smoothness necessary Both amount prior space possible density functions By far common approach assume density parametric form By contrast derive differential learning rule called EMMA optimizes entropy way kernel density estimation Entropy derivative calculated sampling density estimate The resulting parameter update rule surprisingly simple efficient We show EMMA used detect correct corruption magnetic resonance images MRI This application beyond scope existing parametric entropy models',\n",
              " 'A satisficing search problem consists set probabilistic experiments performed order without repetitions satisfying configuration successes failures reached The cost performing experiments depends order chosen Earlier work concentrated finding optimal search strategies special cases model search trees andor graphs cost function success probabilities experiments given In contrast study complexity learning approximately optimal search strategy success probabilities known outset Working fully general model show n number unknown probabilities C maximum cost performing experiments',\n",
              " 'We present method calculating phase diagrams highdimensional variant SelfOrganizing Map SOM The method requires ansatz tesselation data space induced map explicit state map Using method analyze two recently proposed models development orientation ocular dominance column maps The phase transition condition orientation map turns different form corresponding lowdimensional map',\n",
              " 'We study process multiagent reinforcement learning context load balancing distributed system without use either central coordination explicit communication We first define precise framework study adaptive load balancing important features stochastic nature purely local information available individual agents Given framework show illuminating results interplay basic adaptive behavior parameters effect system efficiency We investigate properties adaptive load balancing heterogeneous populations address issue exploration vs exploitation context Finally show naive use communication may improve might even harm system efficiency',\n",
              " 'Brendan J Frey Geoffrey E Hinton Efficient stochastic source coding application Bayesian network source model The Computer Journal In paper introduce new algorithm called bitsback coding makes stochastic source codes efficient For given onetomany source code show algorithm actually efficient algorithm always picks shortest codeword Optimal efficiency achieved codewords chosen according Boltzmann distribution based codeword lengths It turns commonly used technique determining parameters maximum likelihood estimation actually minimizes bitsback coding cost codewords chosen according Boltzmann distribution A tractable approximation maximum likelihood estimation generalized expectation maximization algorithm minimizes bitsback coding cost After presenting binary Bayesian network model assigns exponentially many codewords symbol show tractable approximation Boltzmann distribution used bitsback coding We illustrate performance bitsback coding using using nonsynthetic data binary Bayesian network source model produces possible codewords input symbol The rate bitsback coding nearly one half obtained picking shortest codeword symbol',\n",
              " 'Agents learn agents exploit information possess distinct advantage competitive situations Games provide stylized adversarial environments study agent learning strategies Researchers developed game playing programs learn play better experience We developed learning program learn play better learns identify exploit weaknesses particular opponent repeatedly playing several games We propose scheme learning opponent action probabilities utility maximization framework exploits learned opponent model We show proposed expected utility maximization strategy generalizes traditional maximin strategy allows players benefit taking calculated risks avoided maximin strategy Experiments popular board game Connect show learning player consistently outperforms nonlearning player pitted another automated player using weaker heuristic Though proposed mechanism improve skill level computer player improve ability play effectively weaker opponent',\n",
              " 'Many real world learning problems best characterized interaction multiple independent causes factors Discovering causal structure data focus paper Based Zemel Hintons cooperative vector quantizer CVQ architecture unsupervised learning algorithm derived ExpectationMaximization EM framework Due combinatorial nature data generation process exact Estep computationally intractable Two alternative methods computing Estep proposed Gibbs sampling meanfield approximation promising empirical results presented',\n",
              " 'This paper deals problem blind identification source separation consists estimation mixing matrix andor separation mixture stochastically independent sources without priori knowledge mixing matrix The method propose estimates mixture matrix recurrent InputOutput IO Identification using inputs nonlinear transformation estimated sources Herein nonlinear transformation distortion consists constraining modulus inputs IOIdentification device constant In contrast existing approaches covariance additive noise need modeled estimated regular parameter needed The proposed approach implemented using multilayer neural networks order improve performance separation New associated online unsupervised adaptive learning rules also developed The effectiveness proposed method illustrated computer simulations',\n",
              " 'Source separation consists recovering set n independent signals n observed instantaneous mixtures signals possibly corrupted additive noise Many source separation algorithms use second order information whitening operation reduces non trivial part separation determining unitary matrix Most show kind invariance property exploited predict general results performance Our first contribution exhibit lower bound performance terms accuracy separation This bound independent algorithm iid case distribution source signals Second show performance invariant algorithms depends mixing matrix noise level specific way A consequence low noise levels performance depend mixture distribution sources via function characteristic given source separation algorithm',\n",
              " 'In paper neural network approach reconstruction natural highly correlated images linear additive mixture proposed A multilayer architecture local online learning rules developed solve problem blind separation sources The main motivation using multilayer network instead singlelayer one improve performance robustness separation applying simple local learning rule biologically plausible Moreover architecture onchip learning relatively easy implementable using VLSI electronic circuits Furthermore enables extraction source signals sequentially one starting strongest signal finishing weakest one The experimental part focuses separating highly correlated human faces mixture additive noise unknown number sources',\n",
              " 'We study online learning algorithms predict combining predictions several subordinate prediction algorithms sometimes called experts These simple algorithms belong multiplicative weights family algorithms The performance algorithms degrades logarithmically number experts making particularly useful applications number experts large However applications text categorization often natural experts abstain making predictions instances We show transform algorithms assume experts always awake algorithms require assumption We also show derive corresponding loss bounds Our method general applied large family online learning algorithms We also give applications various prediction models including decision graphs switching experts',\n",
              " 'When dealing classification problems current ILP systems often lag behind stateoftheart attributional learners Part blame ascribed much larger hypothesis space therefore thoroughly explored However sometimes due fact ILP systems take account probabilistic aspects hypotheses classifying unseen examples This paper proposes We developed naive Bayesian classifier within ILPR first order learner The learner uses clever RELIEF based heuristic able detect strong dependencies within literal space dependencies exist We conducted series experiments artificial realworld data sets The results show combination ILPR together naive Bayesian classifier sometimes significantly improves classification unseen instances measured classification accuracy average information score',\n",
              " 'Process simulation emerged valuable tool process design analysis operation In work extend capabilities iterated linear programming LP dealing problems encountered dynamic nonsmooth process simulation A previously developed LP method refined addition new descent strategy combines line search trust region approach This adds stability efficiency method The LP method advantage naturally dealing profile bounds well This demonstrated avoid computational difficulties arise iterates going physically unrealistic regions A new method treatment discontinuities occurring dynamic simulation problems also presented paper The method ensures event occurred within time interval consideration detected one event occurs detected one indeed earliest one A specific class implicitly discontinuous process simulation problems phase equilibrium calculations also looked A new formulation introduced solve multiphase problems fl To correspondence addressed emailbieglercmuedu',\n",
              " 'A frequently observed difficulty application genetic algorithms domain optimization arises premature convergence In order preserve genotype diversity develop new model autoadaptive behavior individuals In model population member active individual assumes sociallike behavior patterns Different individuals living population assume different patterns By moving hierarchy social states individuals change behavior Changes social state controlled arguments plausibility These arguments implemented rule set massivelyparallel genetic algorithm Computational experiments largescale job shop benchmark problems show results new approach dominate ordinary genetic algorithm significantly',\n",
              " 'Proben collection problems neural network learning realm pattern classification function approximation plus set rules conventions carrying benchmark tests similar problems Proben contains data sets different domains All datasets represent realistic problems could called diagnosis tasks one consist real world data The datasets presented simple format using attribute representation directly used neural network training Along datasets Proben defines set rules conduct document neural network benchmarking The purpose problem rule collection give researchers easy access data evaluation algorithms networks make direct comparison published results feasible This report describes datasets benchmarking rules It also gives basic performance measures indicating difficulty various problems These measures used baselines comparison',\n",
              " 'This paper presents NeuroChess program learns play chess final outcome games NeuroChess learns chess board evaluation functions represented artificial neural networks It integrates inductive neural network learning temporal differencing variant explanationbased learning Performance results illustrate strengths weaknesses approach',\n",
              " 'Some important factors play major role determining performances CBR CaseBased Reasoning system complexity accuracy retrieval phase Both flat memory inductive approaches suffer serious drawbacks In first approach search time increases dealing large scale memory base second one modification case memory becomes complex sophisticated architecture In paper show construct simple efficient indexing system structure The idea construct case hierarchy two levels memory lower level contains cases organised groups similar cases upper level contains prototypes prototype represents one group cases This smaller memory used retrieval phase Prototype construction achieved means incremental prototypebased NN Neural Network We show mode CBRNN coupling preprocessing one neural network serves indexing system',\n",
              " 'Developing ability recognize landmark visual image robots current location fundamental problem robotics We consider problem PAClearning concept class geometric patterns target geometric pattern configuration k points real line Each instance configuration n points real line labeled according whether visually resembles target pattern To capture notion visual resemblance use Hausdorff metric Informally two geometric patterns P Q resemble Hausdorff metric every point one pattern close point pattern We relate concept class geometric patterns landmark recognition problem present polynomialtime algorithm PAClearns class onedimensional geometric patterns We also present experimental results algorithm performs',\n",
              " 'The concept measure functions generalization performance suggested This concept provides alternative way selecting evaluating learned models classifiers In addition makes possible state learning problem computational problem The known prior metaknowledge problem domain captured measure function possible combination training set classifier assigns value describing good classifier The computational problem find classifier maximizing measure function We argue measure functions great value practical applications Besides tool model selection force us make explicit relevant prior knowledge learning problem hand ii provide deeper understanding existing algorithms iii help us construction problemspecific algorithms We illustrate last point suggesting novel algorithm based incremental search classifier optimizes given measure function',\n",
              " 'Recurrent attractor networks offer many advantages feedforward networks modeling psychological phenomena Their dynamic nature allows capture time course cognitive processing learned weights may often easily interpreted soft constraints representational components Perhaps significant feature networks however ability facilitate generalization enforcing well formedness constraints intermediate output representations Attractor networks learn systematic regularities well formed representations exposure small number examples said possess articulated attractors This paper investigates conditions articulated attractors arise recurrent networks trained using variants backpropagation The results computational experiments demonstrate structured attractors spontaneously appear emergence systematicity appropriate error signal presented directly recurrent processing elements We show however distal error signals backpropagated intervening weights pose serious problems networks kind We present simulation results discuss reasons difficulty suggest directions future attempts surmount',\n",
              " 'Induced decision trees extensivelyresearched solution classification tasks For many practical tasks trees produced treegeneration algorithms comprehensible users due size complexity Although many tree induction algorithms shown produce simpler comprehensible trees data structures derived trees good classification accuracy tree simplification usually secondary concern relative accuracy attempt made survey literature perspective simplification We present framework organizes approaches tree simplification summarize critique approaches within framework The purpose survey provide researchers practitioners concise overview treesimplification approaches insight relative capabilities In final discussion briefly describe empirical findings discuss application tree induction algorithms case retrieval casebased reasoning systems',\n",
              " 'In paper propose monitor Markov chain sampler using cusum path plot chosen dimensional summary statistic We argue cusum path plot bring effectively sequential plot aspects Markov sampler tell user quickly slowly sampler moving around sample space direction summary statistic The proposal illustrated four examples represent situations cusum path plot works well well Moreover rigorous analysis given one examples We conclude cusum path plot effective tool convergence diagnostics Markov sampler comparing different Markov samplers',\n",
              " 'This paper gives precise easy compute bounds convergence time Gibbs sampler used Bayesian image reconstruction For sampling Gibbs distribution without presence external field bounds N number pixels obtained proportionality constant easy calculate Some key words Bayesian image restoration Convergence Gibbs sampler Ising model Markov chain Monte Carlo',\n",
              " 'c flMIT Media Lab Perceptual Computing Learning Common Sense Technical Report nov revised jun Abstract We present methods coupling hidden Markov models hmms model systems multiple interacting processes The resulting models multiple state variables temporally coupled via matrices conditional probabilities We introduce deterministic OT CN approximation maximum posterior MAP state estimation enables fast classification parameter estimation via expectation maximization An Nheads dynamic programming algorithm samples highest probability paths compact state trellis minimizing upper bound cross entropy full combinatoric dynamic programming problem The complexity OT CN C chains N states apiece observing T data points compared OT N C naive Cartesian product exact state clustering stochastic Monte Carlo methods applied inference problem In several experiments examining training time model likelihoods classification accuracy robustness initial conditions coupled hmms compared favorably conventional hmms energybased approaches coupled inference chains We demonstrate compare algorithms synthetic real data including interpretation video',\n",
              " 'SUMMARY The paper describes Bayesian analysis agricultural field experiments topic received little previous attention despite vast frequentist literature Adoption Bayesian paradigm simplifies interpretation results especially ranking selection Also complex formulations analyzed comparative ease using Markov chain Monte Carlo methods A key ingredient approach need spatial representations unobserved fertility patterns This discussed detail Problems caused outliers jumps fertility tackled via hierarchicalt formulations may find use contexts The paper includes three analyses variety trials yield one example involving binary data none entirely straightforward Some comparisons frequentist analyses made The datasets available httpwwwstatdukeeduhigdontrialsdatahtml',\n",
              " 'We show paper continuous state space Markov chains rigorously discretized finite Markov chains The idea subsample continuous chain renewal times related small sets control discretization Once finite Markov chain derived MCMC output general convergence properties finite state spaces exploited convergence assessment several directions Our choice based divergence criterion derived Kemeny Snell first evaluated parallel chains stopping time implemented efficiently two parallel chains using Birkhoffs pointwise ergodic theorem stopping rules The performance criterion illustrated three standard examples',\n",
              " 'Markov chain Monte Carlo MCMC samplers proved remarkably popular tools Bayesian computation However problems arise application density interest high dimensional strongly correlated In circumstances sampler may slow traverse state space mixing poor In article offer partial solution problem The state space Markov chain augmented accommodate multiple chains parallel Updates individual chains based around genetic style crossover operator acting parent states drawn population chains This process makes efficient use gradient information implicitly encoded within distribution states across population Empirical studies support claim crossover operator acting parallel population chains improves mixing This illustrated example sampling high dimensional posterior probability density complex predictive model By adopting latent variable approach methodology extended deal variable selection model averaging high dimensions This illustrated example knot selection spline interpolant',\n",
              " 'MIT Computational Cognitive Science Technical Report Abstract We describe variational approximation methods efficient probabilistic reasoning applying methods problem diagnostic inference QMRDT database The QMRDT database largescale belief network based statistical expert knowledge internal medicine The size complexity network render exact probabilistic diagnosis infeasible small set cases This hindered development QMR DT network practical diagnostic tool hindered researchers exploring critiquing diagnostic behavior QMR In paper describe variational approximation methods applied QMR network resulting fast diagnostic inference We evaluate accuracy methods set standard diagnostic cases compare stochastic sampling methods',\n",
              " 'The effects neural networks topology performance well known yet question finding optimal configurations automatically remains largely open This paper proposes solution problem RBF networks A self optimising approach driven evolutionary strategy taken The algorithm uses output information computationally efficient approximation RBF networks optimise Kmeans clustering process coevolving two determinant parameters networks layout number centroids centroids positions Empirical results demonstrate promise',\n",
              " 'This paper describes hybrid methodology integrates genetic algorithms decision tree learning order evolve useful subsets discriminatory features recognizing complex visual concepts A genetic algorithm GA used search space possible subsets large set candidate discrimination features Candidate feature subsets evaluated using C decisiontree learning algorithm produce decision tree based given features using limited amount training data The classification performance resulting decision tree unseen testing data used fitness underlying feature subset Experimental results presented show increasing amount learning significantly improves feature set evolution difficult visual recognition problems involving satellite facial image data In addition also report extent subtle aspects Baldwin effect exhibited system',\n",
              " 'In paper examine behavior humancomputer system crisis response As one instance crisis management describe task responding spills fires involving hazardous materials We describe INCA intelligent assistant planning scheduling domain relation human users We focus INCAs strategy retrieving case case library seeding initial schedule helping user adapt seed We also present three hypotheses behavior mixedinitiative system experiments designed test The results suggest approach leads faster response development usergenerated automaticallygenerated schedules without sacrificing solution quality',\n",
              " 'Given adequate simulation model task environment payoff function measures quality partially successful plans competitionbased heuristics genetic algorithms develop high performance reactive rules interesting sequential decision tasks We previously described implemented system called SAMUEL learning reactive plans shown system successfully learn rules laboratory scale tactical problem In paper describe method deriving explanations justify success empirically derived rule sets The method consists inferring plausible subgoals explaining reactive rules trigger sequence actions ie stra tegy satisfy subgoals',\n",
              " 'Machine learning valuable tool improving flexibility efficiency robot applications Many approaches applying machine learning robotics known Some approaches enhance robots highlevel processing planning capabilities Other approaches enhance lowlevel processing control basic actions In contrast approach presented paper uses machine learning enhancing link lowlevel representations sensing action highlevel representation planning The aim facilitate communication robot human user A hierarchy concepts learned route records mobile robot Perception action combined every level ie concepts perceptually anchored The relational learning algorithm grdt developed completely searches hypothesis space restricted rule schemata user defines terms grammars',\n",
              " 'We motivate use convergence diagnostic techniques Markov Chain Monte Carlo algorithms review various methods proposed MCMC literature A common notation established method discussed particular emphasis implementational issues possible extensions The methods compared terms interpretability applicability recommendations provided particular classes problems',\n",
              " 'For target tracking task handheld camera anthropomorphic OSCARrobot manipulator track object moves arbitrarily table The desired camerajoint mapping approximated feedforward neural network Through use time derivatives position object manipulator controller inherently predict next position moving target object In paper several anticipative controllers described successfully applied track moving object',\n",
              " 'Covariance information help algorithm search predictive causal models estimate strengths causal relationships This information discarded conditional independence constraints identified usual contemporary causal induction algorithms Our fbd algorithm combines covariance information effective heuristic build predictive causal models We demonstrate fbd accurate efficient In one experiment assess fbds ability find best predictors variables another compare performance using many measures Pearl Vermas ic algorithm And although fbd based multiple linear regression cite evidence performs well problems difficult regression algorithms',\n",
              " 'The problem learning decision rules sequential tasks addressed focusing problem learning tactical decision rules simple flight simulator The learning method relies notion competition employs genetic algorithms search space decision policies Several experiments presented address issues arising differences simulation model learning occurs target environment decision rules ultimately tested',\n",
              " 'The inductive learning problem consists learning concept given examples nonexamples concept To perform learning task inductive learning algorithms bias learning method Here discuss biasing learning method use previously learned concepts domain These learned concepts highlight useful information concepts domain We describe transference bias present MFOCL Horn clause relational learning algorithm utilizes bias learn multiple concepts We provide preliminary empirical evaluation show effects biasing previous information noisefree noisy data',\n",
              " 'Choosing architecture neural network one important problems making neural networks practically useful accounts applications usually sweep details carpet How many hidden units needed Should weight decay used much What type output units chosen And We address issues within framework statistical theory model This paper principally concerned architecture selection issues feedforward neural networks also known multilayer perceptrons Many issues arise selecting radial basis function networks recurrent networks widely These problems occur much wider context within statistics applied statisticians selecting combining models decades Two recent discussions References discuss neural networks statistical perspective choice provides number workable approximate answers',\n",
              " 'We propose modeltheoretic definition causation show contrary common folklore genuine causal influences distinguished spurious covariations following standard norms inductive reasoning We also establish complete characterization conditions distinction possible Finally provide prooftheoretical procedure inductive causation show large class data structures effective algorithms exist uncover direction causal influences defined',\n",
              " 'This study deals alltoall broadcast CNS We determine lower bound run time present algorithm meeting bound Since study points bottleneck network interface also analyze performance alternative interface designs Our analyses based run time model network',\n",
              " 'Automated decision making often complicated complexity knowledge involved Much complexity arises contextsensitive variations underlying phenomena We propose framework representing descriptive contextsensitive knowledge Our approach attempts integrate categorical uncertain knowledge network formalism This paper outlines basic representation constructs examines expressiveness efficiency discusses potential applications framework',\n",
              " 'We discuss number methods estimating standard error predicted values multilayer perceptron These methods include delta method based Hessian bootstrap estimators sandwich estimator The methods described compared number examples We find bootstrap methods perform best partly capture variability due choice starting weights',\n",
              " 'Discrete mixtures normal distributions widely used modeling amplitude fluctuations electrical potentials synapses human animal nervous systems The usual framework independent data values j arising j j x n j means j come discrete prior G unknown x n j observed x j j n gaussian noise terms A practically important development associated statistical methods issue nonnormality noise terms often norm rather exception neurological context We recently developed models based convolutions Dirichlet process mixtures problems Explicitly model noise data values x j arising Dirichlet process mixture normals addition modeling location prior G Dirichlet process This induces Dirichlet mixture mixtures normals whose analysis may developed using Gibbs sampling techniques We discuss models analysis illustrate context neurological response analysis',\n",
              " 'Neural controllers able position handheld camera DOF anthropomorphic OSCARrobot manipulator object arbitrary placed table The desired camerajoint mapping approximated feedforward neural networks However object moving manipulator lags behind required time preprocess visual information move manipulator Through use time derivatives position object manipulator controller inherently predict next position object In paper several predictive controllers proposed successfully applied track moving object',\n",
              " 'This paper overviews AA Adaptive Algorithm model ASOCS Adaptive Self Organizing Concurrent Systems approach It also presents promising empirical generalization results AA actual data AA topologically dynamic network grows fit problem learned AA generalizes selforganizing fashion network seeks find features discriminate concepts Convergence training set guaranteed bounded linearly time',\n",
              " 'This communication deals source separation problem consists separation noisy mixture independent sources without priori knowledge mixture coefficients In paper consider maximum likelihood ML approach discrete source signals known probability distributions An important feature ML approach Gaussian noise covariance matrix additive noise treated parameter Hence necessary know model spatial structure noise Another striking feature offered case discrete sources mild assumptions possible separate sources sensors In paper consider maximization likelihood via ExpectationMaximization EM algorithm',\n",
              " 'If robust statistical model developed classify health system wellknown Taylor series approximation technique forms basis diagnosticrecovery procedure initiated systems health degrades fails altogether This procedure determines ranked set probable causes degraded health state used prioritized checklist isolating system anomalies quantifying corrective action The diagnosticrecovery procedure applicable classifier known robust applied neural network traditional parametric pattern classifiers generated supervised learning procedure empirical riskbenefit measure optimized We describe procedure mathematically demonstrate ability detect diagnose causes faults NASAs Deep Space Communications Complex Goldstone California',\n",
              " 'Case combination difficult problem Case Based Reasoning subcases often exhibit conflicts merged together In previous work formalized case combination representing case constraint satisfaction problem used minimum conflicts algorithm systematically synthesize global solution However also found instances problem minimum conflicts algorithm perform case combination efficiently In paper describe situations initially retrieved cases easily adaptable propose method improve case adaptability genetic algorithm We introduce fitness function maintains much retrieved case information possible also perturbing subsolution allow subsequent case combination proceed efficiently',\n",
              " 'The Dynamic Constraint Satisfaction Problem DCSP formalism gaining attention valuable often necessary extension static CSP framework Dynamic Constraint Satisfaction enables CSP techniques applied extensively since applied domains set constraints variables involved problem evolves time At time CaseBased Reasoning CBR community working techniques reuse existing solutions solving new problems We observed dynamic constraint satisfaction matches closely casebased reasoning process case adaptation These observations emerged previous work combining CBR CSP achieve constraintbased adaptation This paper summarizes previous results describes similarity challenges facing DCSP case adaptation shows CSP CBR together begin address chal lenges',\n",
              " 'Prior knowledge bias regarding concept speed task learning Probably Approximately Correct PAC learning mathematical model concept learning used quantify speed due different forms bias learning Thus far PAC learning mostly used analyze syntactic bias limiting concepts conjunctions boolean prepositions This paper demonstrates PAC learning also used analyze semantic bias domain theory concept learned The key idea view hypothesis space PAC learning consistent prior knowledge syntactic semantic In particular paper presents PAC analysis determinations type relevance knowledge The results analysis reveal crisp distinctions relations among different determinations illustrate usefulness analysis based PAC model',\n",
              " 'Computational models natural systems often contain free parameters must set optimize predictive accuracy models This process called calibrationcan viewed form supervised learning presence prior knowledge In view fixed aspects model constitute prior knowledge goal learn values free parameters We report series attempts learn parameter values global vegetation model called MAPSS Mapped AtmospherePlantSoil System developed collaborator Ron Neilson Standard machine learning methods work MAPSS constraints introduced structure model create difficult nonlinear optimization problem We developed new divideandconquer approach subsets parameters calibrated others held constant This approach succeeds possible select training examples exercise portions model',\n",
              " 'The paper considers situation learners testing set contains close approximations cases appear training set Such cases considered virtual seens since approximately seen learner Generalisation measures take account frequency virtual seens may misleading The paper shows NN algorithm used derive normalising baseline generalisation statistics The normalisation process demonstrated though application Holtes study generalisation performance R algorithm tested C commonly used datasets',\n",
              " 'Initial Results Abstract Conversational casebased reasoning CBR systems incrementally extract query description userdirected conversation advertised ease use However designing large case libraries good performance ie precision querying efficiency difficult CBR vendors provide guidelines designing libraries manually guidelines difficult apply We describe automated inductive approach revises conversational case libraries increase conformance design guidelines Revision increased performance three conversational case libraries',\n",
              " 'Diagnosis process identifying disorders machine patient considering history symptoms signs Starting possible initial information new information requested sequential manner diagnosis made precise It thus missing data problem since everything known We model joint probability distribution data case database mixture models Model parameters estimated EM algorithm gives additional benefit missing data database also handled correctly Request new information refine diagnosis performed using maximum utility principle decision theory Since system based machine learning domain independent An example using heart disease database presented',\n",
              " 'We give example neural net without hidden layers sigmoid transfer function together training set binary vectors sum squared errors regarded function weights local minimum global minimum The example consists set training instances four weights threshold learnt We know substantially smaller binary examples exist',\n",
              " 'The multiple extension problem arises default theory use different subsets defaults propose different mutually incompatible answers queries This paper presents algorithm uses set observations learn credulous version default theory essentially optimally accurate In detail associate given default theory set related credulous theories R fR g R uses total ordering defaults determine single answer return query Our goal select credulous theory highest expected accuracy R expected accuracy probability answer produces query correspond correctly world Unfortunately theorys expected accuracy depends distribution queries usually known Moreover task identifying optimal R opt R even given distribution information intractable This paper presents method OptAcc sidesteps problems using set samples estimate unknown distribution hillclimbing local optimum In particular given parameters ffi gt OptAcc produces R oa R whose expected accuracy probability least ffi within local optimum Appeared ECAI Workshop Theoretical Foundations Knowledge Representation Reasoning',\n",
              " 'Compression information important concept theory learning We argue hypothesis inherent compression pressure towards short elegant general solutions genetic programming system variable length evolutionary algorithms This pressure becomes visible size complexity solutions measured without noneffective code segments called introns The built parsimony pressure effects complex fitness functions crossover probability generality maximum depth length solutions explicit parsimony granularity fitness function initialization depth length modularization Some effects positive negative In work provide basis analysis effects suggestions overcome negative implications order obtain balance needed successful evolution An empirical investigation supports hypothesis also presented',\n",
              " 'Wilsons recent XCS classifier system forms complete mappings payoff environment reinforcement learning tradition thanks accuracy based fitness According Wilsons Generalization Hypothesis XCS tendency towards generalization With XCS Optimality Hypothesis I suggest XCS systems evolve optimal populations representations populations accurately map inputaction pairs payoff predictions using smallest possible set nonoverlapping classifiers The ability XCS evolve optimal populations boolean multiplexer problems demonstrated using condensation technique evolutionary search suspended setting crossover mutation rates zero Condensation automatically triggered selfmonitoring performance statistics entire learning process terminated autotermination Combined techniques allow classifier system evolve optimal representations boolean functions without form supervision',\n",
              " 'Current rule induction systems eg CN typically rely separate conquer strategy learning rule stilluncovered examples This results dwindling number examples available learning successive rules adversely affecting systems accuracy An alternative learn rules simultaneously using entire training set This approach implemented Rise system Empirical comparison Rise CN suggests conquering without separating performs similarly counterpart simple domains achieves increasingly substantial gains accuracy domain difficulty grows',\n",
              " 'A genetic programming method investigated optimizing architecture connection weights multilayer feedforward neural networks The genotype network represented tree whose depth width dynamically adapted particular application specifically defined genetic operators The weights trained nextascent hillclimbing search A new fitness function proposed quantifies principle Occams razor It makes optimal tradeoff error fitting ability parsimony network We discuss results two problems differing complexity study convergence scaling properties algorithm',\n",
              " 'The performance neural network categorizes facial expressions compared human subjects set experiments using interpolated imagery The experiments human subjects neural networks make use interpolations facial expressions Pictures Facial Affect Database Ekman Friesen The difference materials used human subjects experiments Young et al materials manner interpolated images constructed imagequality morphs versus pixel averages Nevertheless neural network accurately captures categorical nature human responses showing sharp transitions labeling images along interpolated sequence Crucially demonstration categorical perception Harnad model shows highest discrimination transition images crossover point The model also captures shape reaction time curves human subjects along sequences Finally network matches human subjects judgements expressions mixed images The main failing model intrusions neutral responses transitions seen human subjects We attribute difference difference pixel average stimuli image quality morph stimuli These results show simple neural network classifier access biological constraints presumably imposed human emotion processor whose access surrounding culture category labels placed American subjects facial expressions nevertheless simulate fairly well human responses emotional expressions',\n",
              " 'The article hand discusses tool automatic generation structured models complex dynamic processes means genetic programming In contrast techniques use genetic programming find appropriate arithmetic expression order describe inputoutput behaviour process tool based block oriented approach transparent description signal paths A short survey techniques computer based system identification given basic concept SMOG Structured MOdel Generator described Furthermore latest extensions system presented detail including automatically defined submodels quali tative fitness criteria',\n",
              " 'We examine role hyperplane ranking search performed simple genetic algorithm We also develop metric measuring degree ranking exists respect static measurements taken directly function well measurement dynamic ranking hyperplanes genetic search We show degree dynamic ranking induced simple genetic algorithm highly correlated degree static ranking inherent function especially initial genera tions search',\n",
              " 'Genetic algorithms rely two genetic operators crossover mutation Although exists large body conventional wisdom concerning roles crossover mutation roles captured theoretical fashion For example never theoretically shown mutation sense less powerful crossover vice versa This paper provides answers questions theoretically demonstrating important characteristics operator captured',\n",
              " 'In recent paper Friedman Geiger Goldszmidt introduced classifier based Bayesian networks called Tree Augmented Naive Bayes TAN outperforms naive Bayes performs competitively C stateoftheart methods This classifier several advantages including robustness polynomial computational complexity One limitation TAN classifier applies discrete attributes thus continuous attributes must prediscretized In paper extend TAN deal continuous attributes directly via parametric eg Gaussians semiparametric eg mixture Gaussians conditional probabilities The result classifier represent combine discrete continuous attributes In addition propose new method takes advantage modeling language Bayesian networks order represent attributes discrete continuous form simultaneously use versions classification This automates process deciding form attribute relevant classification task It also avoids commitment either discretized semiparametric form since different attributes may correlate better one version Our empirical results show latter method usually achieves classification performance good better either purely discrete purely continuous TAN models',\n",
              " 'This paper considers problem representing complex systems evolve stochastically time Dynamic Bayesian networks provide compact representation stochastic processes Unfortunately often unwieldy since explicitly model complex organizational structure many real life systems fact processes typically composed several interacting subprocesses turn decomposed We propose hierarchically structured representation language extends dynamic Bayesian networks objectoriented Bayesian network framework show language allows us describe systems natural modular way Our language supports natural representation certain system characteristics hard capture using traditional frameworks For example allows us represent systems processes evolve different rate others systems processes interact intermittently We provide simple inference mechanism representation via translation Bayesian networks suggest ways inference algorithm exploit additional structure encoded representation',\n",
              " 'It often difficult predict optimal neural network size particular application Constructive destructive methods add subtract neurons layers connections etc might offer solution problem We prove one method Recurrent Cascade Correlation due topology fundamental limitations representation thus learning capabilities It represent monotone ie sigmoid hardthreshold activation functions certain finite state automata We give preliminary approach get around limitations devising simple constructive training method adds neurons training still preserving powerful fullyrecurrent structure We illustrate approach simulations learn many examples regular grammars',\n",
              " 'Indexing cases important topic MemoryBased ReasoningMBR One key problem assign weights attributes cases Although several weighting methods proposed methods handle numeric attributes directly necessary discretize numeric values classification Furthermore existing methods theoretical background little said optimality We propose new weighting method based statistical technique called Quantification Method II It handle numeric symbolic attributes framework Generated attribute weights optimal sense maximize ratio variance classes variance cases Experiments several benchmark tests show many cases method obtains higher accuracies weighting methods The results also indicate distinguish relevant attributes irrelevant ones tolerate noisy data',\n",
              " 'A General Result Stabilization Linear Systems Using Bounded Controls ABSTRACT We present two constructions controllers globally stabilize linear systems subject control saturation We allow essentially arbitrary saturation functions The conditions imposed system obvious necessary ones namely eigenvalues uncontrolled system positive real part standard stabilizability rank condition hold One constructions terms neuralnetwork type onehidden layer architecture one terms cascades linear maps saturations',\n",
              " 'This paper proposes classification scheme based integration multiple Ensembles ANNs It demonstrated classification problem seismic signals Natural Earthquakes must distinguished seismic signals Artificial Explosions A Redundant Classification Environment consists several Ensembles Neural Networks created trained Bootstrap Sample Sets using various data representations architectures The ANNs within Ensembles aggregated Bagging Ensembles integrated nonlinearly signal adaptive manner using posterior confidence measure based agreement variance within Ensembles The proposed Integrated Classification Machine achieved correct classifications seismic test data Cross Validation evaluations comparisons indicate integration collection ANNs Ensembles robust way handling high dimensional problems complex nonstationary signal space current Seismic Classification problem',\n",
              " 'This first draft chapter Bayesian Biostatistics edited Donald A Berry Darlene K Strangl Adrian E Raftery Professor Statistics Sociology Department Statistics GN University Washington Seattle WA USA Sylvia Richardson Directeur de Recherche INSERM Unite avenue Paul Vaillant Couturier Villejuif CEDEX France Rafterys research supported ONR contract NJ Ministere de la Recherche et de lEspace Paris Universite de Paris VI INRIA Rocquencourt France Raftery thanks latter two institutions Paul Deheuvels Gilles Celeux hearty hospitality Paris sabbatical part chapter written The authors grateful Christine Montfort excellent research assistance Mariette Gerber Michel Chavance David Madigan helpful discussions',\n",
              " 'ProductionManufacturing scheduling typically involves acquisition user optimization preferences The illstructuredness problem space desired objectives make practical scheduling problems difficult formalize costly solve especially problem configurations user optimization preferences change time This paper advocates incremental revision framework improving schedule quality incorporating user dynamically changing preferences CaseBased Reasoning Our implemented system called CABINS records situationdependent tradeoffs consequences result schedule revision guide schedule improvement The preliminary experimental results show CABINS able effectively capture user static dynamic preferences known system exist implicitly extensional manner case base',\n",
              " 'Realization autonomous behavior mobile robots using fuzzy logic control requires formulation rules collectively responsible necessary levels intelligence Such collection rules conveniently decomposed efficiently implemented hierarchy fuzzybehaviors This article describes done using behaviorbased architecture A behavior hierarchy mechanisms control decisionmaking described In addition approach behavior coordination described emphasis evolution fuzzy coordination rules using genetic programming GP paradigm Both conventional GP steadystate GP applied evolve fuzzybehavior sensorbased goalseeking The usefulness behavior hierarchy partial design GP evident performance results simulated autonomous navigation',\n",
              " 'We present distribution model binary vectors called influence combination model show model used basis unsupervised learning algorithms feature selection The model closely related Harmonium model defined Smolensky RMCh In first part paper analyze properties distribution representation scheme We show arbitrary distributions binary vectors approximated combination model We show weight vectors model interpreted high order correlation patterns among input bits We compare combination model mixture model principle component analysis In second part paper present two algorithms learning combination model examples The first algorithm based gradient ascent Here give closed form gradient significantly easier compute corresponding gradient general Boltzmann machine The second learning algorithm greedy method creates hidden units computes weights one time This method variant projection pursuit density estimation In third part paper give experimental results learning methods synthetic data natural data handwritten digit images',\n",
              " 'Complex group behavior arises social insects colonies integration actions simple redundant individual insects Adler Gordon Oster Wilson Furthermore colony act information center expedite foraging Brown We apply lessons natural systems model collective action memory computational agent society Collective action expedite search combinatorial optimization problems Dorigo et al Collective memory improve learning multiagent systems Garland Alterman Our collective adaptation integrates simplicity collective action pattern detection collective memory significantly improve gathering processing knowledge As test role society information center examine ability society distribute task allocation without omnipotent centralized control',\n",
              " 'We study annealed theories learning boolean functions using concept class finite cardinality The naive annealed theory used derive universal learning curve bound zero temperature learning similar inverse square root bound VapnikChervonenkis theory Tighter nonuniversal learning curve bounds also derived A refined annealed theory leads still tighter bounds cases similar results previously obtained using onestep replica symmetry breaking',\n",
              " 'This article describes numerical method may used efficiently locate track underwater sonar targets nearfield bearing range estimation case large passive arrays The approach used requirement priori knowledge source uses limited information receiver array shape The role sensor position uncertainty consequence targets always nearfield analysed problems associated manipulation large matrices inherent conventional eigenvalue type algorithms noted A simpler numerical approach presented reduces problem search optimization When using method location target corresponds finding position maximum weighted sum output sensors Since search procedure dealt using modern stochastic optimization methods genetic algorithm operational requirement acceptable accuracy achieved real time usually met The array studied consists elements positioned along flexible cable towed behind ship sensors giving effective aperture For long array far field assumption used beamforming algorithms longer appropriate The waves emitted targets considered curved rather plane It shown simulated data significant noise',\n",
              " 'This paper introduces new type intelligent agent called constructive inductionbased learning agent CILA This agent differs adaptive agents ability learn assist user task also incrementally adapt knowledge representation space better fit given learning task The agents ability autonomously make problemoriented modifications originally given representation space due constructive induction CI learning method Selective induction SI learning methods agents based methods rely good representation space A good representation space misclassification noise intercorrelated attributes irrelevant attributes Our proposed CILA methods overcoming problems In agent domains poor representations CIbased learning agent learn accurate rules useful SIbased learning agent This paper gives architecture CIbased learning agent gives empirical comparison CI SI set six abstract domains involving DNFtype disjunctive normal form descriptions',\n",
              " 'We propose method decreasing computational complexity selforganising maps The method uses partitioning neurons disjoint clusters Teaching neurons occurs clusterbasis instead neuronbasis For teaching Nneuron network N samples computational complexity decreases ON N ON log N Furthermore introduce measure amount order selforganising map show introduced algorithm behaves well original algorithm',\n",
              " 'Inductive learning relational domains shown intractable general Many approaches task suggested nevertheless way restrict hypothesis space searched They roughly divided two groups datadriven restriction encoded algorithm modelbased restrictions made less explicit form declarative bias This paper describes Incy inductive learner seeks combine aspects approaches Incy initially datadriven using examples background knowledge put forth specialize hypotheses based connectivity data hand It modeldriven hypotheses abstracted rule models used control decisions datadriven phase modelguided induction Key Words Inductive learning relational domains cooperation datadriven modelguided methods implicit declarative bias',\n",
              " 'The problem learning decision rules sequential tasks addressed focusing problem learning tactical plans simple flight simulator plane must avoid missile The learning method relies notion competition employs genetic algorithms search space decision policies Experiments presented address issues arising differences simulation model learning occurs target environment decision rules ultimately tested Specifically either model target environment may contain noise These experiments examine effect learning tactical plans without noise testing plans noisy environment effect learning plans noisy simulator testing plans noisefree environment Empirical results show best result obtained training model closely matches target environment using training environment noisy target environment better using using training environment less noise target environment',\n",
              " 'Navigation obstacles mine fields important capability autonomous underwater vehicles One way produce robust behavior perform projective planning However realtime performance critical requirement navigation What needed truly autonomous vehicle robust reactive rules perform well wide variety situations also achieve realtime performance In work SAMUEL learning system based genetic algorithms used learn highperformance reactive strategies navigation collision avoidance',\n",
              " 'In paper introduce investigate mathematically rigorous theory learning curves based ideas statistical mechanics The advantage theory wellestablished VapnikChervonenkis theory bounds considerably tighter many cases also reflective true behavior functional form learning curves This behavior often exhibit dramatic properties phase transitions well power law asymptotics explained VC theory The disadvantages theory application requires knowledge input distribution limited far finite cardinality function classes We illustrate results many concrete examples learning curve bounds derived theory',\n",
              " 'Although considerable interest shown language inference automata induction using recurrent neural networks success models mostly limited regular languages We previously demonstrated Neural Network Pushdown Automaton NNPDA model capable learning deterministic contextfree languages eg n b n parenthesis languages examples However learning task computationally intensive In paper discuss ways priori knowledge task data could used efficient learning We also observe knowledge often experimental prerequisite learning nontrivial languages eg n b n cb',\n",
              " 'Connectionist learning procedures presented sigmoid noisyOR varieties stochastic feedforward network These networks class belief networks used expert systems They represent probability distribution set visible variables using hidden variables express correlations Conditional probability distributions exhibited stochastic simulation use tasks classification Learning empirical data done via gradientascent method analogous used Boltzmann machines due feedforward nature connections negative phase Boltzmann machine learning unnecessary Experimental results show result learning sigmoid feedforward network faster Boltzmann machine These networks advantages Boltzmann machines pattern classification decision making applications provide link work connectionist learning work representation expert knowledge',\n",
              " 'Genetic Programming GP uses variable size representations programs Size becomes important interesting emergent property structures evolved GP The size programs controlling controlled factor GP search Size influences efficiency search process related generality solutions This paper analyzes size generality issues standard GP GP using subroutines addresses question whether analysis help control search process We relate size generalization modularity issues programs evolved control agent dynamic nondeterministic environment exemplified PacMan game',\n",
              " 'We present definition cause effect terms decisiontheoretic primitives thereby provide principled foundation causal reasoning Our definition departs traditional view causation causal assertions may vary set decisions available We argue approach provides added clarity notion cause Also paper examine encoding causal relationships directed acyclic graphs We describe special class influence diagrams canonical form show relationship Pearls representation cause effect Finally show canonical form facilitates counterfactual reasoning',\n",
              " 'Fuzzy logic evolutionary computation proven convenient tools handling realworld uncertainty designing control systems respectively An approach presented combines attributes paradigms purpose developing intelligent control systems The potential genetic programming paradigm GP learning rules use fuzzy logic controllers FLCs evaluated focussing problem discovering controller mobile robot path tracking Performance results incomplete rulebases compare favorably complete FLC designed usual trialanderror approach A constrained syntactic representation supported structurepreserving genetic operators also introduced',\n",
              " 'We review estimation interval censoring models including nonparametric estimation distribution function estimation regression models In nonparametric setting describe computational procedures asymptotic properties nonparametric maximum likelihood estimators In regression setting focus proportional hazards proportional odds accelerated failure time semiparametric regression models Particular emphasis given calculation Fisher information regression parameters We also discuss computation regression parameter estimators via profile likelihood maximization semiparametric likelihood distributional results maximum likelihood estimators estimation asymptotic variances Some problems open questions also reviewed',\n",
              " 'Genetic programming distinguished evolutionary algorithms uses tree representations variable size instead linear strings fixed length The flexible representation scheme important allows underlying structure data discovered automatically One primary difficulty however solutions may grow big without improvement generalization ability In paper investigate fundamental relationship performance complexity evolved structures The essence parsimony problem demonstrated empirically analyzing error landscapes programs evolved neural network synthesis We consider genetic programming statistical inference problem apply Bayesian modelcomparison framework introduce class fitness functions error complexity terms An adaptive learning method presented automatically balances modelcomplexity factor evolve parsimonious programs without losing diversity population needed achieving desired training accuracy The effectiveness approach empirically shown induction sigmapi neural networks solving realworld medical diagnosis problem well benchmark tasks',\n",
              " 'Dynamic probabilistic networks DPNs useful tool modeling complex stochastic processes The simplest inference task DPNs monitoring computing posterior distribution state variables time step given observations time Recursive constantspace algorithms wellknown monitoring DPNs models This paper concerned hindsight computing posterior distribution given past future observations Hindsight essential subtask learning DPN models data Existing algorithms hindsight DPNs use OSN space time N total length observation sequence S state space size time step They therefore impractical hindsight complex models long observation sequences This paper presents OS log N space OSN log N time hindsight algorithm We demonstrates effectiveness algorithm two realworld DPN learning problems We also discuss possibility OSspace OSN time algorithm',\n",
              " 'Learning methods vary optimism pessimism regard informativeness learned knowledge Pessimism implicit hypothesis testing wish draw cautious conclusions experimental evidence However paper demonstrates optimism utility derived rules may preferred bias learning systems We examine continuum naive pessimism naive optimism context decision tree learner prunes rules based stringent ie pessimistic weak ie optimistic tests significance Our experimental results indicate cases optimism preferred particularly cases sparse training data high noise This work generalizes earlier findings Fisher Schlimmer Schaffer discuss relevance unsupervised learning small disjuncts issues',\n",
              " 'We already shown extracting longterm dependencies sequential data difficult deterministic dynamical systems recurrent networks probabilistic models hidden Markov models HMMs inputoutput hidden Markov models IOHMMs In practice avoid problem researchers used domain specific apriori knowledge give meaning hidden state variables representing past context In paper propose use general type apriori knowledge namely temporal dependencies structured hierarchically This implies longterm dependencies represented variables long time scale This principle applied recurrent network includes delays multiple time scales Experiments confirm advantages structures A similar approach proposed HMMs IOHMMs',\n",
              " 'We present new algorithm finding low complexity neural networks high generalization capability The algorithm searches flat minimum error function A flat minimum large connected region weightspace error remains approximately constant An MDLbased Bayesian argument suggests flat minima correspond simple networks low expected overfitting The argument based Gibbs algorithm variant novel way splitting generalization error underfitting overfitting error Unlike many previous approaches require Gaussian assumptions depend good weight prior instead prior inputoutput functions thus taking account net architecture training set Although algorithm requires computation second order derivatives backprops order complexity Automatically effectively prunes units weights input lines Various experiments feedforward recurrent nets described In application stock market prediction flat minimum search outperforms conventional backprop weight decay optimal brain surgeon optimal brain damage We also provide pseudo code algorithm omitted NCversion',\n",
              " 'This paper describes method improving comprehensibility accuracy generality reactive plans A reactive plan set reactive rules Our method involves two phases formulate explanations execution traces generate new reactive rules explanations Since explanation phase previously described primary focus paper rule generation phase This latter phase consists taking subset explanations using explanations generate set new reactive rules add original set The particular subset explanations chosen yields rules provide new domain knowledge handling knowledge gaps original rule set The original rule set complimentary manner provides expertise fill gaps domain knowledge provided new rules incomplete',\n",
              " 'Technical Report AI May Abstract A new method developing good valueordering strategies constraint satisfaction search presented Using evolutionary technique called SANE individual neurons evolve cooperate form neural network problemspecific knowledge discovered results better valueordering decisions based problemgeneral heuristics A neural network evolved chronological backtrack search decide ordering cars resourcelimited assembly line The network required backtracks random ordering backtracks maximization future options heuristic The SANE approach extend well domains heuristic information either difficult discover problemspecific',\n",
              " 'Conversational casebased reasoning CBR shells eg Inferences CBR Express commercially successful tools supporting development help desk related applications In contrast rulebased expert systems capture knowledge cases rather problematic rules incrementally extended However rather eliminate knowledge engineering bottleneck refocus case engineering task carefully authoring cases according library design guidelines ensure good performance Designing complex libraries according guidelines difficult software needed assist users case authoring We describe approach revising case libraries according design guidelines implementation Clire empirical results showing conditions approach improve conversational CBR performance',\n",
              " 'We present computational model movement skill learning The types skills addressed class trajectory following movements involving multiple accelerations decelerations changes direction lasting seconds These skills acquired observation improved practice We also review speedaccuracy tradeoffone robust phenomena human motor behavior We present two speedaccuracy tradeoff experiments models performance fits human behavior quite well',\n",
              " 'Reinforcement Learning class problems autonomous agent acting given environment improves behavior progressively maximizing function calculated basis succession scalar responses received environment Qlearning classifier systems CS two methods among used solve reinforcement learning problems Notwithstanding popularity shared goal past often considered two different models In paper first show classifier system restricted sharp simplification called discounted max simple classifier system D MAX VSCS boils tabular Qlearning It follows D MAX VSCS converges optimal policy proved Watkins Dayan draw profit results experimental theoretical works dedicated improve Qlearning facilitate use concrete applications In second part paper show three restrictions need impose CS deriving equivalence Qlearning internal states dont care symbols structural changes turn essential recently rediscovered reprogrammed Qlearning adepts Eventually sketch similarities among ongoing work within research contexts The main contribution paper therefore make explicit strong similarities existing Qlearning classifier systems show experience gained research within one domain useful direct future research one',\n",
              " 'Some recent work investigated dichotomy compact coding using dimensionality reduction sparse distributed coding context understanding biological information processing We introduce artificial neural network self organises basis simple Hebbian learning negative feedback activation show capable forming compact codings data distributions also identifying filters sensitive sparse distributed codes The network extremely simple biological relevance investigated via response set images typical everyday life However analysis networks identification filter sparse coding reveals coding may globally optimal exists innate limiting factor transcended',\n",
              " 'For classes concepts defined certain classes analytic functions depending n parameters nonempty open sets samples length n shattered A slighly weaker result also proved piecewiseanalytic functions The special case neural networks discussed',\n",
              " 'Two important goals evaluation AI theory model assess merit design decisions performance implemented computer system analyze impact performance system faces problem domains different characteristics This particularly difficult casebased reasoning systems systems typically complex tasks domains operate We present methodology evaluation casebased reasoning systems systematic empirical experimentation range system configurations environmental conditions coupled rigorous statistical analysis results experiments This methodology enables us understand behavior system terms theory design computational model select best system configuration given domain predict system behave response changing domain problem characteristics A case study multistrategy casebased reinforcement learning system performs autonomous robotic navigation presented example',\n",
              " 'For casebased reasoner use knowledge flexibly must equipped powerful case adapter A casebased reasoner cope variation form problems given extent cases memory efficiently adapted fit wide range new situations In paper address task adapting abstract knowledge planning fit specific planning situations First show adapting abstract cases requires reconciling incommensurate representations planning situations Next describe representation system memory organization adaptation process tailored requirement Our approach implemented brainstormer planner takes abstract advice',\n",
              " 'The maximum likelihood estimator MLE proportional hazards model current status data studied It shown MLE regression parameter asymptotically normal p nconvergence rate achieves information bound even though MLE baseline cumulative hazard function converges n rate Estimation asymptotic variance matrix MLE regression parameter also considered To prove main results also establish general theorem showing MLE finite dimensional parameter class semiparametric models asymptotically efficient even though MLE infinite dimensional parameter converges rate slower The results illustrated applying data set tumoriginicity study Introduction In many survival analysis problems interested p',\n",
              " 'PO Box Wellington New Zealand Tel Fax Internet TechReportscompvuwacnz Technical Report CSTR October Abstract People often give advice telling stories Stories recommend course action exemplify general conditions recommendation appropriate A computational model advice taking using stories must address two related problems determining storys recommendations appropriateness conditions showing obtain new situation In paper present efficient solution second problem based caching results first Our proposal implemented brainstormer planner takes abstract advice',\n",
              " 'There increasing need efficient estimation mixture distributions especially following explosion use modelling tools many applied fields We propose paper Bayesian noninformative approach estimation normal mixtures relies reparameterisation secondary components mixture terms divergence main component As well providing intuitively appealing representation modelling stage reparameterisation important bearing prior distribution performance MCMC algorithms We compare two possible reparameterisations extending Mengersen Robert show reparameterisation link secondary components together associated poor convergence properties MCMC algorithms',\n",
              " 'A WorldWide Web WWW server implemented Common LISP order facilitate exploratory programming global hypermedia domain provide access complex research programs particularly artificial intelligence systems The server initially used provide interfaces document retrieval email servers More advanced applications include interfaces systems inductive rule learning naturallanguage question answering Continuing research seeks fully generalize automatic formprocessing techniques developed email servers operate seamlessly Web The conclusions argue presentationbased interfaces sophisticated form processing moved clients order reduce load servers provide advanced interaction models users',\n",
              " 'Survival analysis concerned finding models predict survival patients assess efficacy clinical treatment A key part modelbuilding process selection predictor variables It standard use stepwise procedure guided series significance tests select single model make inference conditionally selected model However ignores model uncertainty substantial We review standard Bayesian model averaging solution problem extend survival analysis introducing partial Bayes factors Cox proportional hazards model In two examples taking account model uncertainty enhances predictive performance extent could clinically useful',\n",
              " 'We propose bootstrapbased method model averaging selection focuses training points left individual bootstrap samples This information used estimate optimal weighting factors combining estimates different bootstrap samples also finding best subsets linear model setting These proposals provide alternatives Bayesian approaches model averaging selection requiring less computation fewer subjective choices',\n",
              " 'Technical Report December Statistics Department University California Berkeley CA Abstract The theory behind success adaptive reweighting combining algorithms arcing Adaboost Freund Schapire others reducing generalization error well understood By formulating prediction classification regression game one player makes selection instances training set convex linear combination predictors finite set existing arcing algorithms shown algorithms finding good game strategies An optimal game strategy finds combined predictor minimizes maximum error training set A bound generalization error combined predictors terms maximum error proven sharper bounds date Arcing algorithms described converge optimal strategy Schapire etal offered explanation Adaboost works terms ability reduce margin Comparing Adaboost optimal arcing algorithm shows explanation valid answer lies elsewhere In situation VCtype bounds misleading Some empirical results given explore situation',\n",
              " 'Conversational casebased reasoning CCBR form interactive casebased reasoning users input partial problem description text The CCBR system responds ranked solution display lists solutions stored cases whose problem descriptions best match users ranked question display lists unanswered questions cases Users interact displays either refining problem description answering selected questions selecting solution apply CCBR systems support dialogue inferencing infer answers questions implied problem description Otherwise questions listed user believes already answered The standard approach dialogue inferencing allows case library designers insert rules define implications problem description unanswered questions However approach imposes substantial knowledge engineering requirements We introduce alternative approach whereby intelligent assistant guides designer defining model case library implication rules derived We detail approach benefits explain supported integration ParkaDB fast relational database system We evaluate approach context CCBR system named NaCoDAE This paper appeared AAAI Spring Symposium Multimodal Reasoning NCARAI TR AIC We introduce integrated reasoning approach modelbased reasoning component performs important inferencing role conversational casebased reasoning CCBR system named NaCoDAE Breslow Aha Figure CCBR form casebased reasoning users enter text queries describing problem system assists eliciting refinements Aha Breslow Cases three components',\n",
              " 'A readonce formula boolean formula variable occurs Such formulas also called formulas boolean trees This paper treats problem exactly identifying unknown readonce formula using specific kinds queries The main results polynomial time algorithm exact identification monotone readonce formulas using membership queries polynomial time algorithm exact identification general readonce formulas using equivalence membership queries protocol based notion minimally adequate teacher Our results improve Valiants previous results readonce formulas We also show polynomial time algorithm using membership queries equivalence queries exactly identify readonce formulas',\n",
              " 'Recursive AutoAssociative Memory RAAM structures show promise general representation vehicle uses distributed patterns However training often difficult explains least part relatively small networks studied We show technique transforming collection hierarchical structures set training patterns sequential RAAM effectively trained using simple Elmanstyle recurrent network Tr aining produces set distributed patterns corresponding structures',\n",
              " 'We propose analyze distribution learning algorithm variable memory length Markov processes These processes described subclass probabilistic finite automata name Probabilistic Finite Suffix Automata The learning algorithm motivated real applications manmachine interaction handwriting speech recognition Conventionally used fixed memory Markov hidden Markov models either severe practical theoretical drawbacks Though general hardness results known learning distributions generated sources similar structure prove algorithm indeed efficiently learn distributions generated restricted sources In Particular show KLdivergence distribution generated target source distribution generated hypothesis made small high confidence polynomial time sample complexity We demonstrate applicability algorithm learning structure natural English text using hy pothesis correction corrupted text',\n",
              " 'The clausal discovery engine claudien presented claudien discovers regularities data representative inductive logic programming paradigm As represents data regularities means first order clausal theories Because search space clausal theories larger attribute value representation claudien also accepts input declarative specification language bias determines set syntactically wellformed regularities Whereas papers claudien focuss semantics logical problem specification claudien discovery algorithm PAClearning aspects paper wants illustrate power resulting technique In order achieve aim show claudien used learn integrity constraints databases functional dependencies determinations properties sequences mixed quantitative qualitative laws reverse engineering classification rules',\n",
              " 'In context machine learning examples paper deals problem estimating quality attributes without dependencies Greedy search prevents current inductive machine learning algorithms detect significant dependencies attributes Recently Kira Rendell developed RELIEF algorithm estimating quality attributes able detect dependencies attributes We show strong relation RELIEFs estimates impurity functions usually used heuristic guidance inductive learning algorithms We propose use RELIEFF extended version RELIEF instead myopic impurity functions We reimplemented Assistant system top induction decision trees using RELIEFF estimator attributes selection step The algorithm tested several artificial several real world problems Results show advantage presented approach inductive learning open wide rang possibilities using RELIEFF',\n",
              " 'An investigation dynamics Genetic Programming applied chaotic time series prediction reported An interesting characteristic adaptive search techniques ability perform well many problem domains failing others Because Genetic Programmings flexible tree structure particular problem represented myriad forms These representations variegated effects search performance Therefore aspect fundamental engineering significance find representation acted upon Genetic Programming operators optimizes search performance We discover case chaotic time series prediction representation commonly used domain yield optimal solutions Instead find population converges onto one accurately replicating tree trees explored To correct premature convergence make simple modification crossover operator In paper review previous work GP time series prediction pointing anomalous result related overlearning report improvement effected modified crossover operator',\n",
              " 'Current ILP algorithms typically use variants extensions greedy search This prevents detect significant relationships training objects Instead myopic impurity functions propose use heuristic based RELIEF guidance ILP algorithms At step ILPR system heuristic used determine beam candidate literals The beam used exhaustive search potentially good conjunction literals From efficiency point view introduce interesting declarative bias enables us keep growth training set introducing new variables within linear bounds linear respect clause length This bias prohibits crossreferencing variables variable dependency tree The resulting system tested various artificial problems The advantages deficiencies approach discussed',\n",
              " 'Instead myopic impurity functions propose use ReliefF heuristic guidance inductive learning algorithms The basic algoritm RELIEF developed Kira Rendell Kira Rendell ab able efficiently solve classification problems involving highly dependent attributes parity problems However sensitive noise unable deal incomplete data multiclass regression problems continuous class We extended RELIEF several directions The extended algorithm ReliefF able deal noisy incomplete data used multiclass problems regressional variant RReliefF deal regression problems Another area application inductive logic programming ILP instead myopic measures ReliefF used estimate utility literals theory construction',\n",
              " 'In paper present TDLeaf variation TD algorithm enables used conjunction minimax search We present experiments chess backgammon demonstrate utility provide comparisons TD another less radical variant TDdirected In particular chess program KnightCap used TDLeaf learn evaluation function playing Free Internet Chess Server FICS ficsonenetnet It improved rating rating games We discuss reasons success relationship results Tesauros results backgammon',\n",
              " 'This paper deals asymptotic properties MetropolisHastings algorithm distribution interest unknown approximated sequential estimator density We prove simple conditions rate convergence MetropolisHastings algorithm sequential estimator latter introduced reversible measure MetropolisHastings Kernel This problem natural extension previous work new simulated annealing algorithm sequential estimator energy',\n",
              " 'We explored two approaches recognizing faces across changes pose First developed representation face images based independent component analysis ICA compared principal component analysis PCA representation face recognition The ICA basis vectors data set spatially local PCA basis vectors ICA representation greater invariance changes pose Second present model development viewpoint invariant responses faces visual experience biological system The temporal continuity natural visual experience incorporated attractor network model Hebbian learning following lowpass temporal filter unit activities When combined temporal filter basic Hebbian update rule became generalization Griniasty et al associates temporally proximal input patterns basins attraction The system acquired rep resentations faces largely independent pose',\n",
              " 'Problems regression smoothing curve fitting addressed via predictive inference flexible class mixture models Multidimensional density estimation using Dirichlet mixture models provides theoretical basis semiparametric regression methods fitted regression functions may deduced means conditional predictive distributions These Bayesian regression functions features similar generalised kernel regression estimates formal analysis addresses problems multivariate smoothing parameter estimation assessment uncertainties regression functions naturally Computations based multidimensional versions existing Markov chain simulation analysis univariate Dirichlet mixture models',\n",
              " 'On basis early theoretical empirical studies genetic algorithms typically used point crossover operators standard mechanisms implementing recombination However number recent studies primarily empirical nature shown benefits crossover operators involving higher number crossover points From traditional theoretical point view surprising new results relate uniform crossover involves average L crossover points strings length L In paper extend existing theoretical results attempt provide broader explanatory predictive theory role multipoint crossover genetic algorithms In particular extend traditional disruption analysis include two general forms multipoint crossover npoint crossover uniform crossover We also analyze two aspects multipoint crossover operators namely recombination potential exploratory power The results analysis provide much clearer view role multipoint crossover genetic algorithms The implications results implementation issues performance discussed several directions research suggested',\n",
              " 'In paper discuss methodological issues using class neural networks called Mixture Density Networks MDN discriminant analysis MDN models advantage rigorous probabilistic interpretation proven viable alternative classification procedure discrete domains We address classification interpretive aspects discriminant analysis compare approach traditional method linear discrimin ants implemented standard statistical packages We show MDN approach adopted performs well aspects Many observations made restricted particular case hand applicable applications discriminant analysis educational research fl URL httpwwwcsHelsinkiFIresearchcosco',\n",
              " 'Satisfiability SAT refers task finding truth assignment makes arbitrary boolean expression true This paper compares simulated annealing algorithm SASAT GSAT Selman et al greedy algorithm solving satisfiability problems GSAT solve problem instances extremely difficult traditional satisfiability algorithms Results suggest SASAT scales better number variables increases solving least many hard SAT problems less effort The paper presents ablation study helps explain relative advantage SASAT GSAT Next improvement basic SASAT algorithm examined based random walk implemented GSAT Selman et al Finally examine performance SASAT test suite satisfiability problems produced DIMACS challenge',\n",
              " 'We present comparison errorbased entropybased methods discretization continuous features Our study includes extensive empirical comparison well analysis scenarios error minimization may inappropriate discretization criterion We present discretization method based C decision tree algorithm compare existing entropybased discretization algorithm employs Minimum Description Length Principle recently proposed errorbased technique We evaluate discretization methods respect C NaiveBayesian classifiers datasets UCI repository analyze computational complexity method Our results indicate entropybased MDL heuristic outperforms error minimization average We analyze shortcomings errorbased approaches comparison entropybased methods',\n",
              " 'Here show similar construction multipleoutput systems modifications Let A B C discretetime signlinear system state space IR n p outputs Perform change A n fi n invertible A n fi n nilpotent If A B reachable pair A C observable pair minimal sense signlinear system inputoutput behavior dimension least n But n lt n det A observable hence canonical Let us find another system necessarily signlinear inputoutput behavior canonical Let relative degree ith row Markov sequence A minf pg Let initial state x There difference case smallest relative degree greater equal n case lt n Roughly speaking n outputs signlinear system give us information sign Cx sign CAx sign CA x first outputs sys tem After use inputs outputs learn x first n components x When lt n may able use controls learn x last n components x time n nilpotency A finally Lemma Two states x z indistinguishable x z Proof In case n equations x z equality The first output terms exactly terms So equalities satisfied first output terms coincide x z input Equality everything first n components equivalent first n output terms coinciding x z since jth row qth output initial state x example either sign c j A q x j gt q sign c j A q x A j j u q j j q case may use control u q j identify c j A q x using Remark',\n",
              " 'So applying Corollary second equation conclude From get jgy n k obtain jy n k From see righthand side bounded Since system A k b jyj ev N Now suppose lim sup jytj gt Then jyj ev Since j kyj Ljyj using obtain jyj ev L ffi Note righthand side inequality trivial since know jyj ev From ffi N ffi gt N However see still holds So established cases From get jyj ev Taking lim sup lefthand side N ffi ie N ffi Substituting get jyj ev ffi jyj ev N ffi So take N N L conclusion follows To complete proof need deal general case gt inputs This done induction proof omitted Fuller AT In large stability relay saturated control systems linear controllers Int J Control Gutman PO P Hagander A new design constrained controllers linear systems IEEE Transactions Automat Contr AC Kosut RL Design linear systems saturating linear control bounded states IEEE Trans Autom Control AC Krikelis NJ SK Barkas Design tracking systems subject actuator saturation integrator windup Int J Control Schmitendorf WE BR Barmish Null controllability linear systems constrained controls SIAM J Control Opt Slemrod M Feedback stabilization linear control system Hilbert space Math Control Signals Systems Slotine JJE W Li Applied Nonlinear Control PrenticeHall Englewood Cliffs Sontag ED An algebraic approach bounded controllability linear systems Int J Control Sontag ED Remarks stabilization inputtostate stability Proc IEEE CDC Tampa Dec IEEE Publications pp Sontag ED Mathematical Control Theory Deterministic Finite Dimensional Systems Springer New York Sontag ED HJ Sussmann Nonlinear output feedback design linear systems saturating controls Proc IEEE CDC Honolulu Dec IEEE Publications pp Sussmann H J Y Yang On stabilizability multiple integrators means bounded feedback controls Proc IEEE CDC Brighton UK Dec IEEE Publications Teel AR Global stabilization restricted tracking multiple integrators bounded controls Systems Control Letters Yang Y HJ Sussmann ED Sontag Stabilization linear systems bounded controls Proc June NOLCOS Bordeaux M Fliess Ed IFAC Publications pp Yang Y Global Stabilization Linear Systems Bounded Feedback Ph D Thesis Mathematics Department Rutgers University jyj ev M ffi',\n",
              " 'Gross error detection plays vital role parameter estimation data reconciliation dynamic steady state systems In particular recent advances process optimization allow data reconciliation dynamic systems appropriate problem formulations need considered Data errors due either miscalibrated faulty sensors random events nonrepresentative underlying statistical distribution induce heavy biases parameter estimates reconciled data In paper concentrate robust estimators exploratory statistical methods allow us detect gross errors data reconciliation performed These robust methods property insensitive departures ideal statistical distributions therefore insensitive presence outliers Once regression done outliers detected readily using exploratory statistical techniques An important feature performance optimization algorithm uniqueness reconciled data ability classify variables according observability redundancy properties Here observable variable unmeasured quantity estimated measured variables physical model nonredundant variable measured variable estimated measurements Variable classification used aid design instrumentation schemes In',\n",
              " 'Many significant realworld classification tasks involve large number categories arranged hierarchical structure example classifying documents subject categories library congress scheme classifying worldwideweb documents topic hierarchies We investigate potential benefits using given hierarchy base classes learn accurate multicategory classifiers domains First consider possibility exploiting class hierarchy prior knowledge help one learn accurate classifier We explore benefits learning categorydiscriminants hard topdown fashion compare soft approach shares training data among sibling categories In verify hierarchies potential improve prediction accuracy But argue reasons subtle Sometimes improvement using hierarchy happens constrain expressiveness hypothesis class appropriate manner However various controlled experiments show cases performance advantage associated using hierarchy really seem due prior knowledge encodes',\n",
              " 'Many algorithms inferring decision tree data involve twophase process First large decision tree grown typically ends overfitting data To reduce overfitting second phase tree pruned using one number available methods The final tree output used classification test data In paper suggest alternative approach pruning phase Using given unpruned decision tree present new method making predictions test data prove algorithms performance much worse precise technical sense predictions made best reasonably small pruning given decision tree Thus procedure guaranteed competitive terms quality predictions pruning algorithm We prove procedure efficient highly robust Our method viewed synthesis two previously studied techniques First apply CesaBianchi et als results predicting using expert advice view pruning expert obtain algorithm provably low prediction loss computationally infeasible Next generalize apply method developed Buntine Willems Shtarkov Tjalkens derive efficient implementation procedure',\n",
              " 'We present efficient algorithm PAClearning general class geometric concepts lt fixed More specifically let T set halfspaces Let x x x arbitrary point lt With T associate boolean indicator function I x x halfspace The concept class C study consists concepts formed boolean function I I T This concept class much general geometric concept class known PAClearnable Our results easily extended efficiently learn boolean combination polynomial number concepts selected concept class C lt given VCdimension C dependence thus constant constant polynomial time algorithm determine concept C consistent given set labeled examples We also present statistical query version algorithm tolerate random classification noise noise rate strictly less Finally present generalization standard net result Haussler Welzl apply give alternative noisetolerant algorithm based geometric subdivisions',\n",
              " 'In work develop new criteria perform pessimistic decision tree pruning Our method theoretically sound based theoretical concepts uniform convergence VapnikChervonenkis dimension We show criteria well motivated theory side performs well practice The accuracy new criteria comparable current method used C',\n",
              " 'In paper study performance probabilistic networks context protein sequence analysis molecular biology Specifically report results initial experiments applying framework problem protein secondary structure prediction One main advantages probabilistic approach describe ability perform detailed experiments experiment different models We easily perform local substitutions mutations measure probabilistically effect global structure Windowbased methods support experimentation readily Our method efficient training prediction important order able perform many experiments different networks We believe probabilistic methods comparable methods prediction quality In addition predictions generated methods precise quantitative semantics shared classification methods Specifically causal statistical independence assumptions made explicit networks thereby allowing biologists study experiment different causal models convenient manner',\n",
              " 'In paper prove sanitycheck bounds error leaveoneout crossvalidation estimate generalization error bounds showing worstcase error estimate much worse training error estimate The name sanitycheck refers fact although often expect leaveoneout estimate perform considerably better training error estimate seeking assurance performance considerably worse Perhaps surprisingly assurance given limited cases prior literature crossvalidation Any nontrivial bound error leaveoneout must rely notion algorithmic stability Previous bounds relied rather strong notion hypothesis stability whose application primarily limited nearestneighbor local algorithms Here introduce new weaker notion error stability apply obtain sanitycheck bounds leaveoneout classes learning algorithms including training error minimization procedures Bayesian algorithms We also provide lower bounds demonstrating necessity form error stability proving bounds error leaveoneout estimate fact training error minimization algorithms worst case bounds must still depend VapnikChervonenkis dimension hypothesis class',\n",
              " 'This paper describes program observes behaviour actors simulated world uses observations guides conducting experiments An experiment sequence actions carried actor order support weaken case generalisation concept A generalisation attempted program observes state world similar previous state A partial matching algorithm used find substitutions enable two states unified The generalisation two states unifier',\n",
              " 'Widespread adoption Genetic Programming techniques domainindependent problem solving tool depends good underlying software structure A system presented mirrors conceptual makeup GP system Consisting loose collection software components strict interface definitions roles system maximises flexibility minimises effort applied new problem domain',\n",
              " 'Coevolution competitive species provides interesting testbed study role adaptive behavior provides unpredictable dynamic environments In paper experimentally investigate arguments coevolution different adaptive protean behaviors competing species predators preys Both species implemented simulated mobile robots Kheperas infrared proximity sensors predator additional vision module whereas prey maximum speed set twice predator Different types variability life neurocontrollers architecture genetic length compared It shown simple forms proteanism affect coevolutionary dynamics preys rather exploit noisy controllers generate random trajectories whereas predators benefit directionalchange controllers improve pursuit behavior',\n",
              " 'The calculation second derivatives required recent training analysis techniques connectionist networks elimination superfluous weights estimation confidence intervals weights network outputs We review develop exact approximate algorithms calculating second derivatives For networks jwj weights simply writing full matrix second derivatives requires Ojwj operations For networks radial basis units sigmoid units exact calculation necessary intermediate terms requires order h backwardforwardpropagation passes h number hidden units network We also review compare three approximations ignoring components second derivative numerical differentiation scoring Our algorithms apply arbitrary activation functions networks error functions instance connections skip layers radial basis functions crossentropy error Softmax units etc',\n",
              " 'In paper describe principles problem solving analogy applied domain functional program synthesis For reason treat programs syntactical structures We discuss two different methods handle structures graph metric determining distance two program schemes b Structure Mapping Engine existing system examine analogical processing Furthermore show experimental results discuss',\n",
              " 'There many ways learning system generalize training set data This paper presents several generalization styles using prototypes attempt provide accurate generalization training set data wide variety applications These generalization styles efficient terms time space lend well massively parallel architectures Empirical results generalizing several realworld applications given results indicate prototype styles generalization presented potential provide accurate generalization many applications',\n",
              " 'This paper provides exposition recent research regarding systemtheoretic aspects continuoustime recurrent dynamic neural networks sigmoidal activation functions The class systems introduced discussed result cited regarding universal approximation properties Known characterizations controllability observability parameter identifiability reviewed well result minimality Facts regarding computational power recurrent nets also mentioned fl Supported part US Air Force Grant AFOSR',\n",
              " 'Most Artificial Neural Networks ANNs fixed topology learning often suffer number shortcomings result ANNs use dynamic topologies shown ability overcome many problems Adaptive Self Organizing Concurrent Systems ASOCS class learning models inherently dynamic topologies This paper introduces LocationIndependent Transformations LITs general strategy implementing learning models use dynamic topologies efficiently parallel hardware A LIT creates set locationindependent nodes node computes part network output independent nodes using local information This type transformation allows efficient support adding deleting nodes dynamically learning In particular paper presents Location Independent ASOCS LIA model LIT ASOCS Adaptive Algorithm The description LIA gives formal definitions LIA algorithms Because LIA implements basic ASOCS mechanisms definitions provide formal description basic ASOCS mechanisms general addition LIA',\n",
              " 'Reinforcement learning algorithms often work finding functions satisfy Bellman equation This yields optimal solution prediction Markov chains controlling Markov decision process MDP finite number states actions This approach also frequently applied Markov chains MDPs infinite states We show case Bellman equation may multiple solutions many lead erroneous predictions policies Baird Algorithms conditions presented guarantee single optimal solution Bellman equation',\n",
              " 'A major issue casebasedsystems retrieving appropriate cases memory solve given problem This implies case indexed appropriately stored memory A casebased system dynamic stores cases reuse needs learn indices new knowledge system designers envision knowledge Irrespective type indexing structural functional hierarchical organization case memory raises two distinct related issues index learning learning indexing vocabulary learning right level generalization In paper show structurebehaviorfunction SBF models help learning structural indices design cases domain physical devices The SBF model design provides functional causal explanation structure design delivers function We describe SBF model design provides vocabulary structural indexing design cases inductive biases index generalization We discuss modelbased learning integrated similaritybased learning uses prior design cases learning level index generalization',\n",
              " 'We present representational format observed movements The representation temporal structure relating components single complex movement We also present OXBOW unsupervised learning system constructs classes movements Empirical results indicate system builds abstract movement concepts appropriate component structure allowing predict latter portions partially observed movement',\n",
              " 'Constructive induction divides problem learning inductive hypothesis two intertwined searches onefor best representation space twofor best hypothesis space In datadriven constructive induction DCI learning system searches better representation space analyzing input examples data The presented datadriven constructive induction method combines AQtype learning algorithm two classes representation space improvement operators constructors destructors The implemented system AQDCI experimentally applied GNP prediction problem using World Bank database The results show decision rules learned AQDCI outperformed rules learned original representation space predictive accuracy rule simplicity',\n",
              " 'We report novel possibility extracting small subset data base contains information necessary solve given classification task using Support Vector Algorithm train three different types handwritten digit classifiers observed types classifiers construct decision surface strongly overlapping small subsets data base This finding opens possibility compressing data bases significantly disposing data important solution given task In addition show theory allows us predict classifier best generalization ability based solely performance training set characteristics learning machines This finding important cases amount available data limited',\n",
              " 'Physical variables orientation line visual field location body space coded activity levels populations neurons Reconstruction decoding inverse problem physical variables estimated observed neural activity Reconstruction useful first quantifying much information physical variables present population second providing insight brain might use distributed representations solving related computational problems visual object recognition spatial navigation Two classes reconstruction methods namely probabilistic Bayesian methods basis function methods discussed They include important existing methods special cases population vector coding optimal linear estimation template matching As representative example reconstruction problem different methods applied multielectrode spike train data hippocampal place cells freely moving rats The reconstruction accuracy trajectories rats compared different methods Bayesian methods especially accurate continuity constraint enforced best errors within factor two informationtheoretic limit accurate reconstruction comparable intrinsic experimental errors position tracking In addition reconstruction analysis uncovered interesting aspects place cell activity tendency erratic jumps reconstructed trajectory animal stopped running In general theoretical values minimal achievable reconstruction errors quantify accurately physical variable encoded neuronal population sense mean square error regardless method used reading information One related result theoretical accuracy independent width Gaussian tuning function two dimensions Finally reconstruction methods considered paper implemented unified neural network architecture brain could feasibly use solve related problems',\n",
              " 'The headdirection HD cells found limbic system freely moving rats represent instantaneous head direction animal horizontal plane regardless location animal The internal direction represented cells uses selfmotion information inertially based updating familiar visual landmarks calibration Here model dynamics HD cell ensemble presented The stability localized static activity profile network dynamic shift mechanism explained naturally synaptic weight distribution components even odd symmetry respectively Under symmetric weights symmetric reciprocal connections stable activity profile close known directional tuning curves emerge By adding slight asymmetry weights activity profile shift continuously without disturbances shape shift speed accurately controlled strength oddweight component The generic formulation shift mechanism determined uniquely within current theoretical framework The attractor dynamics system ensures modalityindependence internal representation facilitates correction cumulative error putative localview detectors The model offers specific onedimensional example computational mechanism truly worldcentered representation derived observercentered sensory inputs integrating selfmotion information',\n",
              " 'We present biasvariance decomposition expected misclassification rate commonly used loss function supervised classification learning The biasvariance decomposition quadratic loss functions well known serves important tool analyzing learning algorithms yet decomposition offered commonly used zeroone misclassification loss functions recent work Kong Dietterich Breiman Their decomposition suffers major shortcomings though eg potentially negative variance decomposition avoids We show practice naive frequencybased estimation decomposition terms biased show correct bias We illustrate decomposition various algorithms datasets UCI repository',\n",
              " 'The dominant component computational burden solving n n trivial p r b l e w h evolutionary algorithms task measuring fitness individual generation evolving population The advent r p l r e c n f g u r b l e f e l programmable gate arrays FPGAs idea evolvable hardware opens possiblity e b n g individual evolving population hardware purpose accelerating timeconsuming fitness evaluation task This paper demonstrates massive parallelism rapidly r e c n f g u r b l e X l n x X C FPGA exploited accelerate computationally burdensome fitness evaluation task genetic programming The work done Virtual Computing Corporations lowcost HOTS expansion board PC type computers A step sorter evolved two fewer steps sorting network described OConnor Nelson patent sorting networks number steps minimal sorter devised Floyd Knuth subsequent patent',\n",
              " 'A theoretically justifiable fast finite successive linear approximation algorithm proposed obtaining parsimonious solution corrupted linear system Ax b p corruption p due noise error measurement The proposed linearprogrammingbased algorithm finds solution x parametrically minimizing number nonzero elements x error k Ax b p k Numerical tests signalprocessingbased example indicate proposed method comparable method parametrically minimizes norm solution x error k Ax b p k methods superior orders magnitude solutions obtained least squares well combinatorially choosing optimal solution specific number nonzero elements',\n",
              " 'In natural visual experience different views object face tend appear close temporal proximity A set simulations presented demonstrate viewpoint invariant representations faces developed visual experience capturing temporal relationships among input patterns The simulations explored interaction temporal smoothing activity signals Hebbian learning Foldiak feedforward system recurrent system The recurrent system generalization Hopfield network lowpass temporal filter unit activities Following training sequences graylevel images faces changed pose multiple views given face fell basin attraction system acquired representations faces approximately viewpoint invariant',\n",
              " 'Neural networks successfully applied wide range supervised unsupervised learning applications Neuralnetwork methods commonly used datamining tasks however often produce incomprehensible models require long training times In article describe neuralnetwork learning algorithms able produce comprehensible models require excessive training times Specifically discuss two classes approaches data mining neural networks The first type approach often called rule extraction involves extracting symbolic models trained neural networks The second approach directly learn simple easytounderstand networks We argue given current state art neuralnetwork methods deserve place tool boxes datamining specialists',\n",
              " 'A statistical theory overtraining proposed The analysis treats realizable stochastic neural networks trained KullbackLeibler loss asymptotic case It shown asymptotic gain generalization error small perform early stopping even access optimal stopping time Considering crossvalidation stopping answer question In ratio examples divided training testing sets order obtain optimum performance In nonasymptotic region crossvalidated early stopping always decreases generalization error Our large scale simulations done CM nice agreement analytical findings',\n",
              " 'Mathematical programming approaches three fundamental problems described feature selection clustering robust representation The feature selection problem considered discriminating two sets recognizing irrelevant redundant features suppressing This creates lean model often generalizes better new unseen data Computational results real data confirm improved generalization leaner models Clustering exemplified unsupervised learning patterns clusters may exist given database useful tool knowledge discovery databases KDD A mathematical programming formulation problem proposed theoretically justifiable computationally implementable finite number steps A resulting kMedian Algorithm utilized discover useful survival curves breast cancer patients medical database Robust representation concerned minimizing trained model degradation applied new problems A novel approach proposed purposely tolerates small error training process order avoid overfitting data may contain errors Examples applications concepts given',\n",
              " 'Concept learning viewed search space concept descriptions The hypothesis language determines search space In standard inductive learning algorithms structure search space determined generalizationspecialization operators Algorithms perform locally optimal search using hillclimbing andor beamsearch strategy To overcome limitation concept learning viewed stochastic search space concept descriptions The proposed stochastic search method based simulated annealing known successful means solving combinatorial optimization problems The stochastic search method implemented rule learning system ATRIS based compact efficient representation problem appropriate operators structuring search space Furthermore heuristic pruning search space method enables also handling imperfect data The paper introduces stochastic search method describes ATRIS learning algorithm gives results experiments',\n",
              " 'The increasing availability finelygrained parallel architectures resulted variety evolutionary algorithms EAs population spatially distributed local selection algorithms operate parallel small overlapping neighborhoods The effects design choices regarding particular type local selection algorithm well size shape neighborhood particularly well understood generally tested empirically In paper extend techniques used formally analyze selection methods sequential EAs apply local neighborhood models resulting much clearer understanding effects neighborhood size shape',\n",
              " 'Qualitative probabilistic reasoning Bayesian network often reveals tradeoffs relationships ambiguous due competing qualitative influences We present two techniques combine qualitative numeric probabilistic reasoning resolve tradeoffs inferring qualitative relationship nodes Bayesian network The first approach incrementally marginalizes nodes contribute ambiguous qualitative relationships The second approach evaluates approximate Bayesian networks bounds probability distributions uses bounds determinate qualitative relationships question This approach also incremental algorithm refines state spaces random variables tighter bounds qualitative relationships resolved Both approaches provide systematic methods tradeoff resolution potentially lower computational cost application purely numeric methods',\n",
              " 'A simple powerful modification standard Gaussian distribution studied The variables rectified Gaussian constrained nonnegative enabling use nonconvex energy functions Two multimodal examples competitive cooperative distributions illustrate representational power rectified Gaussian Since cooperative distribution represent translations pattern demonstrates potential rectified Gaussian modeling pattern manifolds',\n",
              " 'This paper appear Neural Computation Abstract We introduce novel fast algorithm Independent Component Analysis used blind source separation feature extraction It shown neural network learning rule transformed txedpoint iteration provides algorithm simple depend userdetned parameters fast converge accurate solution allowed data The algorithm tnds one time nonGaussian independent components regardless probability distributions The computations performed either batch mode semiadaptive manner The convergence algorithm rigorously proven convergence speed shown cubic Some comparisons gradient based algorithms made showing new algorithm usually times faster sometimes giving solution iterations',\n",
              " 'Barlows seminal work minimal entropy codes unsupervised learning reiterated In particular need transmit probability events put practical neuronal framework detecting suspicious events A variant BCM learning rule presented together mathematical results suggesting optimal minimal entropy coding',\n",
              " 'Constructive induction divides problem learning inductive hypothesis two intertwined searches onefor best representation space twofor best hypothesis space In datadriven constructive induction DCI learning system searches better representation space analyzing input examples data The presented datadriven constructive induction method combines AQtype learning algorithm two classes representation space improvement operators constructors destructors The implemented system AQDCI experimentally applied GNP prediction problem using World Bank database The results show decision rules learned AQDCI outperformed rules learned original representation space predictive accuracy rule simplicity',\n",
              " 'Heuristic measures estimating quality attributes mostly assume independence attributes domains strong dependencies attributes performance poor Relief extension ReliefF capable correctly estimating quality attributes classification problems strong dependencies attributes By exploiting local information provided different contexts provide global view We present analysis ReliefF lead us adaptation regression continuous class problems The experiments artificial realworld data sets show Regressional ReliefF correctly estimates quality attributes various conditions used nonmyopic learning regression trees Regressional ReliefF ReliefF provide unified view estimating attribute quality regression classification',\n",
              " 'A new research area Inductive Logic Programming presently emerging While inheriting various positive characteristics parent subjects Logic Programming Machine Learning hoped new area overcome many limitations forebears The background present developments within area discussed various goals aspirations increasing body researchers identified Inductive Logic Programming needs based sound principles Logic Statistics On side statistical justification hypotheses discuss possible relationship Algorithmic Complexity theory ProbablyApproximatelyCorrect PAC Learning In terms logic provide unifying framework Muggleton Buntines Inverse Resolution IR Plotkins Relative Least General Generalisation RLGG rederiving RLGG terms IR This leads discussion feasibility extending RLGG framework allow invention new predicates previously discussed within context IR',\n",
              " 'Bayesian methods applicable complex modeling tasks In review principles Bayesian inference presented applied neural network models Several approximate implementations discussed advantages conventional frequentist model training selection outlined It argued Bayesian methods preferable traditional approaches although empirical evidence still sparse',\n",
              " 'This paper presents efficient algorithm learning Bayesian belief networks databases The algorithm takes database input constructs belief network structure output The construction process based computation mutual information attribute pairs Given data set large enough algorithm generate belief network close underlying model time enjoys time When data set normal DAGFaithful see Section probability distribution algorithm guarantees structure perfect map Pearl underlying dependency model generated To evaluate algorithm present experimental results three versions wellknown ALARM network database attributes records The results show algorithm accurate efficient The proof correctness analysis complexity O N conditional independence CI tests',\n",
              " 'This paper presents efficient algorithm constructing Bayesian belief networks databases The algorithm takes database attributes ordering ie causal attributes attribute appear earlier order input constructs belief network structure output The construction process based computation mutual information attribute pairs Given data set large enough DAGIsomorphic probability distribution algorithm guarantees perfect map underlying dependency tests To evaluate algorithm present experimental results three versions wellknown ALARM network database attributes records The correctness proof analysis computational complexity also presented We also discuss features work relate previous works model generated time enjoys time complexity O N conditional independence CI',\n",
              " 'A novel method regression recently proposed V Vapnik et al The technique called Support Vector Machine SVM well founded mathematical point view seems provide new insight function approximation We implemented SVM tested data base chaotic time series used compare performances different approximation techniques including polynomial rational approximation local polynomial techniques Radial Basis Functions Neural Networks The SVM performs better approaches presented We also study particular time series variability performance respect free parameters SVM',\n",
              " 'The requirement dense interconnect artificial neural network systems led researchers seek highdensity interconnect technologies This paper reports implementation using multichip modules MCMs interconnect medium The specific system described selforganizing parallel dynamic learning model requires dense interconnect technology effective implementation requirement fulfilled exploiting MCM technology The ideas presented paper regarding MCM implementation artificial neural networks versatile adapted apply neural network connectionist models',\n",
              " 'When specializing recursive predicate order exclude set negative examples without excluding set positive examples may possible specialize remove clauses refutation negative example without excluding positive exam ples A previously proposed solution problem apply program transformation order obtain nonrecursive target predicates recursive ones However application method prevents recursive specializations found In work present algorithm spectre ii limited specializing nonrecursive predicates The key idea upon algorithm based enough specialize remove clauses refutations negative examples order obtain correct specializations sometimes necessary specialize clauses appear refutations positive examples In contrast predecessor spectre new algorithm limited specializing clauses defining one predicate may specialize clauses defining multiple predicates Furthermore positive negative examples longer required instances predicate It proven algorithm produces correct specialization positive examples logical consequences original program finite number derivations positive negative examples positive negative examples sequence input clauses refutations',\n",
              " 'program wrt positive negative examples viewed problem pruning SLDtree refutations negative examples refutations positive examples excluded It shown actual pruning performed applying unfolding clause removal The algorithm spectre presented based idea The input algorithm besides logic program positive negative examples computation rule determines shape SLDtree pruned It shown generality resulting specialization dependent computation rule experimental results presented using three different computation rules The experiments indicate computation rule formulated number applications unfolding kept low possible The algorithm uses divideandconquer method also compared covering algorithm The experiments show higher predictive accuracy achieved focus discriminating positive negative examples rather achieving high coverage positive examples',\n",
              " 'Cognitive mapping qualitative decision modeling technique developed twenty years ago political scientists continues see occasional use social science decisionaiding applications In paper I show cognitive maps viewed context recent formalisms qualitative decision modeling latter provide firm semantic foundation facilitate development powerful inference procedures well extensions expressiveness models sort',\n",
              " 'Casebased reasoning systems traditionally used perform highlevel reasoning problem domains adequately described using discrete symbolic representations However many realworld problem domains autonomous robotic navigation better characterized using continuous representations Such problem domains also require continuous performance continuous sensorimotor interaction environment continuous adaptation learning performance task We introduce new method continuous casebased reasoning discuss applied dynamic selection modification acquisition robot behaviors autonomous navigation systems We conclude general discussion casebased reasoning issues addressed work',\n",
              " 'The EastWest Challenge title second international competition machine learning programs organized Fall Donald Michie Stephen Muggleton David Page Ashwin Srinivasan Oxford University The goal competition solve TRAINS problems discover simplest classification rules trainlike structured objects The rule complexity judged Prolog program counted number various components rule expressed Prolog Horn clauses There entries several countries submitted competition The GMU teams entry generated three members AQ family learning programs AQDT INDUCE AQHCI The paper analyses results obtained programs compares obtained learning programs It also presents ideas research inspired competition One ideas challenge machine learning community develop measure knowledge complexity would adequately capture cognitive complexity knowledge A preliminary measure cognitive complexity called Ccomplexity different Prologcomplexity Pcomplexity used competition briefly discussed The authors thank Professors Donald Michie Steve Muggleton David Page Ashwin Srinivasan organizing WestEast Challenge competition machine learning programs provided us stimulating challenge learning programs inspired new ideas improving The authors also thank Nabil Allkharouf Ali Hadjarian help suggestions efforts solve problems posed competition This research conducted Center Machine Learning Inference George Mason University The Centers research supported part Advanced Research Projects Agency Grant No NJ administered Office Naval Research Grant No FJ administered Air Force Office Scientific Research part Office Naval Research Grant No NJ part National Science Foundation Grants No IRI CDA DMI',\n",
              " 'Previous algorithms recovery Bayesianbelief network structures data either highly dependent conditional independence CI tests required ordering nodes supplied user We present algorithm integrates two approaches CI tests used generate ordering nodes database used recover underlying Bayesian network structure using non CI test based method Results evaluation algorithm number databases eg ALARM LED SOYBEAN presented We also discuss algorithm performance issues open problems',\n",
              " 'We propose new criterion model selection prediction problems The covariance inflation criterion adjusts training error average covariance predictions responses prediction rule applied permuted versions dataset This criterion applied general prediction problems example regression classification general prediction rules example stepwise regression treebased models neural nets As byproduct obtain measure effective number parameters used adaptive procedure We relate covariance inflation criterion model selection procedures illustrate use regression classification problems We also revisit conditional bootstrap approach model selection',\n",
              " 'This paper introduces magnetic neural gas MNG algorithm extends unsupervised competitive learning class information improve positioning radial basis functions The basic idea MNG discover heterogeneous clusters ie clusters data different classes migrate additional neurons towards The discovery effected heterogeneity coefficient associated neuron migration guided introducing kind magnetic effect The performance MNG tested number data sets including thyroid data set Results demonstrate promise',\n",
              " 'This research supported National Science Foundation Fellowship awarded Dario Salvucci Office Naval Research grant N awarded John Anderson The views conclusions contained document authors interpreted representing official policies either expressed implied National Science Foundation Office Naval Research United States government',\n",
              " 'Efficient algorithms developed estimating model parameters measured data even presence gross errors In addition point estimates parameters however assessments uncertainty needed Linear approximations provide standard errors misleading applied models substantially nonlinear To overcome difficulty profiling methods developed case regressor variables error free In paper extend profiling methods ErrorinVariableMeasurement EVM models We use Laplaces method integrate incidental parameters associated measurement errors apply profiling methods obtain approximate confidence contours parameters This approach computationally efficient requiring function evaluations applied large scale problems It useful certain measurement errors eg input variables relatively small small ignored',\n",
              " 'A novel architecture set learning rules cortical selforganization proposed The model based idea multiple information channels modulate one anothers plasticity Features learned bottomup information sources thus influenced learned contextual pathways vice versa A maximum likelihood cost function allows scheme implemented biologically feasible hierarchical neural circuit In simulations model first demonstrate utility temporal context modulating plasticity The model learns representation categorizes peoples faces according identity independent viewpoint taking advantage temporal continuity image sequences In second set simulations add plasticity contextual stream explore variations architecture In case model learns twotiered representation starting coarse viewbased clustering proceeding finer clustering specific stimulus features This model provides tenable account people may perform D object recognition hierarchical bottomup fashion',\n",
              " 'The boosting algorithm AdaBoost developed Freund Schapire exhibited outstanding performance several benchmark problems using C weak algorithm boosted Like ensemble learning approaches AdaBoost constructs composite hypothesis voting many individual hypotheses In practice large amount memory required store hypotheses make ensemble methods hard deploy applications This paper shows selecting subset hypotheses possible obtain nearly levels performance entire set The results also provide insight behavior AdaBoost',\n",
              " 'Infusion GABA agonist Reiter Stryker infusion NMDA receptor antagonist Bear et al primary visual cortex kittens monocular deprivation shifts ocular dominance toward closed eye cortical region near infusion site This reverse ocular dominance shift previously modeled variants covariance synaptic plasticity rule Bear et al Clothiaux et al Miller et al Reiter Stryker Kasamatsu et al showed infusion NMDA receptor antagonist adult cat primary visual cortex changes ocular dominance distribution reduces binocularity reduces orientation direction selectivity This paper presents novel account effects pharmacological treatments based EXIN synaptic plasticity rules Marshall include instar afferent excitatory outstar lateral inhibitory rule Functionally EXIN plasticity rules enhance efficiency discrimination contextsensitivity neural networks representation perceptual patterns Marshall Marshall Gupta The EXIN model decreases lateral inhibition neurons outside infusion site control regions neurons inside infusion region monocular deprivation In model plasticity afferent pathways neurons affected pharmacological treatments assumed blocked opposed previous models Bear et al Miller et al Reiter Stryker afferent pathways open eye neurons infusion region weakened The proposed model consistent results suggesting longterm plasticity blocked NMDA antagonists postsynaptic hyperpolarization Bear et al Dudek Bear Goda Stevens Kirkwood et al Since role plasticity lateral inhibitory pathways producing cortical plasticity received much attention several predictions made based EXIN lateral inhibitory plasticity rule',\n",
              " 'We present two algorithms use membership equivalence queries exactly identify concepts given union discretized axisparallel boxes ddimensional discretized Euclidean space coordinate n discrete values The first algorithm receives sd counterexamples uses time membership queries polynomial log n constant Further equivalence queries made formulated union Osd log axisparallel boxes Next introduce new complexity measure better captures complexity union boxes simply number boxes dimensions Our new measure number segments target polyhedron segment maximum portion one sides polyhedron lies entirely inside entirely outside halfspaces defining polyhedron We present improvement first algorithm uses time queries polynomial log n The hypothesis class used decision trees height sd Further show time queries used algorithm polynomial log n constant thus generalizing exact learnability DNF formulas constant number terms In fact single algorithm efficient either constant',\n",
              " 'Approaches combining genetic algorithms neural networks received great deal attention recent years As result much work reported two major areas neural network design training topology optimization This paper focuses key issues associated problem pruning multilayer perceptron using genetic algorithms simulated annealing The study presented considers number aspects associated network training may alter behavior stochastic topology optimizer Enhancements discussed improve topology searches Simulation results two mentioned stochastic optimization methods applied nonlinear system identification presented compared simple random search',\n",
              " 'Local belief propagation rules sort proposed Pearl guaranteed converge optimal beliefs singly connected networks Recently number researchers empirically demonstrated good performance algorithms networks loops theoretical understanding performance yet achieved Here lay foundation understanding belief propagation networks loops For networks single loop derive analytical relationship steady state beliefs loopy network true posterior probability Using relationship show category networks MAP estimate obtained belief update belief revision proven optimal although beliefs incorrect We show nodes use local information messages receive order correct steady state beliefs Furthermore prove networks single loop MAP estimate obtained belief revision convergence guaranteed give globally optimal sequence states The result independent length cycle size state space For networks multiple loops introduce concept balanced network show simulation results comparing belief revision update networks We show Turbo code structure balanced present simulations toy Turbo code problem indicating decoding obtained belief revision convergence significantly likely correct This report describes research done Center Biological Computational Learning Department Brain Cognitive Sciences Massachusetts Institute Technology Support Center provided part grant National Science Foundation contract ASC YW also supported NEI R EY E H Adelson',\n",
              " 'Previous work showed combination Genetic Algorithm using order permutation chromosome combined hand coded Greedy Optimizers readily produce optimal schedule four node test problem Langdon Following GA used find low cost schedules South Wales region UK high voltage power network This paper describes evolution best known schedule base South Wales problem using Genetic Programming starting hand coded heuris tics used GA',\n",
              " 'Search mechanisms artificial intelligence combine two elements representation determines search space search mechanism actually explores space Unfortunately many searches may explore redundant andor invalid solutions Genetic programming refers class evolutionary algorithms based genetic algorithms utilizing parameterized representation form trees These algorithms perform searches based simulation nature They face problems redundantinvalid subspaces These problems recently addressed systematic manner This paper presents methodology devised public domain genetic programming tool lilgp This methodology uses data typing semantic information constrain representation space valid possibly unique solutions explored The user enters problemspecific constraints transformed normal set This set checked feasibility subsequently used limit space explored The constraints determine valid possibly unique space Moreover also used exclude subspaces user considers uninteresting using problemspecific knowledge A simple example followed thoroughly illustrate constraint language transformations normal set Experiments boolean multiplexer illustrate practical applications method limit redundant space exploration utilizing problemspecific knowledge fl Supported grant NASAJSC NAG',\n",
              " 'Knowledge acquisition difficult errorprone timeconsuming task The task automatically improving existing knowledge base using learning methods addressed class systems performing theory refinement This paper presents system Forte FirstOrder Revision Theories Examples refines firstorder Hornclause theories integrating variety different revision techniques coherent whole Forte uses techniques within hillclimbing framework guided global heuristic It identifies possible errors theory calls library operators develop possible revisions The best revision implemented process repeats revisions possible Operators drawn variety sources including propositional theory refinement firstorder induction inverse resolution Forte demonstrated several domains including logic programming qualitative modelling',\n",
              " 'The problem sequence categorization generalize corpus labeled sequences procedures accurately labeling future unlabeled sequences The choice representation sequences major impact task absence background knowledge good representation often known straightforward representations often far optimal We propose feature generation method called FGEN creates Boolean features check presence absence heuristically selected collections subsequences We show empirically representation computed FGEN improves accuracy two commonly used learning systems C Ripper new features added existing representations sequence data We show superiority FGEN across range tasks selected three domains DNA sequences Unix command sequences English text',\n",
              " 'Genetic algorithms one example use random element within algorithm combinatorial optimization We consider application genetic algorithm particular problem Assembly Line Balancing Problem A general description genetic algorithms given specialized use testbed problems discussed We carry extensive computational testing find appropriate values various parameters associated genetic algorithm These experiments underscore importance correct choice scaling parameter mutation rate ensure good performance genetic algorithm We also describe parallel implementation genetic algorithm give comparisons parallel serial implementations Both versions algorithm shown effective producing good solutions problems type appropriately chosen parameters',\n",
              " 'This paper presents application CaseBased Reasoning methods KOSIMO data base international conflicts A CaseBased Reasoning tool VIECBR deveolped used classification various outcome variables like political military territorial outcome solution modalities conflict intensity In addition case retrieval algorithms presented interactive usermodifiable tool intelli gently searching conflict data base precedent cases',\n",
              " 'In order learn behaviour casebased reasoners learning systems formalise simple casebased learner PAC learning algorithm using casebased representation hCB We first consider naive casebased learning algorithm CB H learns collecting available cases casebase calculates similarity counting number features two problem descriptions agree We present results concerning consistency learning algorithm give partial results regarding sample complexity We able characterise CB H weak general learning algorithm We consider sample complexity casebased learning reduced specific classes target concept application inductive bias prior knowledge class target concepts Following recent work demonstrating casebased learning improved choosing similarity measure appropriate concept learnt define second casebased learning algorithm CB learns using best possible similarity measure might inferred chosen target concept While CB executable learning strategy since chosen similarity measure defined terms priori knowledge actual target concept allows us assess limit maximum possible contribution approach casebased learning Also addition illustrating role inductive bias definition CB simplifies general problem establishing functions might represented form hCB Reasoning casebased representation special case therefore little straightforward general case CB H allowing substantial results regarding representable functions sample complexity presented CB In assessing results forced conclude casebased learning best approach learning chosen concept space space monomial functions We discuss however study demonstrated context casebased learning operation concepts well known machine learning inductive bias tradeoff computational complexity sample complexity',\n",
              " 'In past years evolutionary computation landscape rapidly changing result increased levels interaction various research groups injection new ideas challenge old tenets The effect simultaneously exciting invigorating annoying bewildering oldtimers well newcomers field Emerging activity beginnings structure common themes agreement important open issues We attempt summarize emergent properties paper',\n",
              " 'We quantify experimentally analytically performance memorybased reasoning MBR algorithms To start gaining insight capabilities MBR algorithms compare MBR algorithm using value difference metric popular Bayesian classifier These two approaches similar make certain independence assumptions data However whereas MBR uses specific cases perform classification Bayesian methods summarize data probabilistically We demonstrate particular MBR system called Pebls works comparatively well wide range domains using real artificial data With respect artificial data consider distributions concept classes separated functional discriminants well timeseries data generated Markov models varying complexity Finally show formally Pebls learn limit natural concept classes Bayesian classifier learn attain perfect accuracy whenever',\n",
              " 'The Knearestneighbor decision rule assigns object unknown class plurality class among K labeled training objects closest Closeness usually deflned terms metric distance Euclidean space input measurement variables axes The metric chosen deflne distance strongly efiect performance An optimal choice depends problem hand characterized respective class distributions input measurement space within given problem location unknown object space In paper new types Knearestneighbor procedures described estimate local relevance input variable linear combinations individual point classifled This information used separately customize metric used deflne distance object flnding nearest neighbors These procedures hybrid regular Knearestneighbor methods treestructured recursive partitioning techniques popular statistics machine learning',\n",
              " 'Seismic data interpretation problems typically solved using computationally intensive local search methods often result inferior solutions Here traditional hybrid genetic algorithm compared different staged hybrid genetic algorithms geophysical imaging static corrections problem The traditional hybrid genetic algorithm used applied local search every offspring produced genetic search The staged hybrid genetic algorithms designed temporally separate local genetic search components distinct phases minimize interference two search methods The results show staged hybrid genetic algorithms produce higher quality solutions using significantly less computational time problem',\n",
              " 'We describe immune system model based universe binary strings The model directed understanding pattern recognition processes learning take place individual species levels immune system The genetic algorithm GA central component model In paper study behavior GA two pattern recognition problems relevant natural immune systems Finally compare model explicit fitness sharing techniques genetic algorithms show model implements form implicit fitness sharing',\n",
              " 'We present method accurate representation highdimensional unknown functions random samples drawn input space The method builds representations function recursively splitting input space smaller subspaces subspaces linear approximation computed The representations function levels ie depths tree retained learning process good generalisation available well accurate representations subareas Therefore fast accurate learning combined method',\n",
              " 'Several authors made link hidden Markov models time series energybased models Luttrell Williams Saul Jordan Saul Jordan discuss linear Boltzmann chain model statestate transition energies A ii going state state symbol emission energies B ij probability entire state fi l j l g L Whilst HMM written linear Boltzmann chain setting expA ii ii expB ij b ij exp linear Boltzmann chains represented HMMs Saul Jordan However difference two models minimal To precise final hidden state L linear Boltzmann chain constrained particular end state distribution sequences identical hidden Markov model',\n",
              " 'We present coevolutionary approach learning sequential decision rules appears number advantages noncoevolutionary approaches The coevolutionary approach encourages formation stable niches representing simpler subbehaviors The evolutionary direction subbehavior controlled independently providing alternative evolving complex behavior using intermediate training steps Results presented showing significant learning rate speedup noncoevolutionary approach simulated robot domain In addition results suggest coevolutionary approach may lead emer gent problem decompositions',\n",
              " 'Appropriate bias widely viewed key efficient learning generalization I present new algorithm Incremental DeltaBarDelta IDBD algorithm learning appropriate biases based previous learning experience The IDBD algorithm developed case simple linear learning systemthe LMS delta rule separate learningrate parameter input The IDBD algorithm adjusts learningrate parameters important form bias system Because bias approach adapted based previous learning experience appropriate testbeds drifting nonstationary learning tasks For particular tasks type I show IDBD algorithm performs better ordinary LMS fact finds optimal learning rates The IDBD algorithm extends improves prior work Jacobs fully incremental single free parameter This paper also extends previous work presenting derivation IDBD algorithm gradient descent space learningrate parameters Finally I offer novel interpretation IDBD algorithm incremental form holdoneout cross validation',\n",
              " 'Neural network pruning methods level individual network parameters eg connection weights improve generalization An open problem pruning methods known today OBD OBS autoprune epsiprune selection number parameters removed pruning step pruning strength This paper presents pruning method lprune automatically adapts pruning strength evolution weights loss generalization training The method requires algorithm parameter adjustment user The results extensive experimentation indicate lprune often superior autoprune superior OBD diagnosis tasks unless severe pruning early training process required Results statistical significance tests comparing autoprune new method lprune well backpropagation early stopping given different problems',\n",
              " 'ICSIM connectionist net simulator developed ICSI written Sather It objectoriented meet requirements flexibility reuse homogeneous structured connectionist nets allow user encapsulate efficient customized implementations perhaps running dedicated hardware Nets composed combining offtheshelf library classes necessary specializing behaviour General user interface classes allow uniform customized graphic presentation nets modeled',\n",
              " 'In experiencebased casebased reasoning new problems solved retrieving adapting solutions similar problems encountered past An important issue experiencebased reasoning identify different types knowledge reasoning useful different classes caseadaptation tasks In paper examine class nonroutine caseadaptation tasks involve patterned insertions new elements old solutions We describe modelbased method solving task context design physical devices The method uses knowledge generic teleological mechanisms GTMs cascading Old designs adapted meet new functional specifications accessing instantiating appropriate GTM The Kritik system evaluates computational feasibility sufficiency method design adaptation',\n",
              " 'The utility problem learning systems occurs knowledge learned attempt improve systems performance degrades performance instead We present methodology analysis utility problems uses computational models problem solving systems isolate root causes utility problem detect threshold conditions problem arise design strategies eliminate We present models casebased reasoning controlrule learning systems compare performance respect swamping utility problem Our analysis suggests casebased reasoning systems resistant utility problem controlrule learning systems',\n",
              " 'We present model similaritybased retrieval attempts capture three psychological phenomena people extremely good judging similarity analogy given items compare Superficial remindings much frequent structural remindings People sometimes experience use purely structural analogical remindings Our model called MACFAC many called chosen consists two stages The first stage MAC uses computationally cheap nonstructural matcher filter candidates pool memory items That redundantly encode structured representations content vectors whose dot product yields estimate well corresponding structural representations match The second stage FAC uses SME compute true structural match probe output first stage MACFAC fully implemented show capable modeling patterns access found psychological data',\n",
              " 'We present algorithm online learning linear functions optimal within constant factor respect bounds sum squared errors worst case sequence trials The bounds logarithmic number variables Furthermore algorithm shown optimally robust respect noise data within constant factor Key words Machine learning computational learning theory online learning linear functions worstcase loss bounds adaptive filter theory Subject classifications T',\n",
              " 'A fundamental issue casebased reasoning similarity assessment determining similarities differences new retrieved cases Many methods developed comparing input case descriptions cases already memory However success methods depends input case description sufficiently complete reflect important features new situation assured In casebased explanation anomalous events story understanding anomaly arises current situation incompletely understood consequently similarity assessment based matches known current features old cases likely fail gaps current cases description Our solution problem gaps new cases description approach call constructive similarity assessment Constructive similarity assessment treats similarity assessment simple comparison fixed new old cases process deciding types features investigated new situation features borne knowledge added description current case Constructive similarity assessment merely compare new cases old using prior cases guide dynamically carves augmented descriptions new cases memory',\n",
              " 'Much recent research modeling memory processes focused identifying useful indices retrieval strategies support particular memory tasks Another important question concerning memory processes however retrieval criteria learned This paper examines issues involved modeling learning memory search strategies It discusses general requirements appropriate strategy learning presents model memory search strategy learning applied problem retrieving relevant information adapting cases casebased reasoning It discusses implementation model based lessons learned implementation points towards issues directions refining model',\n",
              " 'The problem approximating probability distribution occurs frequently many areas applied mathematics including statistics communication theory machine learning theoretical analysis complex systems neural networks Saul Jordan recently proposed powerful method efficiently approximating probability distributions known structured variational approximations In structured variational approximations exact algorithms probability computation tractable substructures combined variational methods handle interactions substructures make system whole intractable In note I present mathematical result simplify derivation struc tured variational approximations exponential family distributions',\n",
              " 'This paper presents ASOCS adaptive selforganizing concurrent system model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control An ASOCS adaptive network composed many simple computing elements operating asynchronously parallel This paper focuses adaptive algorithm AA details architecture learning algorithm It advantages previous ASOCS models simplicity implementability cost An ASOCS operate either data processing mode learning mode During data processing mode ASOCS acts parallel hardware circuit In learning mode rules expressed boolean conjunctions incrementally presented ASOCS All ASOCS learning algorithms incorporate new rule distributed fashion short bounded time',\n",
              " 'This paper describes novel search algorithm called dynamic hill climbing borrows ideas genetic algorithms hill climbing techniques Unlike genetic hill climbing algorithms dynamic hill climbing ability dynamically change coordinate frame course optimization Furthermore algorithm moves coarsegrained search finegrained search function space changing mutation rate uses diversitybased distance metric ensure searches new regions space Dynamic hill climbing empirically compared traditional genetic algorithm using De Jongs wellknown five function test suite shown vastly surpass performance genetic algorithm often finding better solutions using many function evaluations',\n",
              " 'Autonomous vehicles likely require sophisticated software controllers maintain vehicle performance presence vehicle faults The test evaluation complex software controllers expected challenging task The goal e ffort apply machine learning techniques field arti ficial intelligence general problem evaluating intelligent controller autonomous vehicle The approach involves subjecting controller adaptively chosen set fault scenarios within vehicle simulator searching combinations faults produce noteworthy performance vehicle controller The search employs genetic algorithm We illustrate approach evaluating performance subsumptionbased controller autonomous vehicle The preliminary evidence suggests approach e ffective alternative manual testing sophisticated software controllers',\n",
              " 'Speedup learning seeks improve efficiency searchbased problem solvers In paper propose new theoretical model speedup learning captures systems improve problem solving performance solving usergiven set problems We also use model motivate notion batch problem solving argue congenial learning sequential problem solving Our theoretical results applicable serially decomposable domains We empirically validate results domain Eight Puzzle',\n",
              " 'Nonparametric density estimation problem approximating values probability density function given samples associated distribution Nonparametric estimation finds applications discriminant analysis cluster analysis flow calculations based Smoothed Particle Hydrodynamics Usual estimators make use kernel functions require order n arithmetic operations evaluate density n sample points We describe sequence special weight functions requires almost linear number operations n computation',\n",
              " 'In paper consider learning firstorder Horn programs entailment In particular show subclass firstorder acyclic Horn programs constant arity exactly learnable equivalence entailment membership queries provided allows polynomialtime subsumption procedure satisfies closure conditions One consequence firstorder acyclic determinate Horn programs constant arity exactly learnable equiv alence entailment membership queries',\n",
              " 'A strategy using Genetic Algorithms GAs solve NPcomplete problems presented The key aspect approach taken exploit observation although NPcomplete problems equally difficult general computational sense much better GA representations others leading much successful use GAs NPcomplete problems others Since NPcomplete problem mapped one polynomial time strategy described consists identifying canonical NPcomplete problem GAs work well solving NPcomplete problems indirectly mapping onto canonical problem Initial empirical results presented support claim Boolean Satisfiability Problem SAT GAeffective canonical problem NPcomplete problems poor GA representations solved efficiently mapping first onto SAT problems',\n",
              " 'Fully cooperative multiagent systemsthose agents share joint utility modelis special interest AI A key problem ensuring actions individual agents coordinated especially settings agents autonomous decision makers We investigate approaches learning coordinated strategies stochastic domains agents actions directly observable others Much recent work game theory adopted Bayesian learning perspective general problem equilibrium selection tends assume actions observed We discuss special problems arise actions observable including effects rates convergence effect action failure probabilities asymmetries We also use likelihood estimates means generalizing fictitious play learning models setting Finally propose use maximum likelihood means removing strategies consideration aim convergence conventional equilibrium point learning deliberation cease',\n",
              " 'Humans appear often solve problems new domain transferring expertise familiar domain However making crossdomain analogies hard often requires abstractions common source target domains Recent work casebased design suggests generic mechanisms one type abstractions used designers However one important yet unexplored issue generic mechanisms come We hypothesize acquired incrementally problemsolving experiences familiar domains generalization patterns regularity Three important issues generalization experiences generalize experience far generalize methods use In paper show mental models familiar domain provide content together problemsolving context learning occurs also provide constraints learning generic mechanisms design experiences In particular show modelbased learning method integrated similaritybased learning addresses issues generalization experiences',\n",
              " 'The optimization single bit string means iterated mutation selection best Genetic Algorithm discussed respect three simple fitness functions The counting ones problem standard binary encoded integer Gray coded integer optimization problem A mutation rate schedule optimal respect success probability mutation presented objective functions turns standard binary code hamper search process even case unimodal objective functions While normally mutation rate l l denotes bit string length recommendable results indicate variation mutation rate useful cases fitness function multimodal pseudoboolean function multimodality may caused objective function well encoding mechanism',\n",
              " 'Genetic Algorithms used learn navigation collision avoidance behaviors robots The learning performed simulation resulting behaviors used control The approach learning behaviors robots described reflects particular methodology learning via simulation model The motivation making mistakes real systems may costly dangerous In addition time constraints might limit number experiences learning real world many cases simulation model made run faster real time Since learning may require experimenting behaviors might occasionally produce unacceptable results applied real world might require much time real environment assume hypothetical behaviors evaluated simulation model offline system As illustrated Figure current best behavior placed real online system learning continues offline system The learning algorithm designed learn useful behaviors simulations limited fidelity The expectation behaviors learned simulations useful realworld environments Previous studies illustrated knowledge learned simulation robust might applicable real world simulation general ie noise varied conditions etc real world environment Where possible important identify differences simulation world note effect upon learning process The research reported continues examine hypothesis The next section briefly explains learning algorithm gives pointers extensive documentation found After actual robot described Then describe simulation robot The task actual robot',\n",
              " 'Conventional Intelligent Tutoring Systems ITS acknowledge uncertainty students knowledge Yet outcome teaching intervention exact state students knowledge uncertain In recent years researchers made startling progress management uncertainty knowledgebased systems Building developments describe ITS architecture explicitly models uncertainty This facilitate accurate student modeling provide ITSs learn',\n",
              " 'Satisfiability SAT refers task finding truth assignment makes arbitrary boolean expression true This paper compares neural network algorithm NNSAT GSAT greedy algorithm solving satisfiability problems GSAT solve problem instances difficult traditional satisfiability algorithms Results suggest NNSAT scales better number variables increase solving least many hard SAT problems',\n",
              " 'In last years several researchers within Artificial Life Mobile Robotics community used Artificial Neural Networks Explicitly viewing Neural Networks Artificial Life perspective number consequences make research call Artificial Life Neural Networks ALNNs rather different traditional connectionist research The aim paper make differences ALNNs classical neural networks explicit',\n",
              " 'The recognition D objects sequences D views modeled family selforganizing neural architectures called VIEWNET use View Information Encoded With NETworks VIEWNET incorporates preprocessor generates compressed D invariant representation image supervised incremental learning system Fuzzy ARTMAP classifies preprocessed representations D view categories whose outputs combined D invariant object categories working memory makes D object prediction accumulating evidence time D object category nodes multiple D views experienced VIEWNET benchmarked MIT Lincoln Laboratory database x D views aircraft including small frontal views without additive noise A recognition rate achieved one D view correct three D views The properties D view D object category nodes compared cells monkey inferotemporal cortex',\n",
              " 'Recent interest come deriving various neural network architectures modelling timedependent signals A number algorithms published multilayer perceptrons synapses described finite impulse response FIR infinite impulse response IIR filters latter case also known Locally Recurrent Globally Feedforward Networks The derivations algorithms used different approaches calculating gradients note present short unifying account different algorithms compare FIR case derivation performance New algorithms subsequently presented Simulation results performed benchmark algorithms In note results compared MackeyGlass chaotic time series number methods including standard multilayer perceptron local approximation method',\n",
              " 'This paper presents methodology estimate optimal number learning samples number hidden units needed obtain desired accuracy function approximation feedforward network The representation error generalization error components total approximation error analyzed approximation accuracy feedforward network investigated function number hidden units number learning samples Based asymptotical behavior approximation error asymptotical model error function AMEF introduced parameters determined experimentally An alternative model error function include theoretical results general bounds approximation also analyzed In combination knowledge computational complexity learning rule optimal learning set size number hidden units found resulting minimum computation time given desired precision approximation This approach applied optimize learning camerarobot mapping visually guided robot arm complex logarithm function approximation',\n",
              " 'We propose methodology Bayesian model determination decomposable graphical Gaussian models To achieve aim consider hyper inverse Wishart prior distribution concentration matrix given graph To ensure compatibility across models prior distributions obtained marginalisation prior conditional complete graph We explore alternative structures hyperparameters latter consequences model Model determination carried implementing reversible jump MCMC sampler In particular dimensionchanging move propose involves adding dropping edge graph We characterise set moves preserve decomposability graph giving fast algorithm maintaining junction tree representation graph sweep As state variable propose use incomplete variancecovariance matrix containing elements corresponding element inverse nonzero This allows computations performed locally clique level clear advantage analysis large complex datasets Finally statistical computational performance procedure illustrated means artificial real multidimensional datasets',\n",
              " 'An essential component opportunistic behavior opportunity recognition recognition conditions facilitate pursuit suspended goal Opportunity recognition special case situation assessment process sizing novel situation The ability recognize opportunities reinstating suspended problem contexts one way goals manifest design crucial creative design In order deal real world opportunity recognition attribute limited inferential power relevant suspended goals We propose goals suspended working memory monitor internal hidden representations currently recognized objects A suspended goal satisfied current internal representation suspended goal match We propose computational model working memory compare relevant theories opportunistic planning This working memory model implemented part IMPROVISER system',\n",
              " 'Technical Report UMIACSTR CSTR Institute Advanced Computer Studies University Maryland College Park MD Abstract One important aspects machine learning paradigm scales according problem size complexity Using task known optimal training error prespecified maximum number training updates investigate convergence backpropagation algorithm respect complexity required function approximation b size network relation size required optimal solution c degree noise training data In general solution found worse function approximated complex b oversized networks result lower training generalization error certain cases c use committee ensemble techniques beneficial level noise training data increased For experiments performed obtain optimal solution case We support observation larger networks produce better training generalization error using face recognition example network many parameters training points generalizes better smaller networks',\n",
              " 'For many reasons neural networks become popular AI machine learning models Two important aspects machine learning models well model generalizes unseen data well model scales problem complexity Using controlled task known optimal training error investigate convergence backpropagation BP algorithm We find optimal solution typically found Furthermore observe networks larger might expected result lower training generalization error This result supported another real world example We investigate training behavior analyzing weights trained networks excess degrees freedom seen little harm aid convergence contrasting interpolation characteristics multilayer perceptron neural networks MLPs polynomial models overfitting behavior different MLP often biased towards smoother solutions Finally analyze relevant theory outlining reasons significant practical differences These results bring question common beliefs neural network training regarding convergence optimal network size suggest alternate guidelines practical use lower fear excess degrees freedom help direct future work eg methods creation parsimonious solutions importance MLPBP bias possibly worse performance improved training algorithms',\n",
              " 'This paper presents novel induction algorithm Rulearner induces classification rules using Galois lattice explicit map search space rules The Rulearner system shown compare favorably commonly used symbolic learning methods use heuristics rather explicit map guide search rule space Furthermore learning system shown robust presence noisy data The Rulearner system also capable learning decision lists unordered rule sets allowing comparisons different learning paradigms within algorithmic framework',\n",
              " 'Keywords CaseBased Reasoning case retrieval case representation This paper deals retrieval useful cases casebased reasoning It focuses questions useful could mean search useful cases organized We present new search algorithm Fish Shrink able search quickly case base even aspects deflne usefulness spontaneously combined query time We compare Fish Shrink algorithms show make implicit closed world assumption We flnally refer realization presented idea context prototype FABELProject The scenery follows Previously collected cases stored large scaled case base An expert describes problem gives aspects requested case similar The similarity measure thus given spontaneously shall used explore case base within short time shall present required number cases make sure none cases similar The question prepare previously collected cases deflne retrieval algorithm able deal sponta neously userdeflned similarity measures',\n",
              " 'The parallel genetic algorithm PGA uses two major modifications compared genetic algorithm Firstly selection mating distributed Individuals live D world Selection mate done individual independently neighborhood Secondly individual may improve fitness lifetime eg local hillclimbing The PGA totally asynchronous running maximal efficiency MIMD parallel computers The search strategy PGA based small number active intelligent individuals whereas GA uses large population passive individuals We investigate PGA deceptive problems traveling salesman problem We outline PGA succesful Abstractly PGA parallel search information exchange individuals If represent optimization problem fitness landscape certain configuration space see PGA tries jump two local minima third still better local minima using crossover operator This jump probabilistically successful fitness landscape certain correlation We show correlation traveling salesman problem configuration space analysis The PGA explores implicitly correlation',\n",
              " 'The dominant theme casebased research recent ML conferences classifying cases represented feature vectors However useful tasks targeted representations often preferable We review recent literature casebased learning focusing alternative performance tasks expressive case representations We also highlight topics need additional research',\n",
              " 'Current approaches computational lexicology language technology knowledgebased competenceoriented try abstract away specific formalisms domains applications This results severe complexity acquisition reusability bottlenecks As alternative propose particular performanceoriented approach Natural Language Processing based automatic memorybased learning linguistic lexical tasks The consequences approach computational lexicology discussed application approach number lexical acquisition disambiguation tasks phonology morphology syntax described',\n",
              " 'Let H function explicitly defined approximable sequence H n n functional estimators In context propose new sequential algorithm optimise asymptotically H using stepwise estimators H n We prove mild conditions almost sure convergence law algorithm',\n",
              " 'In paper study new informationtheoretically justified approach missing data estimation multivariate categorical data The approach discussed modelbased imputation procedure relative model class ie functional form probability distribution complete data matrix case set multinomial models independence assumptions Based given model class assumption informationtheoretic criterion derived select different complete data matrices Intuitively general criterion called stochastic complexity represents shortest code length needed coding complete data matrix relative model class chosen Using informationtheoretic criteria missing data problem reduced search problem ie finding data completion minimal stochastic complexity In experimental part paper present empirical results approach using two real data sets compare results achived commonly used techniques case deletion imputating sample averages',\n",
              " 'We present paper new evolutionary procedure solving general optimization problems combines efficiently mechanisms genetic algorithms tabu search In order explore solution space properly interaction phases interspersed periods optimization algorithm An adaptation search principle National Hockey League NHL problem discussed The hybrid method developed paper well suited Open Shop Scheduling problems OSSP The results obtained appear quite satisfactory',\n",
              " 'Behavioural observations often described sequence symbols drawn finite alphabet However inductive inference strings automated technique produce models data nontrivial task This paper considers modelling behavioural data using probabilistic finite state automata PFSAs There number informationtheoretic techniques evaluating possible hypotheses The measure used paper Minimum Message Length MML Wallace Although attempts made construct PFSA models incremental addition substrings using heuristic rules MML give lowest information cost resultant models shown globally optimal Fogels Evolutionary Programming produce globally optimal PFSA models evolving data structures arbitrary complexity without requirement encode PFSA binary strings Genetic Algorithms However evaluation PFSAs evolution process MML PFSA alone possible since symbols consumed partially correct solution It suggested addition cant consume symbol symbol alphabet obviates difficulty The addition null symbol alphabet also permits evolution explanatory models need explain data useful property avoid overfitting noisy data Results given test set optimal pfsa model known set eye glance data derived instrument panel simulator',\n",
              " 'Technical Report CUEDFINFENGTR We use reversible jump Markov chain Monte Carlo MCMC methods Green address problem model order uncertainty autoregressive AR time series within Bayesian framework Efficient model jumping achieved proposing model space moves full conditional density AR parameters obtained analytically This compared alternative method moves cheaper compute proposals made new parameters move Results presented synthetic audio time series',\n",
              " 'Learning viewed problem planning series modifications memory We adopt view learning propose applicability casebased planning methodology task planning learn We argue relatively simple finegrained primitive inferential operators needed support flexible planning We show possible obtain benefits casebased reasoning within planning learn framework',\n",
              " 'V SCBR simple instancebased learning algorithm adjusts weighted similarity measure well collecting cases This paper presents PAC analysis V SCBR motivated PAC learning framework demonstrates two main ideas relevant study instancebased learners Firstly hypothesis spaces learner different target concepts compared predict difficulty target concepts learner Secondly helpful consider constituent parts instancebased learner explore separately many examples needed infer good similarity measure many examples needed case base Applying approaches show V SCBR learns quickly variables representation irrelevant target concept slowly relevant variables The paper relates overall behaviour behaviour constituent parts V SCBR',\n",
              " 'Partial determinations interesting form dependency attributes relation They generalize functional dependencies allowing exceptions We modify known MDL formula evaluating partial determinations allow use admissible heuristic exhaustive search Furthermore describe efficient preprocessingbased approach handling numerical attributes An empirical investigation tries evaluate viability presented ideas',\n",
              " 'The induction optimal finite state machine explanation symbol strings known least NPcomplete However satisfactory approximately optimal explanations may found use Evolutionary Programming It shown information theoretic measure finite state machine explanations used fitness function required evaluation candidate explanations search nearoptimal explanation It obvious measure class explanation favoured others search By empirical studies possible gain insight dimensions measure optimising In general probabilistic finite state machines explanations assessed minimum message length estimator minimum number transitions favoured explanations The information measure also favour explanations uneven distributions frequencies transitions node suggesting repeated sequences symbol strings preferred explanation Approximate bounds acceptance explanations length string required induction successful also derived considerations simplest possible random explanations information measure',\n",
              " 'How evolutionary process interact decentralized distributed system order produce globally coordinated behavior Using genetic algorithm GA evolve cellular automata CAs show evolution spontaneous synchronization one type emergent coordination takes advantage underlying mediums potential form embedded particles The particles typically phase defects synchronous regions designed evolutionary process resolve frustrations global phase We describe detail one typical solution discovered GA delineating discovered synchronization algorithm terms embedded particles interactions We also use particlelevel description analyze evolutionary sequence solution discovered Our results implications understanding emergent collective behavior natural systems automatic programming decentralized spatially extended multiprocessor systems',\n",
              " 'We prove general bootstrap theorem possibly infinitedimensional Zestimators builds recent infinitedimensional Ztheorem due Van der Vaart Our result extends finitedimensional results type bootstrap due Arcones Gine Lele Newton Raftery We sketch three examples models infinitedimensional parameter spaces fi applicatons general theorem',\n",
              " 'The prediction survival time recurrence time important learning problem medical domains The Recurrence Surface Approximation RSA method natural effective method predicting recurrence times using censored input data This paper introduces Survival Curve RSA SCRSA extension RSA approach produces accurate predicted rates recurrence maintaining accuracy individual predicted recurrence times The method applied problem breast cancer recurrence using two different datasets',\n",
              " 'We analyze query committee algorithm method filtering informative queries random stream inputs We show twomember committee algorithm achieves information gain positive lower bound prediction error decreases exponentially number queries We show particular exponential decrease holds query learning perceptrons Keywords selective sampling query learning Bayesian Learning experimental design fl Yoav Freund Room B ATT Bell Laboratories Mountain Ave Murray Hill NJ Telephone',\n",
              " 'A new method performing nonlinear form Principal Component Analysis proposed By use integral operator kernel functions one efficiently compute principal components highdimensional feature spaces related input space nonlinear map instance space possible pixel products fi images We give derivation method present first experimental results polynomial feature extraction pattern recognition',\n",
              " 'Modeling techniques developed recently AI uncertain reasoning communities permit significantly flexible specifications probabilistic knowledge Specifically graphical decisionmodeling formalismsbelief networks influence diagrams variantsprovide compact representation probabilistic relationships support inference algorithms automatically exploit dependence structure models These advances brought resurgence interest computational decision systems based normative theories belief preference However graphical decisionmodeling languages still quite limited purposes knowledge representation describe relationships among particular event instances capture general knowledge probabilistic relationships across classes events The inability capture general knowledge serious impediment AI tasks relevant factors decision problem enumerated advance A graphical decision model encodes particular set probabilistic dependencies predefined set decision alternatives specific mathematical form utility function Given properly specified model exist relatively efficient algorithms calculating posterior probabilities optimal decision policies A range similar cases may handled parametric variations original model However structure dependencies set available alternatives form utility function changes situation situation fixed network representation longer adequate An ideal computational decision system would possess general broad knowledge domain would ability reason particular circumstances given decision problem within domain One obvious approachwhich call call knowledgebased model construction KBMCis generate decision model dynamically runtime based problem description information received thus far Model construction consists selection instantiation assembly causal associational relationships broad knowledge base general relationships among domain concepts For example suppose wish develop system recommend appropriate actions maintaining computer network The natural graphical decision model would include chance',\n",
              " 'Determining conditions given learning algorithm appropriate open problem machine learning Methods selecting learning algorithm given domain met limited success This paper proposes new approach predicting given examples class locating example space choosing best learners region example space make predictions The regions example space defined prediction patterns learners used The learners chosen prediction selected according past performance region This dynamic approach learning algorithm selection compared methods selecting multiple learning algorithms The approach extended weight rather select algorithms according past performance given region Both approaches evaluated set Determining conditions given learning algorithm appropriate open problem machine learning Methods selecting learning algorithm given domain eg Aha Breiman portion domain Brodley Brodley met limited success This paper proposes new approach dynamically selects learning algorithm example locating example space choosing best learners prediction part example space The regions example space formed observed prediction patterns learners used The learners chosen prediction selected according past performance region defined crossvalidation history This paper introduces DS method dynamic selection learning algorithms We call dynamic learning algorithms used classify novel example depends example Preliminary experimentation motivated DW extension DS dynamically weights learners predictions according regional accuracy Further experimentation compares DS DW collection metalearning strategies crossvalidation Breiman various forms stacking Wolpert In phase experiementation metalearners six constituent learners heterogeneous search representation methods eg rule learner CN Clark decision tree learner C Quinlan oblique decision tree learner OC Murthy instancebased learner PEBLS Cost knearest neighbor learner ten domains compared several metalearning strategies',\n",
              " 'Tw important issues machine learning explored role memory plays acquiring new concepts extent learner take active part acquiring concepts This chapter describes program called Marvin uses concepts learned previously learn new concepts The program forms hypotheses concept learned tests hypotheses asking trainer questions Learning begins trainer shows Marvin example concept learned The program determines objects example belong concepts stored memory A description new concept formed using information obtained memory generalize description training example The generalized description tested program constructs new examples shows trainer asking belong target concept',\n",
              " 'Adaptation ecological systems environments commonly viewed explicit fitness function defined priori experimenter measured posteriori estimations based population size andor reproductive rates These methods capture role environmental complexity shaping selective pressures control adaptive process Ecological simulations enabled computational tools Latent Energy Environments LEE model allow us characterize closely effects environmental complexity evolution adaptive behaviors LEE described paper Its motivation arises need vary complexity controlled predictable ways without assuming relationship changes adaptive behaviors engender This goal achieved careful characterization environments different forms energy welldefined A genetic algorithm using endogenous fitness local selection used model evolutionary process Individuals population modeled neural networks simple sensorymotor systems variations behaviors related interactions varying environments We outline results three experiments analyze different sources environmental complexity effects collective behaviors evolving populations',\n",
              " 'In paper investigate efficiency subsumption basic provability relation ILP As D C NPcomplete even restrict linked Horn clauses fix C contain small constant number literals investigate several restrictions D We first adapt notion determinate clauses used ILP show subsumption decidable polynomial time D determinate respect C Secondly adapt notion klocal Horn clauses show subsumption efficiently computable reasonably small k We show results combined give efficient reasoning procedure determinate klocal Horn clauses ILPproblem recently suggested polynomial predictable Cohen simple counting argument We finally outline reduction algorithm essential part every lgg ILPlearning algorithm im proved ideas',\n",
              " 'BBN Technical Report Abstract Genetic programming powerful method automatically generating computer programs via process natural selection Koza However limitation known closure ie variables constants arguments functions values returned functions must data type To correct deficiency introduce variation genetic programming called strongly typed genetic programming STGP In STGP variables constants arguments returned values data type provision data type value specified beforehand This allows initialization process genetic operators generate syntactically correct parse trees Key concepts STGP generic functions true strongly typed functions rather templates classes functions generic data types analogous To illustrate STGP present four examples involving vectormatrix manipulation list manipulation multidimensional leastsquares regression problem multidimensional Kalman filter list manipulation function NTH list manipulation function MAPCAR',\n",
              " 'Category algorithms architectures recurrent networks No part paper submitted elsewhere Preference poster Abstract Existing proofs demonstrating computational limitations Recurrent Cascade Correlation RCC Network Fahlman explicitly limit results units sigmoidal hardthreshold transfer functions Giles et al Kremer The proof given shows given finite discrete deterministic transfer function used units RCC network finitestate automata FSA network model matter many units used The proof applies equally well continuous transfer functions finite number fixedpoints sigmoid function',\n",
              " 'subsumption decidable incomplete approximation logic implication important inductive logic programming theorem proving We show context based elimination possible matches certain superset determinate clauses tested subsumption polynomial time We discuss relation subsumption clique problem showing particular using additional prior knowledge substitution space small fraction search space identified possibly containing globally consistent solutions leads effective pruning rule We present empirical results demonstrating combination approaches provides extreme reduction computational effort',\n",
              " 'We introduce new algorithm designed learn sparse perceptrons input representations include highorder features Our algorithm based hypothesisboosting method able PAClearn relatively natural class target concepts Moreover algorithm appears work well practice set three problem domains algorithm produces classifiers utilize small numbers features yet exhibit good generalization performance Perhaps importantly algorithm generates concept descriptions easy humans understand',\n",
              " 'Current inductive machine learning algorithms typically use greedy search limited lookahead This prevents detect significant conditional dependencies attributes describe training objects Instead myopic impurity functions lookahead propose use RELIEFF extension RELIEF developed Kira Rendell heuristic guidance inductive learning algorithms We reimplemented Assistant system top induction decision trees using RELIEFF estimator attributes selection step The algorithm tested several artificial several real world problems results compared well known machine learning algorithms Excellent results artificial data sets two real world problems show advantage presented approach inductive learning',\n",
              " 'This paper presents new approach hierarchical reinforcement learning based MAXQ decomposition value function The MAXQ decomposition procedural semanticsas subroutine hierarchyand declarative semanticsas representation value function hierarchical policy MAXQ unifies extends previous work hierarchical reinforcement learning Singh Kaelbling Dayan Hinton Conditions MAXQ decomposition represent optimal value function derived The paper defines hierarchical Q learning algorithm proves convergence shows experimentally learn much faster ordinary flat Q learning Finally paper discusses interesting issues arise hierarchical reinforcement learning including hierarchical credit assignment problem nonhierarchical execution MAXQ hierarchy',\n",
              " 'Causality relates changes structure object effects changes changes properties behavior object This paper analyzes concept causality Genetic Programming GP suggests used adapting control parameters speeding GP search We first analyze effects crossover show weak causality GP representation operators Hierarchical GP approaches based discovery evolution functions amplify phenomenon However selection gradually retains strongly causal changes Causality correlated search space exploitation discussed context explorationexploitation tradeoff The results described argue bottomup GP evolutionary thesis Finally new developments based idea GP architecture evolution Koza discussed causality perspective',\n",
              " 'Methods voting classification algorithms Bagging AdaBoost shown successful improving accuracy certain classifiers artificial realworld datasets We review algorithms describe large empirical study comparing several variants conjunction decision tree inducer three variants NaiveBayes inducer The purpose study improve understanding algorithms use perturbation reweighting combination techniques affect classification error We provide bias variance decomposition error show different methods variants influence two terms This allowed us determine Bagging reduced variance unstable methods boosting methods AdaBoost Arcx reduced bias variance unstable methods increased variance NaiveBayes stable We observed Arcx behaves differently AdaBoost reweighting used instead resampling indicating fundamental difference Voting variants introduced paper include pruning versus pruning use probabilistic estimates weight perturbations Wagging backfitting data We found Bagging improves probabilistic estimates conjunction nopruning used well data backfit We measure tree sizes show interesting positive correlation increase average tree size AdaBoost trials success reducing error We compare meansquared error voting methods nonvoting methods show voting methods lead large significant reductions meansquared errors Practical problems arise implementing boosting algorithms explored including numerical instabilities underflows We use scatterplots graphically show AdaBoost reweights instances emphasizing hard areas also outliers noise',\n",
              " 'The longterm goal field creation understanding intelligence Productive research AI practical theoretical benefits notion intelligence precise enough allow cumulative development robust systems general results The concept rational agency long considered leading candidate fulfill role This paper outlines gradual evolution formal conception rationality brings closer informal conception intelligence simultaneously reduces gap theory practice Some directions future research indicated',\n",
              " 'A solution problem representing compositional structure using distributed representations described The method uses circular convolution associate items represented vectors Arbitrary variable bindings short sequences various lengths frames reduced representations compressed fixed width vector These representations items right used constructing compositional structures The noisy reconstructions given convolution memories cleaned using separate associative memory good reconstructive properties',\n",
              " 'based Angluins L fl algorithm The algorithm maintains model consistent past examples When new counterexample arrives tries extend model minimal fashion We conducted set experiments random automata represent different strategies generated algorithm tried learn based prefixclosed samples behavior The algorithm managed learn compact models agree samples The size sample small effect size model The experimental results suggest random prefixclosed samples algorithm behaves well However following Angluins result difficulty learning almost uniform complete samples Angluin obvious algorithm solve complexity issue inferring DFA general prefixclosed sample We currently looking classes prefixclosed samples USL behaves well Carmel Markovitch D Carmel S Markovitch The M algorithm Incorporating opponent models adversary search Technical Report CIS report Technion March Carmel Markovitch D Carmel S Markovitch Unsupervised learning finite automata A practical approach Technical Report CIS report Technion March Shoham Tennenholtz Y Shoham M Tennenholtz CoLearning evolution social activity Technical Report STANCSTR Stanford Univrsity Department Computer Science',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "abstracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U1Okh8_5CVYH",
        "outputId": "30e91924-2056-42f7-a973-55dd0c75e43d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in abstracts: 205936\n"
          ]
        }
      ],
      "source": [
        "total_words = 0\n",
        "for a in abstracts:\n",
        "    total_words += len(a.split())\n",
        "\n",
        "print(f'Total words in abstracts: {total_words}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owkfCUw3CVYH"
      },
      "source": [
        "# ***KeyBERT & KeyLLM***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aogt42tCVYH"
      },
      "source": [
        "## Keyword & Keyphrase Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hMxfu20kCVYI"
      },
      "outputs": [],
      "source": [
        "def save_keywords_to_files(lists_of_keywords, file_names):\n",
        "    \"\"\"\n",
        "    Save the keywords from each list to a separate text file.\n",
        "\n",
        "    :param lists_of_keywords: A list of lists, where each sublist contains keywords for an abstract\n",
        "    :param file_names: A list of file names corresponding to each list\n",
        "    \"\"\"\n",
        "    for keywords_list, file_name in zip(lists_of_keywords, file_names):\n",
        "        with open(file_name, 'w') as file:\n",
        "            for keywords in keywords_list:\n",
        "                # Join the keywords for the abstract into a single string\n",
        "                line = ' '.join(keywords)\n",
        "                # Write the line to the file\n",
        "                file.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qAYnWJ8rCVYI"
      },
      "outputs": [],
      "source": [
        "def modify_keyword_list(abstract_keywords):\n",
        "    new_abstract_keywords = []\n",
        "    for keyphrases in abstract_keywords:\n",
        "        new_abstract_keywords.append([keyphrase[0] for keyphrase in keyphrases])\n",
        "    return new_abstract_keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JPqhSGcSCVYI"
      },
      "outputs": [],
      "source": [
        "T = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1YXD9n5CVYJ"
      },
      "source": [
        "## KeyBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COfMbY4GCVYJ"
      },
      "outputs": [],
      "source": [
        "kbert = KeyBERT(model='all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM2RnmqkCVYJ"
      },
      "outputs": [],
      "source": [
        "abstract_embeddings, word_embeddings = kbert.extract_embeddings(abstracts, keyphrase_ngram_range=(1,3), stop_words='english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1gHI7JJCVYJ",
        "outputId": "d441409b-edfa-48f7-e6f4-052a9ef48301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> \n",
            "<class 'numpy.ndarray'>\n",
            "\n",
            "(2277, 384) \n",
            "(307075, 384)\n"
          ]
        }
      ],
      "source": [
        "print(f'{type(abstract_embeddings)} \\n{type(word_embeddings)}')\n",
        "print(f'\\n{abstract_embeddings.shape} \\n{word_embeddings.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5LN6fE_CVYK"
      },
      "outputs": [],
      "source": [
        "np.save('KeyBERT_Abstract_Embeddings_Unigram.npy', abstract_embeddings)\n",
        "np.save('KeyBERT_Abstract_Word_Embeddings_Unigram.npy', word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hBU3KqSCVYK"
      },
      "outputs": [],
      "source": [
        "np.save('KeyBERT_Abstract_Embeddings_Trigram.npy', abstract_embeddings)\n",
        "np.save('KeyBERT_Abstract_Word_Embeddings_Trigram.npy', word_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG6c3hWtCVYK"
      },
      "source": [
        "### KeyBERT for keyword extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaBUxEmOCVYL"
      },
      "outputs": [],
      "source": [
        "keybert_kws = kbert.extract_keywords(abstracts, keyphrase_ngram_range=(1,1), stop_words='english',\n",
        "                                       use_maxsum=True, nr_candidates=20, top_n=T,\n",
        "                                       doc_embeddings=abstract_embeddings, word_embeddings=word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiwMNl1rCVYL",
        "outputId": "f40494d3-5f73-4fa9-bcf4-625e58d2fa42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2277"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(keybert_kws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpC4K3C0CVYM"
      },
      "outputs": [],
      "source": [
        "modified_keybert_kws = modify_keyword_list(keybert_kws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLCQ9W31CVYM"
      },
      "source": [
        "### KeyBERT for keyphrase extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYruSzuDCVYM"
      },
      "outputs": [],
      "source": [
        "keybert_kphs = kbert.extract_keywords(abstracts, keyphrase_ngram_range=(1,3), stop_words='english',\n",
        "                                       use_maxsum=True, nr_candidates=20, top_n=T,\n",
        "                                       doc_embeddings=abstract_embeddings, word_embeddings=word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLHPLJ8FCVYM",
        "outputId": "aad72bf9-8124-4c5f-dfc7-45e9799e89bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2277"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(keybert_kphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SXia_ogCVYM"
      },
      "outputs": [],
      "source": [
        "modified_keybert_kphs = modify_keyword_list(keybert_kphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISStHDzDCVYM"
      },
      "source": [
        "## KeyLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0vjBrgLHCVYN"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "I have the following document:\n",
        "[DOCUMENT]\n",
        "\n",
        "Based on the information above, extract five keywords that best describe the topic of the text.\n",
        "The keywords should be separated by commas. Make sure you to only return the keywords and say nothing else.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1WqI-i7CVYN"
      },
      "source": [
        "The cell below uses a LLM from OpenAI. OpenAI requires an API key. Usage for OpenAI LLMs is limited and thus not appropriate for the current task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUxEfRbrCVYN"
      },
      "outputs": [],
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "llm = OpenAI(client=client, prompt=prompt)\n",
        "\n",
        "kLLM = KeyLLM(llm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'bit-dny/MindLLM-1b3-chat-zh-v2.0'\n",
        "\n",
        "# 4-bit Quantization to load Llama 2 with less GPU memory\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# Llama 2 Model & Tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Our text generator\n",
        "generator = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=500,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "hDerMbGhDUAM",
        "outputId": "d432ae8e-97cf-41db-f92a-2637ddf0295b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "13a6621178734170a15815fe0dea5714",
            "0a982d1c97f646f6b268f0f7961e4d76",
            "fccf56d8cbe149049c4208731ac2c4b0",
            "ad427c417eb04d0893be59aefd3145d5",
            "54ef48976f2a44ef9d8c3e40bf375225",
            "42f56375bede428bb42b13cfdc9816b2",
            "9fae73875734402c811077df2d0dbdcd",
            "f331df45d6aa4cfc92a9ecd950e40bd2",
            "aa94781089c04cdeb47b47dd3e6c49bc",
            "0e0b4ff3925f482f9e8dc028a8bdc8aa",
            "34208c70c6934456b4a281c5dedcfbb3",
            "472cdc8fdc324869bce7ff7e2b286a59",
            "527b2fc1449343578dd748f79e8d1b1d",
            "7355186b0a4d4c18b86049bc041fcf14",
            "a2ff11ae5f714f24bebac731d344cea2",
            "ac0abb99dc3f4e34bad3a9e714d83979",
            "59096319cf924f2485c5712d23775b56",
            "31b576c5063044bdbd5db260863bbde0",
            "505f1714d48c47fab8a98a215dcfbfa6",
            "485d07bdc63e459c84187dcd6360d7b5",
            "3fdad734bdbd4c43883ec19771a1d40e",
            "34809d1852a34f43990f095f94836204",
            "51f8ea87a02c41f083dacda74785e981",
            "2c3e19fe704e42559a90f2bf9f97693f",
            "ce97967ee77540819436f229f175b531",
            "88fe2abe1a0e47708a534186a1f89c24",
            "83e161617a5a46a881da8cf478d109a4",
            "d15db1967fe34e8f9f32aa816482c037",
            "2c1c5c7731484bb6b3b996a0ddb6d5f5",
            "106f80b62b4b489aa43ae65ab067f9c7",
            "e47364ab8c87455ca99347a955b80bd5",
            "a454ae41b5f44270a90ee29864da735a",
            "b13abdeeaf7c4748b99b14bf39aff9fa",
            "b3dd7997f67e4201bee13455c4c2e8bf",
            "366f49b695e94b16b99e1af7abc4b462",
            "438142e2727140388e93acf157b3721b",
            "1b1f9ab57b9d4a7f98c83efacaa56866",
            "8080e85a159b4ec9ade5dfea99871dbc",
            "dd6b167150f44ec1ac1628fdfb696415",
            "d780f5e8f3094b73a5030b7ae1ff22f2",
            "e67ed7b7d17749fa801106e65b7e4b12",
            "f9d5a13ca32f42c889b51c2e4d470337",
            "ddfcfe1fe1824efbb74974b188d48424",
            "e6c2571e1996421cb28f14fbc57b5bd5",
            "587be1d0ce684d3aa80c1ae128b45ae5",
            "6d6bdb7b68c24f078235ebb12e5302e9",
            "39373c70ce3f411aba9bca3836942ab8",
            "84b38692d1c94315b9528307029faf16",
            "54ca62a9eec3496ea4f4608a615f3bb5",
            "ac8be1871de14b6db71469424f00d531",
            "80eaab873b3845bd850c936a391aabdb",
            "aa674be4de0d4222a42bd83e3993a225",
            "b3b7810ab8b9446b942d5198fc0782e0",
            "6f06945c7d9b426f848f870e91004bd2",
            "48a240ce58aa40cca40ac67a454b3b8e",
            "b817566d29fb42129db86eb02df563ff",
            "6faff715cafa442986cfa324cd90c455",
            "6ef5e176d0584bde984dd48007b66a32",
            "78bc970a855946b89509e2f70de14bd9",
            "3ada84d468894f60b06a6ec5307377cd",
            "86b31b25003b43b892c41765ff8d3fd4",
            "5946e9dd81e94bb9a3e112a3a378b724",
            "5b3c28e5d52047589962f30de105e9d8",
            "88875c12812c454a943003a2f7875a25",
            "a97121beee224816bb85a4b5f71f05df",
            "56051b1eacd9458d92c89022328da8be",
            "35dd1be502f64fe5982e549679dacf0d",
            "ae5eee90f91a4a409279689e2f85887c",
            "8f1af3ed98544b90a9e950a2c3a54ab2",
            "dc0a678e30fb45808645ad37a38399c7",
            "b812caa7a43340789d21593509b80fc0",
            "6289129732ea49e099cae426b472344c",
            "c1a496e243b943aa9bd0320b5dcce8e1",
            "e5cbec580802479193139f979f2891b2",
            "b0f3e63c99744d3ba320a49fd0803354",
            "775769d9a0ab4588a5b3524cda162deb",
            "121d9aaff97645a5b31a11344f6ba58e",
            "651c390c614843aab89d89a9f7749d2e",
            "7d67b2757c65467ab5b742a4cc991350",
            "3ff8d4a820df4745b7b5082a274b6993",
            "25be43ee8351461a80119f8ed1ef5b6f",
            "b20ea9c3ca674565b84d4aa883794d4c",
            "b2c9a5e1c3cb4dd0a89373786a95acac",
            "382113e47f804012a81acd420cfbfd10",
            "8c4dd91756bc43c2abb6e6870092936d",
            "2f0d53766c3c4b1c8ff7c1d6d6518757",
            "79ff1611b71b480cb9cb93591f377d51",
            "29d96c8dd70844d3a42b565bfd85c4d0"
          ]
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/4.32M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13a6621178734170a15815fe0dea5714"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "472cdc8fdc324869bce7ff7e2b286a59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51f8ea87a02c41f083dacda74785e981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/507k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3dd7997f67e4201bee13455c4c2e8bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "587be1d0ce684d3aa80c1ae128b45ae5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b817566d29fb42129db86eb02df563ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/3.04G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35dd1be502f64fe5982e549679dacf0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "651c390c614843aab89d89a9f7749d2e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = TextGeneration(generator, prompt=prompt)\n",
        "kLLM = KeyLLM(llm)"
      ],
      "metadata": {
        "id": "bKlFpMUcIRSR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aISUJ58CVYO"
      },
      "source": [
        "### KeyLLM for keyword extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hERGP9c6CVYO"
      },
      "outputs": [],
      "source": [
        "kLLM_openai_kws = kLLM.extract_keywords(abstracts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDW1TqmGCVYQ"
      },
      "source": [
        "### KeyLLM for keyphrase extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejlZ1pE_CVYQ"
      },
      "source": [
        "## Create Keyword Text Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkpDfaI_CVYQ"
      },
      "outputs": [],
      "source": [
        "# Combine the lists and provide corresponding file names\n",
        "lists_of_keywords = [modified_keybert_kws, modified_keybert_kphs]\n",
        "file_names = ['cora/KeyBERT_Unigram.txt', 'cora/KeyBERT_Trigram.txt']\n",
        "\n",
        "# Save the keywords to separate text files\n",
        "save_keywords_to_files(lists_of_keywords, file_names)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13a6621178734170a15815fe0dea5714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a982d1c97f646f6b268f0f7961e4d76",
              "IPY_MODEL_fccf56d8cbe149049c4208731ac2c4b0",
              "IPY_MODEL_ad427c417eb04d0893be59aefd3145d5"
            ],
            "layout": "IPY_MODEL_54ef48976f2a44ef9d8c3e40bf375225"
          }
        },
        "0a982d1c97f646f6b268f0f7961e4d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f56375bede428bb42b13cfdc9816b2",
            "placeholder": "​",
            "style": "IPY_MODEL_9fae73875734402c811077df2d0dbdcd",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fccf56d8cbe149049c4208731ac2c4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f331df45d6aa4cfc92a9ecd950e40bd2",
            "max": 4319583,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa94781089c04cdeb47b47dd3e6c49bc",
            "value": 4319583
          }
        },
        "ad427c417eb04d0893be59aefd3145d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e0b4ff3925f482f9e8dc028a8bdc8aa",
            "placeholder": "​",
            "style": "IPY_MODEL_34208c70c6934456b4a281c5dedcfbb3",
            "value": " 4.32M/4.32M [00:03&lt;00:00, 1.20MB/s]"
          }
        },
        "54ef48976f2a44ef9d8c3e40bf375225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42f56375bede428bb42b13cfdc9816b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fae73875734402c811077df2d0dbdcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f331df45d6aa4cfc92a9ecd950e40bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa94781089c04cdeb47b47dd3e6c49bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e0b4ff3925f482f9e8dc028a8bdc8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34208c70c6934456b4a281c5dedcfbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "472cdc8fdc324869bce7ff7e2b286a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_527b2fc1449343578dd748f79e8d1b1d",
              "IPY_MODEL_7355186b0a4d4c18b86049bc041fcf14",
              "IPY_MODEL_a2ff11ae5f714f24bebac731d344cea2"
            ],
            "layout": "IPY_MODEL_ac0abb99dc3f4e34bad3a9e714d83979"
          }
        },
        "527b2fc1449343578dd748f79e8d1b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59096319cf924f2485c5712d23775b56",
            "placeholder": "​",
            "style": "IPY_MODEL_31b576c5063044bdbd5db260863bbde0",
            "value": "vocab.json: 100%"
          }
        },
        "7355186b0a4d4c18b86049bc041fcf14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_505f1714d48c47fab8a98a215dcfbfa6",
            "max": 999186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_485d07bdc63e459c84187dcd6360d7b5",
            "value": 999186
          }
        },
        "a2ff11ae5f714f24bebac731d344cea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fdad734bdbd4c43883ec19771a1d40e",
            "placeholder": "​",
            "style": "IPY_MODEL_34809d1852a34f43990f095f94836204",
            "value": " 999k/999k [00:00&lt;00:00, 1.57MB/s]"
          }
        },
        "ac0abb99dc3f4e34bad3a9e714d83979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59096319cf924f2485c5712d23775b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b576c5063044bdbd5db260863bbde0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "505f1714d48c47fab8a98a215dcfbfa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485d07bdc63e459c84187dcd6360d7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fdad734bdbd4c43883ec19771a1d40e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34809d1852a34f43990f095f94836204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51f8ea87a02c41f083dacda74785e981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c3e19fe704e42559a90f2bf9f97693f",
              "IPY_MODEL_ce97967ee77540819436f229f175b531",
              "IPY_MODEL_88fe2abe1a0e47708a534186a1f89c24"
            ],
            "layout": "IPY_MODEL_83e161617a5a46a881da8cf478d109a4"
          }
        },
        "2c3e19fe704e42559a90f2bf9f97693f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d15db1967fe34e8f9f32aa816482c037",
            "placeholder": "​",
            "style": "IPY_MODEL_2c1c5c7731484bb6b3b996a0ddb6d5f5",
            "value": "merges.txt: 100%"
          }
        },
        "ce97967ee77540819436f229f175b531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_106f80b62b4b489aa43ae65ab067f9c7",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e47364ab8c87455ca99347a955b80bd5",
            "value": 456318
          }
        },
        "88fe2abe1a0e47708a534186a1f89c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a454ae41b5f44270a90ee29864da735a",
            "placeholder": "​",
            "style": "IPY_MODEL_b13abdeeaf7c4748b99b14bf39aff9fa",
            "value": " 456k/456k [00:00&lt;00:00, 1.07MB/s]"
          }
        },
        "83e161617a5a46a881da8cf478d109a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d15db1967fe34e8f9f32aa816482c037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1c5c7731484bb6b3b996a0ddb6d5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "106f80b62b4b489aa43ae65ab067f9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e47364ab8c87455ca99347a955b80bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a454ae41b5f44270a90ee29864da735a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13abdeeaf7c4748b99b14bf39aff9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3dd7997f67e4201bee13455c4c2e8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_366f49b695e94b16b99e1af7abc4b462",
              "IPY_MODEL_438142e2727140388e93acf157b3721b",
              "IPY_MODEL_1b1f9ab57b9d4a7f98c83efacaa56866"
            ],
            "layout": "IPY_MODEL_8080e85a159b4ec9ade5dfea99871dbc"
          }
        },
        "366f49b695e94b16b99e1af7abc4b462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd6b167150f44ec1ac1628fdfb696415",
            "placeholder": "​",
            "style": "IPY_MODEL_d780f5e8f3094b73a5030b7ae1ff22f2",
            "value": "added_tokens.json: 100%"
          }
        },
        "438142e2727140388e93acf157b3721b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e67ed7b7d17749fa801106e65b7e4b12",
            "max": 507309,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9d5a13ca32f42c889b51c2e4d470337",
            "value": 507309
          }
        },
        "1b1f9ab57b9d4a7f98c83efacaa56866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddfcfe1fe1824efbb74974b188d48424",
            "placeholder": "​",
            "style": "IPY_MODEL_e6c2571e1996421cb28f14fbc57b5bd5",
            "value": " 507k/507k [00:00&lt;00:00, 5.65MB/s]"
          }
        },
        "8080e85a159b4ec9ade5dfea99871dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd6b167150f44ec1ac1628fdfb696415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d780f5e8f3094b73a5030b7ae1ff22f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e67ed7b7d17749fa801106e65b7e4b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9d5a13ca32f42c889b51c2e4d470337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddfcfe1fe1824efbb74974b188d48424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c2571e1996421cb28f14fbc57b5bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "587be1d0ce684d3aa80c1ae128b45ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d6bdb7b68c24f078235ebb12e5302e9",
              "IPY_MODEL_39373c70ce3f411aba9bca3836942ab8",
              "IPY_MODEL_84b38692d1c94315b9528307029faf16"
            ],
            "layout": "IPY_MODEL_54ca62a9eec3496ea4f4608a615f3bb5"
          }
        },
        "6d6bdb7b68c24f078235ebb12e5302e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac8be1871de14b6db71469424f00d531",
            "placeholder": "​",
            "style": "IPY_MODEL_80eaab873b3845bd850c936a391aabdb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "39373c70ce3f411aba9bca3836942ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa674be4de0d4222a42bd83e3993a225",
            "max": 576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3b7810ab8b9446b942d5198fc0782e0",
            "value": 576
          }
        },
        "84b38692d1c94315b9528307029faf16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f06945c7d9b426f848f870e91004bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_48a240ce58aa40cca40ac67a454b3b8e",
            "value": " 576/576 [00:00&lt;00:00, 17.4kB/s]"
          }
        },
        "54ca62a9eec3496ea4f4608a615f3bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac8be1871de14b6db71469424f00d531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80eaab873b3845bd850c936a391aabdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa674be4de0d4222a42bd83e3993a225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b7810ab8b9446b942d5198fc0782e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f06945c7d9b426f848f870e91004bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a240ce58aa40cca40ac67a454b3b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b817566d29fb42129db86eb02df563ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6faff715cafa442986cfa324cd90c455",
              "IPY_MODEL_6ef5e176d0584bde984dd48007b66a32",
              "IPY_MODEL_78bc970a855946b89509e2f70de14bd9"
            ],
            "layout": "IPY_MODEL_3ada84d468894f60b06a6ec5307377cd"
          }
        },
        "6faff715cafa442986cfa324cd90c455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86b31b25003b43b892c41765ff8d3fd4",
            "placeholder": "​",
            "style": "IPY_MODEL_5946e9dd81e94bb9a3e112a3a378b724",
            "value": "config.json: 100%"
          }
        },
        "6ef5e176d0584bde984dd48007b66a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b3c28e5d52047589962f30de105e9d8",
            "max": 1483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88875c12812c454a943003a2f7875a25",
            "value": 1483
          }
        },
        "78bc970a855946b89509e2f70de14bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97121beee224816bb85a4b5f71f05df",
            "placeholder": "​",
            "style": "IPY_MODEL_56051b1eacd9458d92c89022328da8be",
            "value": " 1.48k/1.48k [00:00&lt;00:00, 21.0kB/s]"
          }
        },
        "3ada84d468894f60b06a6ec5307377cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b31b25003b43b892c41765ff8d3fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5946e9dd81e94bb9a3e112a3a378b724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b3c28e5d52047589962f30de105e9d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88875c12812c454a943003a2f7875a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a97121beee224816bb85a4b5f71f05df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56051b1eacd9458d92c89022328da8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35dd1be502f64fe5982e549679dacf0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae5eee90f91a4a409279689e2f85887c",
              "IPY_MODEL_8f1af3ed98544b90a9e950a2c3a54ab2",
              "IPY_MODEL_dc0a678e30fb45808645ad37a38399c7"
            ],
            "layout": "IPY_MODEL_b812caa7a43340789d21593509b80fc0"
          }
        },
        "ae5eee90f91a4a409279689e2f85887c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6289129732ea49e099cae426b472344c",
            "placeholder": "​",
            "style": "IPY_MODEL_c1a496e243b943aa9bd0320b5dcce8e1",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8f1af3ed98544b90a9e950a2c3a54ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5cbec580802479193139f979f2891b2",
            "max": 3041192638,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0f3e63c99744d3ba320a49fd0803354",
            "value": 3041192638
          }
        },
        "dc0a678e30fb45808645ad37a38399c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_775769d9a0ab4588a5b3524cda162deb",
            "placeholder": "​",
            "style": "IPY_MODEL_121d9aaff97645a5b31a11344f6ba58e",
            "value": " 3.04G/3.04G [01:12&lt;00:00, 42.1MB/s]"
          }
        },
        "b812caa7a43340789d21593509b80fc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6289129732ea49e099cae426b472344c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a496e243b943aa9bd0320b5dcce8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5cbec580802479193139f979f2891b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f3e63c99744d3ba320a49fd0803354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "775769d9a0ab4588a5b3524cda162deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "121d9aaff97645a5b31a11344f6ba58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "651c390c614843aab89d89a9f7749d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d67b2757c65467ab5b742a4cc991350",
              "IPY_MODEL_3ff8d4a820df4745b7b5082a274b6993",
              "IPY_MODEL_25be43ee8351461a80119f8ed1ef5b6f"
            ],
            "layout": "IPY_MODEL_b20ea9c3ca674565b84d4aa883794d4c"
          }
        },
        "7d67b2757c65467ab5b742a4cc991350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2c9a5e1c3cb4dd0a89373786a95acac",
            "placeholder": "​",
            "style": "IPY_MODEL_382113e47f804012a81acd420cfbfd10",
            "value": "generation_config.json: 100%"
          }
        },
        "3ff8d4a820df4745b7b5082a274b6993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c4dd91756bc43c2abb6e6870092936d",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f0d53766c3c4b1c8ff7c1d6d6518757",
            "value": 119
          }
        },
        "25be43ee8351461a80119f8ed1ef5b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79ff1611b71b480cb9cb93591f377d51",
            "placeholder": "​",
            "style": "IPY_MODEL_29d96c8dd70844d3a42b565bfd85c4d0",
            "value": " 119/119 [00:00&lt;00:00, 7.17kB/s]"
          }
        },
        "b20ea9c3ca674565b84d4aa883794d4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c9a5e1c3cb4dd0a89373786a95acac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382113e47f804012a81acd420cfbfd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c4dd91756bc43c2abb6e6870092936d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f0d53766c3c4b1c8ff7c1d6d6518757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79ff1611b71b480cb9cb93591f377d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d96c8dd70844d3a42b565bfd85c4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}