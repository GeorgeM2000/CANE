{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2Vt_wvryq0oh"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Libraries & Tools***"
      ],
      "metadata": {
        "id": "Lm_6DHW8WU30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wDqukMh63rB7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from math import pow\n",
        "from datetime import datetime\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Cs_tnKOwef_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Global Variables***"
      ],
      "metadata": {
        "id": "Rt0e5GPf8RBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 300 # Default value\n",
        "neg_table_size = 1000000\n",
        "NEG_SAMPLE_POWER = 0.75\n",
        "batch_size = 64\n",
        "num_epoch = 200\n",
        "embed_size = 200\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "datasetName = \"cora\"\n",
        "ratio = 0.75\n",
        "rho = \"1.0,0.3,0.3\""
      ],
      "metadata": {
        "id": "0iTSFdmS8THg"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_word_count = 0\n",
        "min_word_count = float('inf')\n",
        "\n",
        "with open('/content/datasets/%s/data.txt' % datasetName, 'r') as file:\n",
        "    for line in file:\n",
        "        word_count = len(line.split())\n",
        "\n",
        "        if word_count > max_word_count:\n",
        "            max_word_count = word_count\n",
        "\n",
        "        if word_count < min_word_count:\n",
        "            min_word_count = word_count\n",
        "\n",
        "print(\"Max word count:\", max_word_count)\n",
        "print(\"Min word count:\", min_word_count)\n",
        "MAX_LEN = max_word_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xEhbCPl7UQY",
        "outputId": "c8c578cd-5006-4fa2-84a6-e9c16c6445f9"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max word count: 410\n",
            "Min word count: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***DataSet***"
      ],
      "metadata": {
        "id": "UwNH2N0iVJ1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataSet:\n",
        "    def __init__(self, text_path, graph_path):\n",
        "\n",
        "        text_file, graph_file = self.load(text_path, graph_path)\n",
        "\n",
        "        self.edges = self.load_edges(graph_file)\n",
        "\n",
        "        self.text, self.num_vocab, self.num_nodes = self.load_text(text_file)\n",
        "\n",
        "        self.negative_table = InitNegTable(self.edges)\n",
        "\n",
        "    def load(self, text_path, graph_path):\n",
        "        text_file = open(text_path, 'rb').readlines()\n",
        "        for a in range(0, len(text_file)):\n",
        "            text_file[a] = str(text_file[a])\n",
        "        graph_file = open(graph_path, 'rb').readlines()\n",
        "\n",
        "        return text_file, graph_file\n",
        "\n",
        "    def load_edges(self, graph_file):\n",
        "        edges = []\n",
        "        for i in graph_file:\n",
        "            edges.append(list(map(int, i.strip().decode().split('\\t'))))\n",
        "\n",
        "        print(\"Total load %d edges.\" % len(edges))\n",
        "\n",
        "        return edges\n",
        "\n",
        "    def load_text(self, text_file):\n",
        "        \"\"\"\n",
        "        Adapting with adapt(text_data):\n",
        "\n",
        "        vectorize_layer.adapt(text_data) analyzes text_data, builds a vocabulary, and assigns a unique integer ID to each word based on its frequency (most frequent words get lower IDs).\n",
        "        Transforming with vectorize_layer(text_data):\n",
        "\n",
        "        This maps each word in text_data to its corresponding integer token ID, producing a 2D array where each row represents a sequence of token IDs for a given input line, padded or truncated to max_len.\n",
        "        \"\"\"\n",
        "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=None,  # Set a limit if needed\n",
        "            output_mode='int',\n",
        "            output_sequence_length=MAX_LEN\n",
        "        )\n",
        "\n",
        "        text_data = [line.strip() for line in text_file]\n",
        "\n",
        "        vectorize_layer.adapt(text_data)\n",
        "\n",
        "        text = vectorize_layer(text_data).numpy()\n",
        "\n",
        "        num_vocab = len(vectorize_layer.get_vocabulary())\n",
        "        print(f'Vocabulary: {num_vocab}')\n",
        "        num_nodes = len(text)\n",
        "\n",
        "        return text, num_vocab, num_nodes\n",
        "\n",
        "    def negative_sample(self, edges):\n",
        "        node1, node2 = zip(*edges)\n",
        "        sample_edges = []\n",
        "        func = lambda: self.negative_table[random.randint(0, neg_table_size - 1)]\n",
        "        for i in range(len(edges)):\n",
        "            neg_node = func()\n",
        "            while node1[i] == neg_node or node2[i] == neg_node:\n",
        "                neg_node = func()\n",
        "            sample_edges.append([node1[i], node2[i], neg_node])\n",
        "\n",
        "        return sample_edges\n",
        "\n",
        "    def generate_batches(self, mode=None):\n",
        "\n",
        "        num_batch = len(self.edges) // batch_size\n",
        "        edges = self.edges\n",
        "        # if mode == 'add':\n",
        "        #     num_batch += 1\n",
        "        #     edges.extend(edges[:(config.batch_size - len(self.edges) // config.batch_size)])\n",
        "        if mode != 'add':\n",
        "            random.shuffle(edges)\n",
        "        sample_edges = edges[:num_batch * batch_size]\n",
        "        sample_edges = self.negative_sample(sample_edges)\n",
        "\n",
        "        batches = []\n",
        "        for i in range(num_batch):\n",
        "            batches.append(sample_edges[i * batch_size:(i + 1) * batch_size])\n",
        "        # print sample_edges[0]\n",
        "        return batches\n"
      ],
      "metadata": {
        "id": "oz7H_lORVMOm"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***CANE***"
      ],
      "metadata": {
        "id": "-AV9wwI_7NSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, vocab_size, num_nodes, rho):\n",
        "        rho = rho.split(\",\")\n",
        "        self.rho1 = float(rho[0])\n",
        "        self.rho2 = float(rho[1])\n",
        "        self.rho3 = float(rho[2])\n",
        "        # '''hyperparameter'''\n",
        "        with tf.name_scope('read_inputs') as scope:\n",
        "          \"\"\"\n",
        "          self.Text_a = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='Ta')\n",
        "          self.Text_b = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='Tb')\n",
        "          self.Text_neg = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='Tneg')\n",
        "          self.Node_a = tf.keras.Input(shape=(1,), dtype=tf.int32, name='n1')  # or shape=(,) for a single int\n",
        "          self.Node_b = tf.keras.Input(shape=(1,), dtype=tf.int32, name='n2')\n",
        "          self.Node_neg = tf.keras.Input(shape=(1,), dtype=tf.int32, name='n3')\n",
        "          \"\"\"\n",
        "          self.Text_a = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Ta')\n",
        "          self.Text_b = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tb')\n",
        "          self.Text_neg = tf.compat.v1.placeholder(tf.int32, [batch_size, MAX_LEN], name='Tneg')\n",
        "          self.Node_a = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n1')\n",
        "          self.Node_b = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n2')\n",
        "          self.Node_neg = tf.compat.v1.placeholder(tf.int32, [batch_size], name='n3')\n",
        "\n",
        "        with tf.name_scope('initialize_embedding') as scope:\n",
        "            self.text_embed = tf.Variable(tf.random.truncated_normal([vocab_size, embed_size // 2], stddev=0.3))\n",
        "            self.node_embed = tf.clip_by_norm(tf.Variable(tf.random.truncated_normal([num_nodes, embed_size // 2], stddev=0.3)), clip_norm=1, axes=1)\n",
        "\n",
        "        with tf.name_scope('lookup_embeddings') as scope:\n",
        "            #self.text_embed_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size // 2, name='text_embedding')\n",
        "            #self.node_embed_layer = tf.keras.layers.Embedding(input_dim=num_nodes, output_dim=embed_size // 2, name='node_embedding')\n",
        "\n",
        "            self.TA = tf.nn.embedding_lookup(self.text_embed, self.Text_a)#self.text_embed_layer(self.Text_a)\n",
        "            #print(f'\\nTA: {self.TA} \\n{type(self.TA)} {self.TA.shape}')\n",
        "            self.T_A = tf.expand_dims(self.TA, -1)\n",
        "\n",
        "            self.TB = tf.nn.embedding_lookup(self.text_embed, self.Text_b)#self.text_embed_layer(self.Text_b)\n",
        "            self.T_B = tf.expand_dims(self.TB, -1)\n",
        "\n",
        "            self.TNEG = tf.nn.embedding_lookup(self.text_embed, self.Text_neg)#self.text_embed_layer(self.Text_neg)\n",
        "            self.T_NEG = tf.expand_dims(self.TNEG, -1)\n",
        "\n",
        "            self.N_A = tf.nn.embedding_lookup(self.node_embed, self.Node_a)#self.node_embed_layer(self.Node_a)\n",
        "            self.N_B = tf.nn.embedding_lookup(self.node_embed, self.Node_b)#self.node_embed_layer(self.Node_b)\n",
        "            self.N_NEG = tf.nn.embedding_lookup(self.node_embed, self.Node_neg)#self.node_embed_layer(self.Node_neg)\n",
        "\n",
        "        self.convA, self.convB, self.convNeg = self.conv()\n",
        "        self.loss = self.compute_loss()\n",
        "\n",
        "    def conv(self):\n",
        "        W2 = tf.Variable(tf.random.truncated_normal([2, embed_size // 2, 1, 100], stddev=0.3))\n",
        "        rand_matrix = tf.Variable(tf.random.truncated_normal([100, 100], stddev=0.3))\n",
        "\n",
        "        convA = tf.nn.conv2d(self.T_A, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convB = tf.nn.conv2d(self.T_B, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "        convNEG = tf.nn.conv2d(self.T_NEG, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "        hA = tf.tanh(tf.squeeze(convA))\n",
        "        hB = tf.tanh(tf.squeeze(convB))\n",
        "        hNEG = tf.tanh(tf.squeeze(convNEG))\n",
        "\n",
        "        tmphA = tf.reshape(hA, [batch_size * (MAX_LEN - 1), embed_size // 2])\n",
        "        ha_mul_rand = tf.reshape(tf.matmul(tmphA, rand_matrix),\n",
        "                                 [batch_size, MAX_LEN - 1, embed_size // 2])\n",
        "        r1 = tf.matmul(ha_mul_rand, hB, adjoint_b=True)\n",
        "        r3 = tf.matmul(ha_mul_rand, hNEG, adjoint_b=True)\n",
        "        att1 = tf.expand_dims(tf.stack(r1), -1)\n",
        "        att3 = tf.expand_dims(tf.stack(r3), -1)\n",
        "\n",
        "        att1 = tf.tanh(att1)\n",
        "        att3 = tf.tanh(att3)\n",
        "\n",
        "        pooled_A = tf.reduce_mean(att1, 2)\n",
        "        pooled_B = tf.reduce_mean(att1, 1)\n",
        "        pooled_NEG = tf.reduce_mean(att3, 1)\n",
        "\n",
        "        a_flat = tf.squeeze(pooled_A)\n",
        "        b_flat = tf.squeeze(pooled_B)\n",
        "        neg_flat = tf.squeeze(pooled_NEG)\n",
        "\n",
        "        w_A = tf.nn.softmax(a_flat)\n",
        "        w_B = tf.nn.softmax(b_flat)\n",
        "        w_NEG = tf.nn.softmax(neg_flat)\n",
        "\n",
        "        rep_A = tf.expand_dims(w_A, -1)\n",
        "        rep_B = tf.expand_dims(w_B, -1)\n",
        "        rep_NEG = tf.expand_dims(w_NEG, -1)\n",
        "\n",
        "        hA = tf.transpose(hA, perm=[0, 2, 1])\n",
        "        hB = tf.transpose(hB, perm=[0, 2, 1])\n",
        "        hNEG = tf.transpose(hNEG, perm=[0, 2, 1])\n",
        "\n",
        "        rep1 = tf.matmul(hA, rep_A)\n",
        "        rep2 = tf.matmul(hB, rep_B)\n",
        "        rep3 = tf.matmul(hNEG, rep_NEG)\n",
        "\n",
        "        attA = tf.squeeze(rep1)\n",
        "        attB = tf.squeeze(rep2)\n",
        "        attNEG = tf.squeeze(rep3)\n",
        "\n",
        "        return attA, attB, attNEG\n",
        "\n",
        "    def compute_loss(self):\n",
        "        p1 = tf.reduce_sum(tf.multiply(self.convA, self.convB), 1)\n",
        "        p1 = tf.math.log(tf.nn.sigmoid(p1) + 0.001)\n",
        "\n",
        "        p2 = tf.reduce_sum(tf.multiply(self.convA, self.convNeg), 1)\n",
        "        p2 = tf.math.log(tf.nn.sigmoid(-p2) + 0.001)\n",
        "\n",
        "        p3 = tf.reduce_sum(tf.multiply(self.N_A, self.N_B), 1)\n",
        "        p3 = tf.math.log(tf.nn.sigmoid(p3) + 0.001)\n",
        "\n",
        "        p4 = tf.reduce_sum(tf.multiply(self.N_A, self.N_NEG), 1)\n",
        "        p4 = tf.math.log(tf.nn.sigmoid(-p4) + 0.001)\n",
        "\n",
        "        p5 = tf.reduce_sum(tf.multiply(self.convB, self.N_A), 1)\n",
        "        p5 = tf.math.log(tf.nn.sigmoid(p5) + 0.001)\n",
        "\n",
        "        p6 = tf.reduce_sum(tf.multiply(self.convNeg, self.N_A), 1)\n",
        "        p6 = tf.math.log(tf.nn.sigmoid(-p6) + 0.001)\n",
        "\n",
        "        p7 = tf.reduce_sum(tf.multiply(self.N_B, self.convA), 1)\n",
        "        p7 = tf.math.log(tf.nn.sigmoid(p7) + 0.001)\n",
        "\n",
        "        p8 = tf.reduce_sum(tf.multiply(self.N_B, self.convNeg), 1)\n",
        "        p8 = tf.math.log(tf.nn.sigmoid(-p8) + 0.001)\n",
        "\n",
        "        rho1 = self.rho1\n",
        "        rho2 = self.rho2\n",
        "        rho3 = self.rho3\n",
        "        temp_loss = rho1 * (p1 + p2) + rho2 * (p3 + p4) + rho3 * (p5 + p6) + rho3 * (p7 + p8)\n",
        "        loss = -tf.reduce_sum(temp_loss)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "nxoCqzz37O8y"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Negative Sample***"
      ],
      "metadata": {
        "id": "2Vt_wvryq0oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def InitNegTable(edges):\n",
        "    a_list, b_list = zip(*edges)\n",
        "    a_list = list(a_list)\n",
        "    b_list = list(b_list)\n",
        "    node = a_list\n",
        "    node.extend(b_list)\n",
        "\n",
        "    node_degree = {}\n",
        "    for i in node:\n",
        "        if i in node_degree:\n",
        "            node_degree[i] += 1\n",
        "        else:\n",
        "            node_degree[i] = 1\n",
        "    sum_degree = 0\n",
        "    for i in node_degree.values():\n",
        "        sum_degree += pow(i, 0.75)\n",
        "\n",
        "    por = 0\n",
        "    cur_sum = 0\n",
        "    vid = -1\n",
        "    neg_table = []\n",
        "    degree_list = list(node_degree.values())\n",
        "    node_id = list(node_degree.keys())\n",
        "    for i in range(neg_table_size):\n",
        "        if ((i + 1) / float(neg_table_size)) > por:\n",
        "            cur_sum += pow(degree_list[vid + 1], NEG_SAMPLE_POWER)\n",
        "            por = cur_sum / sum_degree\n",
        "            vid += 1\n",
        "        neg_table.append(node_id[vid])\n",
        "    print(len(neg_table))\n",
        "    return neg_table\n"
      ],
      "metadata": {
        "id": "UdwgdktTq504"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Run***"
      ],
      "metadata": {
        "id": "rLaqva64WoA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(datasetName, ratio):\n",
        "  f = open('/content/datasets/%s/graph.txt' % datasetName, 'rb')\n",
        "  edges = [i for i in f]\n",
        "  selected = int(len(edges) * float(ratio))\n",
        "  selected = selected - selected % batch_size\n",
        "  selected = random.sample(edges, selected)\n",
        "  remain = [i for i in edges if i not in selected]\n",
        "  try:\n",
        "    Path('temp').mkdir(parents=True, exist_ok=True)  # parents=True allows creating parent directories; exist_ok=True avoids errors if the directory already exists\n",
        "    print(f\"Directory '{directory}' created successfully.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "  fw1 = open('temp/graph.txt', 'wb')\n",
        "  fw2 = open('temp/test_graph.txt', 'wb')\n",
        "\n",
        "  for i in selected:\n",
        "      fw1.write(i)\n",
        "  for i in remain:\n",
        "      fw2.write(i)"
      ],
      "metadata": {
        "id": "dgbe18H4WsgX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepareData(datasetName, ratio)"
      ],
      "metadata": {
        "id": "yK_Z-E1HXRVm"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "dataset_name = datasetName\n",
        "graph_path = os.path.join('/content/temp/graph.txt')\n",
        "text_path = os.path.join(\"/content\", \"datasets\", dataset_name, 'data.txt')\n",
        "\n",
        "data = dataSet(text_path, graph_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wFgChvWggiu",
        "outputId": "657b23ff-1559-4588-fb4f-b20341b27f64"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total load 3904 edges.\n",
            "Vocabulary: 16169\n",
            "1000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    with sess.as_default():\n",
        "        model = Model(data.num_vocab, data.num_nodes, rho)\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(lr)#tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        train_op = opt.minimize(model.loss)#opt.minimize(model.loss, var_list=model.trainable_variables)\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        time = 0\n",
        "\n",
        "        # training\n",
        "        print('start training.......')\n",
        "\n",
        "        for epoch in range(num_epoch):\n",
        "            start = datetime.now()\n",
        "            loss_epoch = 0\n",
        "            batches = data.generate_batches()\n",
        "            h1 = 0\n",
        "            num_batch = len(batches)\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3\n",
        "                }\n",
        "\n",
        "                \"\"\"\n",
        "                # Directly call the model with inputs as arguments\n",
        "                with tf.GradientTape() as tape:\n",
        "                    model_outputs = model([text1, text2, text3, node1, node2, node3], training=True)\n",
        "                    loss_batch = model.compute_loss()  # Adjust compute_loss for easy access\n",
        "\n",
        "                grads = tape.gradient(loss_batch, model.trainable_variables)\n",
        "                opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "                loss_epoch += loss_batch.numpy()  # Convert tensor to a scalar for logging\n",
        "                \"\"\"\n",
        "                # run the graph\n",
        "                _, loss_batch = sess.run([train_op, model.loss], feed_dict=feed_dict)\n",
        "\n",
        "                loss_epoch += loss_batch\n",
        "\n",
        "            end = datetime.now()\n",
        "            time += (end - start).total_seconds()\n",
        "            print('epoch: ', epoch + 1, ' loss: ', loss_epoch)\n",
        "\n",
        "        print(f'Time: {time}')\n",
        "        # Saving embeddings\n",
        "        with open('temp/embed.txt', 'wb') as file:\n",
        "            batches = data.generate_batches(mode='add')\n",
        "            num_batch = len(batches)\n",
        "            embed = [[] for _ in range(data.num_nodes)]\n",
        "\n",
        "            for i in range(num_batch):\n",
        "                batch = batches[i]\n",
        "                node1, node2, node3 = zip(*batch)\n",
        "                node1, node2, node3 = np.array(node1), np.array(node2), np.array(node3)\n",
        "                text1, text2, text3 = data.text[node1], data.text[node2], data.text[node3]\n",
        "\n",
        "                feed_dict = {\n",
        "                    model.Text_a: text1,\n",
        "                    model.Text_b: text2,\n",
        "                    model.Text_neg: text3,\n",
        "                    model.Node_a: node1,\n",
        "                    model.Node_b: node2,\n",
        "                    model.Node_neg: node3\n",
        "                }\n",
        "\n",
        "                # Fetch embeddings\n",
        "                #convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B])\n",
        "                convA, convB, TA, TB = sess.run([model.convA, model.convB, model.N_A, model.N_B], feed_dict=feed_dict)\n",
        "\n",
        "                for j in range(batch_size):\n",
        "                    em = list(TA[j])\n",
        "                    embed[node1[j]].append(em)\n",
        "                    em = list(TB[j])\n",
        "                    embed[node2[j]].append(em)\n",
        "\n",
        "\n",
        "            for i in range(data.num_nodes):\n",
        "                if embed[i]:\n",
        "                    tmp = np.mean(embed[i], axis=0)\n",
        "                    file.write((' '.join(map(str, tmp)) + '\\n').encode())\n",
        "                else:\n",
        "                    file.write('\\n'.encode())"
      ],
      "metadata": {
        "id": "1QsRmNqLXrGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2580d052-7a8f-407c-9be9-31a307539dff"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training.......\n",
            "epoch:  1  loss:  13871.87629699707\n",
            "epoch:  2  loss:  10191.769454956055\n",
            "epoch:  3  loss:  10159.646377563477\n",
            "epoch:  4  loss:  10121.372207641602\n",
            "epoch:  5  loss:  10063.07632446289\n",
            "epoch:  6  loss:  9990.905639648438\n",
            "epoch:  7  loss:  9899.683502197266\n",
            "epoch:  8  loss:  9746.915420532227\n",
            "epoch:  9  loss:  9550.246963500977\n",
            "epoch:  10  loss:  9268.257263183594\n",
            "epoch:  11  loss:  8989.82534790039\n",
            "epoch:  12  loss:  8732.20588684082\n",
            "epoch:  13  loss:  8545.261978149414\n",
            "epoch:  14  loss:  8402.643768310547\n",
            "epoch:  15  loss:  8218.621681213379\n",
            "epoch:  16  loss:  8014.382919311523\n",
            "epoch:  17  loss:  7916.400215148926\n",
            "epoch:  18  loss:  7764.685127258301\n",
            "epoch:  19  loss:  7670.103096008301\n",
            "epoch:  20  loss:  7541.582809448242\n",
            "epoch:  21  loss:  7386.544410705566\n",
            "epoch:  22  loss:  7223.188362121582\n",
            "epoch:  23  loss:  7068.006080627441\n",
            "epoch:  24  loss:  6991.621826171875\n",
            "epoch:  25  loss:  6908.595809936523\n",
            "epoch:  26  loss:  6746.548187255859\n",
            "epoch:  27  loss:  6766.24910736084\n",
            "epoch:  28  loss:  6571.514533996582\n",
            "epoch:  29  loss:  6450.395248413086\n",
            "epoch:  30  loss:  6453.806976318359\n",
            "epoch:  31  loss:  6345.573661804199\n",
            "epoch:  32  loss:  6258.509742736816\n",
            "epoch:  33  loss:  6148.0766677856445\n",
            "epoch:  34  loss:  6045.4002685546875\n",
            "epoch:  35  loss:  5999.927619934082\n",
            "epoch:  36  loss:  5954.887046813965\n",
            "epoch:  37  loss:  5955.325401306152\n",
            "epoch:  38  loss:  5779.113822937012\n",
            "epoch:  39  loss:  5728.676704406738\n",
            "epoch:  40  loss:  5724.41633605957\n",
            "epoch:  41  loss:  5658.403335571289\n",
            "epoch:  42  loss:  5633.20231628418\n",
            "epoch:  43  loss:  5652.363433837891\n",
            "epoch:  44  loss:  5492.528060913086\n",
            "epoch:  45  loss:  5491.035041809082\n",
            "epoch:  46  loss:  5419.509140014648\n",
            "epoch:  47  loss:  5411.610557556152\n",
            "epoch:  48  loss:  5347.034324645996\n",
            "epoch:  49  loss:  5265.924423217773\n",
            "epoch:  50  loss:  5251.814651489258\n",
            "epoch:  51  loss:  5246.316429138184\n",
            "epoch:  52  loss:  5239.56551361084\n",
            "epoch:  53  loss:  5214.296600341797\n",
            "epoch:  54  loss:  5166.206985473633\n",
            "epoch:  55  loss:  5224.927917480469\n",
            "epoch:  56  loss:  5142.023422241211\n",
            "epoch:  57  loss:  5079.168037414551\n",
            "epoch:  58  loss:  5047.254196166992\n",
            "epoch:  59  loss:  5000.7687911987305\n",
            "epoch:  60  loss:  5002.4874267578125\n",
            "epoch:  61  loss:  4989.019630432129\n",
            "epoch:  62  loss:  4895.3666915893555\n",
            "epoch:  63  loss:  4867.158782958984\n",
            "epoch:  64  loss:  4957.691970825195\n",
            "epoch:  65  loss:  4904.030075073242\n",
            "epoch:  66  loss:  4909.901123046875\n",
            "epoch:  67  loss:  4798.027854919434\n",
            "epoch:  68  loss:  4853.018363952637\n",
            "epoch:  69  loss:  4889.168273925781\n",
            "epoch:  70  loss:  4853.232078552246\n",
            "epoch:  71  loss:  4788.398941040039\n",
            "epoch:  72  loss:  4799.390968322754\n",
            "epoch:  73  loss:  4801.952980041504\n",
            "epoch:  74  loss:  4767.224296569824\n",
            "epoch:  75  loss:  4696.502655029297\n",
            "epoch:  76  loss:  4686.434226989746\n",
            "epoch:  77  loss:  4717.115219116211\n",
            "epoch:  78  loss:  4725.613700866699\n",
            "epoch:  79  loss:  4672.0639572143555\n",
            "epoch:  80  loss:  4708.483665466309\n",
            "epoch:  81  loss:  4602.081386566162\n",
            "epoch:  82  loss:  4654.037750244141\n",
            "epoch:  83  loss:  4573.886581420898\n",
            "epoch:  84  loss:  4534.119934082031\n",
            "epoch:  85  loss:  4612.308921813965\n",
            "epoch:  86  loss:  4561.2263107299805\n",
            "epoch:  87  loss:  4610.772567749023\n",
            "epoch:  88  loss:  4600.006057739258\n",
            "epoch:  89  loss:  4524.918235778809\n",
            "epoch:  90  loss:  4554.664962768555\n",
            "epoch:  91  loss:  4517.417652130127\n",
            "epoch:  92  loss:  4451.916542053223\n",
            "epoch:  93  loss:  4500.007858276367\n",
            "epoch:  94  loss:  4534.202606201172\n",
            "epoch:  95  loss:  4507.364757537842\n",
            "epoch:  96  loss:  4535.475856781006\n",
            "epoch:  97  loss:  4590.864761352539\n",
            "epoch:  98  loss:  4506.365081787109\n",
            "epoch:  99  loss:  4496.170425415039\n",
            "epoch:  100  loss:  4441.717567443848\n",
            "epoch:  101  loss:  4473.91796875\n",
            "epoch:  102  loss:  4445.626396179199\n",
            "epoch:  103  loss:  4446.974437713623\n",
            "epoch:  104  loss:  4420.31999206543\n",
            "epoch:  105  loss:  4374.045467376709\n",
            "epoch:  106  loss:  4368.59451675415\n",
            "epoch:  107  loss:  4430.891998291016\n",
            "epoch:  108  loss:  4391.828254699707\n",
            "epoch:  109  loss:  4378.29740524292\n",
            "epoch:  110  loss:  4436.88249206543\n",
            "epoch:  111  loss:  4392.826446533203\n",
            "epoch:  112  loss:  4354.733657836914\n",
            "epoch:  113  loss:  4393.873512268066\n",
            "epoch:  114  loss:  4369.124443054199\n",
            "epoch:  115  loss:  4386.915920257568\n",
            "epoch:  116  loss:  4394.494213104248\n",
            "epoch:  117  loss:  4379.880577087402\n",
            "epoch:  118  loss:  4412.911403656006\n",
            "epoch:  119  loss:  4357.012195587158\n",
            "epoch:  120  loss:  4314.809989929199\n",
            "epoch:  121  loss:  4346.129482269287\n",
            "epoch:  122  loss:  4334.076118469238\n",
            "epoch:  123  loss:  4386.929889678955\n",
            "epoch:  124  loss:  4337.851524353027\n",
            "epoch:  125  loss:  4284.626209259033\n",
            "epoch:  126  loss:  4383.932346343994\n",
            "epoch:  127  loss:  4276.47114944458\n",
            "epoch:  128  loss:  4341.400016784668\n",
            "epoch:  129  loss:  4240.484882354736\n",
            "epoch:  130  loss:  4285.998497009277\n",
            "epoch:  131  loss:  4239.784683227539\n",
            "epoch:  132  loss:  4341.1381912231445\n",
            "epoch:  133  loss:  4190.313137054443\n",
            "epoch:  134  loss:  4200.614936828613\n",
            "epoch:  135  loss:  4249.271297454834\n",
            "epoch:  136  loss:  4192.425434112549\n",
            "epoch:  137  loss:  4316.236812591553\n",
            "epoch:  138  loss:  4352.406337738037\n",
            "epoch:  139  loss:  4232.378162384033\n",
            "epoch:  140  loss:  4221.964099884033\n",
            "epoch:  141  loss:  4216.859661102295\n",
            "epoch:  142  loss:  4230.794597625732\n",
            "epoch:  143  loss:  4191.053840637207\n",
            "epoch:  144  loss:  4241.662055969238\n",
            "epoch:  145  loss:  4181.976764678955\n",
            "epoch:  146  loss:  4195.797676086426\n",
            "epoch:  147  loss:  4205.548309326172\n",
            "epoch:  148  loss:  4168.950588226318\n",
            "epoch:  149  loss:  4178.009929656982\n",
            "epoch:  150  loss:  4131.389602661133\n",
            "epoch:  151  loss:  4239.264064788818\n",
            "epoch:  152  loss:  4166.36266708374\n",
            "epoch:  153  loss:  4151.781494140625\n",
            "epoch:  154  loss:  4135.646022796631\n",
            "epoch:  155  loss:  4182.791877746582\n",
            "epoch:  156  loss:  4237.491703033447\n",
            "epoch:  157  loss:  4178.943660736084\n",
            "epoch:  158  loss:  4163.97106552124\n",
            "epoch:  159  loss:  4156.409934997559\n",
            "epoch:  160  loss:  4144.807540893555\n",
            "epoch:  161  loss:  4040.4152488708496\n",
            "epoch:  162  loss:  4187.307189941406\n",
            "epoch:  163  loss:  4124.919273376465\n",
            "epoch:  164  loss:  4166.6105880737305\n",
            "epoch:  165  loss:  4162.301300048828\n",
            "epoch:  166  loss:  4119.451496124268\n",
            "epoch:  167  loss:  4096.893394470215\n",
            "epoch:  168  loss:  4048.159149169922\n",
            "epoch:  169  loss:  4109.544937133789\n",
            "epoch:  170  loss:  4104.453212738037\n",
            "epoch:  171  loss:  4112.222602844238\n",
            "epoch:  172  loss:  4107.70267868042\n",
            "epoch:  173  loss:  4148.515869140625\n",
            "epoch:  174  loss:  4073.137508392334\n",
            "epoch:  175  loss:  4111.78462600708\n",
            "epoch:  176  loss:  4118.098606109619\n",
            "epoch:  177  loss:  4140.785717010498\n",
            "epoch:  178  loss:  4097.487777709961\n",
            "epoch:  179  loss:  4066.9360542297363\n",
            "epoch:  180  loss:  4082.5914459228516\n",
            "epoch:  181  loss:  4100.791996002197\n",
            "epoch:  182  loss:  4102.976036071777\n",
            "epoch:  183  loss:  4070.5179595947266\n",
            "epoch:  184  loss:  4145.677703857422\n",
            "epoch:  185  loss:  4133.916481018066\n",
            "epoch:  186  loss:  4033.4923515319824\n",
            "epoch:  187  loss:  3993.8358039855957\n",
            "epoch:  188  loss:  4038.9445724487305\n",
            "epoch:  189  loss:  4012.808795928955\n",
            "epoch:  190  loss:  4108.544746398926\n",
            "epoch:  191  loss:  4039.629222869873\n",
            "epoch:  192  loss:  4086.0986671447754\n",
            "epoch:  193  loss:  4101.3568115234375\n",
            "epoch:  194  loss:  4074.436912536621\n",
            "epoch:  195  loss:  4060.932834625244\n",
            "epoch:  196  loss:  4044.358627319336\n",
            "epoch:  197  loss:  4079.532783508301\n",
            "epoch:  198  loss:  4089.0375442504883\n",
            "epoch:  199  loss:  4070.6479873657227\n",
            "epoch:  200  loss:  4067.4929275512695\n",
            "Time: 201.559478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node2vec = {}\n",
        "# dataset_name = \"zhihu\"\n",
        "f = open('temp/embed.txt', 'rb')\n",
        "for i, j in enumerate(f):\n",
        "    if j.decode() != '\\n':\n",
        "        node2vec[i] = list(map(float, j.strip().decode().split(' ')))\n",
        "f1 = open(os.path.join('temp/test_graph.txt'), 'rb')\n",
        "edges = [list(map(int, i.strip().decode().split('\\t'))) for i in f1]\n",
        "nodes = list(set([i for j in edges for i in j]))\n",
        "a = 0\n",
        "b = 0\n",
        "for i, j in edges:\n",
        "    if i in node2vec.keys() and j in node2vec.keys():\n",
        "        dot1 = np.dot(node2vec[i], node2vec[j])\n",
        "        random_node = random.sample(nodes, 1)[0]\n",
        "        while random_node == j or random_node not in node2vec.keys():\n",
        "            random_node = random.sample(nodes, 1)[0]\n",
        "        dot2 = np.dot(node2vec[i], node2vec[random_node])\n",
        "        if dot1 > dot2:\n",
        "            a += 1\n",
        "        elif dot1 == dot2:\n",
        "            a += 0.5\n",
        "        b += 1\n",
        "\n",
        "print(\"Auc value:\", float(a) / b)"
      ],
      "metadata": {
        "id": "NZsQv5KyY83v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d56e57-77aa-479a-8d0c-af1c78e3f810"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auc value: 0.9487847222222222\n"
          ]
        }
      ]
    }
  ]
}